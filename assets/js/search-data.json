{
  
    
        "post0": {
            "title": "Comparing the Sentiment of Reviews and Ratings, with VADER and BERT",
            "content": "In another notebook we trained a recommender using collaborative filtering on a million ratings from Apple Podcasts. However, we didn&#39;t use the content of the reviews, which are an additional source of signal of user preference. Some of that signal can be extracted using sentiment analysis and could then be used to train a recommender system. . The sentiment of each review is (of course) highly correlated with the rating given by the user, but this correlation is not absolute. For example, there are some 1 star ratings for which the review text nonetheless clearly reflects a positive user preference, but the user rated it with 1 star to bring attention to some complaint: issues with the sound, or even that no new episodes have been released in a while. The recommender which is only trained on the ratings will miss these distinctions. . We will compare two different sentiment analysis techniques. . VADER . First we look VADER. This method associates a sentiment score to each text, which ranges from -1 for very negative text to 1 for very positive text (there are multiple scores but we use the compound score). To classify the reviews by sentiment we will need to set thresholds for this score. . VADER consists of a bag of words approach modified by some heuristic rules. The bag of words part refers to getting the score of a review simply by adding the scores of the individual words. Note that this approach would disregard the order of the words, so we can think of the words as being randomly shuffled in a metaphorical bag (of words). The problem with such a simplistic approach is that the order of the words actually matter quite a bit. This is why VADER adds some useful heuristics which take into account the order of the words to some extent. For example, &quot;not&quot; appearing closely before a word inverts the polarity of that word. The rules are explained in the original paper, which is very well written and worth a read. . BERT . The other method we will use is based on the newer and very popular BERT transformer (clearly ML researchers love puns). More precisely we will use distilBERT, a smaller version which is almost as precise but more efficient in both time and memory thanks to knowledge distillation. What makes BERT so popular is that it was of the first large language model that was made widely accessible for people to fine-tune for their own NLP tasks. This allows us to take advantage of the enormous resources spent by Google in training BERT with general text data and simply fine-tune it for our particular use case in just hours or even minutes on a single GPU. . On Hugging Face there is a distilBERT transfomer which has already been fine-tuned for the task of sentiment analysis. They used a variation of the Stanford Sentiment Treebank (SST), called SST2. SST consists of sentences from movie reviews which have been annotated by human judges (giving sentiment scores between 0 and 1 with a slider). In the SST2 version the labels are binary (0 or 1) instead of floats. . In this notebook we will compare VADER to the distilBERT model fine-tuned on SST2. In a separate notebook we will fine-tune the original pretrained distilBERT ourselves on this podcast reviews dataset. . Fine-tuning the model ourselves will result in significantly better predictions of the sentiment, or at least the sentiment reflected by the ratings (which are our labels for training). . On the flip side, using the model fine-tuned on SST2 allows us to explore the sentiment associated with the various reviews independently from the ratings. We mentioned above that some 1 star ratings are actually just constructive feedback and the review content itself is mostly positive. The model trained on a different dataset (like SST2) is more likely classify those as being positive despite the low rating. In contrast, the model we train on the podcast reviews will learn to correlate the sentiment predictions with the star ratings as much as possible. . 1. Sentiment Analysis with VADER . VADER relies on a lexicon of words, each with an associated polarity score. There are actually multiple scores but we will use compound score, which ranges from -1 (very negative) to 1 (very positive), and can be anywhere in between depending on the intensity of the sentiment. As mentioned in the introduction, the score of a sentence is roughly given by adding the scores of the individual words up, except that there are some heuristic rules. One such rule is inverting the score of a word if it is preceded by &quot;not&quot;. Considering how simple this method is, it works surprisingly well. One helpful feature is that the sentiment lexicon even contains emojis, which are used in many reviews. . 1.1 Load and Explore Data . In this section we: . Load the data from an SQLite file. | Compute the VADER polarity score of the reviews. | Visualize the score distribution. | &quot;Demojize&quot; the reviews (for distilBERT). | Save the polarity scores and demojized reviews with Pickle. | . First we need to load the data and save it in a Pandas DataFrame. . with sqlite3.connect(os.path.join(PATH, &#39;data&#39;, &#39;database.sqlite&#39;)) as con: get_reviews = &quot;&quot;&quot;SELECT author_id AS user_id, p.podcast_id, r.title, r.content, rating, p.title AS name, created_at FROM podcasts p INNER JOIN reviews r USING(podcast_id) &quot;&quot;&quot; reviews_raw = pd.read_sql(get_reviews, con, parse_dates=&#39;created_at&#39;) . Next we will compute the polarity score for each review. We use the SentimentIntensityAnalyzer from vaderSentiment. The polarity score has multiple components but we only need the compound score. . def polarity_score(text): sia = SentimentIntensityAnalyzer() return sia.polarity_scores(text)[&#39;compound&#39;] . polarity_score(&#39;I did not hate the movie.&#39;) . 0.4585 . It even works on emojis! This is actually relevant here because some podcast reviews contain emojis. . polarity_score(&#39;üòä&#39;) . 0.7184 . Two smiley faces are better than one: . polarity_score(&#39;üòäüòä&#39;) . 0.9001 . To compute one polarity score per review we will concatenate the title and the body of the review: . reviews_raw[&#39;review&#39;] = reviews_raw[&#39;title&#39;] + &#39;. &#39; + reviews_raw[&#39;content&#39;] . Now we compute the polarity score for all one million reviews, which takes a few minutes! . reviews_raw[&#39;polarity score&#39;] = reviews_raw[&#39;review&#39;].apply(polarity_score) . To feed the reviews to distilBERT later we need to convert emojis to text. Otherwise, they will be tokenized as &#39;unkown&#39; and the information will be lost. We use the emoji Python package. . reviews_raw[&#39;demojized review&#39;] = reviews_raw[&#39;review&#39;].apply(emoji.demojize) . We pickle the reviews dataframe to use in other notebooks. It also makes our life easier because we don&#39;t have to repeat the computation of the polarity score (which takes over 15 minutes) every time we start a new session. . reviews_raw.to_pickle(os.path.join(PATH, &#39;data&#39;, &#39;reviews_raw_sentiment.pkl&#39;)) . reviews_raw = pd.read_pickle(os.path.join(PATH, &#39;data&#39;, &#39;reviews_raw_sentiment.pkl&#39;)) . reviews_raw.head(2) . user_id podcast_id title content rating name created_at review polarity score demojized review . 0 F7E5A318989779D | c61aa81c9b929a66f0c1db6cbe5d8548 | really interesting! | Thanks for providing these insights. Really e... | 5 | Backstage at Tilles Center | 2018-04-24 12:05:16-07:00 | really interesting!. Thanks for providing thes... | 0.9109 | really interesting!. Thanks for providing thes... | . 1 F6BF5472689BD12 | c61aa81c9b929a66f0c1db6cbe5d8548 | Must listen for anyone interested in the arts!!! | Super excited to see this podcast grow. So man... | 5 | Backstage at Tilles Center | 2018-05-09 18:14:32-07:00 | Must listen for anyone interested in the arts!... | 0.9739 | Must listen for anyone interested in the arts!... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Having a look at the reviews, we see that VADER does catch some reviews that show the user actually likes the podcast but has some minor complaint to make. In that sense, one could use VADER to get the true user preference, which the rating is not reflecting correctly in those cases. . The following is an example of such a review. The user clearly likes the podcast yet left a 1 star rating. . reviews_raw.loc[9, &#39;content&#39;], reviews_raw.loc[9, &#39;rating&#39;] . (&#39;Great podcast, but the editors turn the volume down for the talks. The intros are loud, then you have to crank up the volume for the talk.&#39;, 1) . However, VADER exhibits a positivity bias and classifies many clearly negative reviews as positive. Because of this, it is probably not precise enough to give a useful signal of user preference in addition to the user rating. We will see that the sentiment predicted by the distilBERT model is much accurate. . Below we visualize the distribution of the VADER sentiment score for negative (1 and 2 star), neutral (3 star) and positive (4 and 5 star) ratings. . def plot_histograms_by_sentiment(reviews, column_name): fig, axs = plt.subplots(1, 3, figsize=(12, 4)) sns.histplot( reviews[reviews[&#39;rating&#39;].isin([1, 2])][column_name], ax=axs[0], bins=30, kde=True, ) sns.histplot( reviews[reviews[&#39;rating&#39;] == 3][column_name], ax=axs[1], bins=30, kde=True, ) sns.histplot( reviews[reviews[&#39;rating&#39;].isin([4, 5])][column_name], ax=axs[2], bins=30, kde=True, ) axs[0].set_title(&#39;1 and 2 stars&#39;) axs[1].set_title(&#39;3 stars&#39;) axs[2].set_title(&#39;4 and 5 stars&#39;) fig.tight_layout() plot_histograms_by_sentiment(reviews_raw, &#39;polarity score&#39;) . The histograms clearly show a positivity bias. We see that even for negative ratings the mean sentiment score is just over 0: . neg_mean = reviews_raw[reviews_raw[&#39;rating&#39;].isin([1, 2])][&#39;polarity score&#39;].mean() neut_mean = reviews_raw[reviews_raw[&#39;rating&#39;] == 3][&#39;polarity score&#39;].mean() pos_mean = reviews_raw[reviews_raw[&#39;rating&#39;].isin([4, 5])][&#39;polarity score&#39;].mean() print( f&#39;The mean VADER compound score for 1 and 2 star reviews is {neg_mean:.2} n&#39; f&#39;The mean VADER compound score for 3 star reviews is {neut_mean:.2} n&#39; f&#39;The mean VADER compound score for 4 and 5 star reviews is {pos_mean:.2}&#39; ) . The mean VADER compound score for 1 and 2 star reviews is 0.017 The mean VADER compound score for 3 star reviews is 0.37 The mean VADER compound score for 4 and 5 star reviews is 0.79 . The peaks at 0 are probably reviews for which VADER can&#39;t actually identify the sentiment. Regarding the 0 scores, a word of caution: The histograms can be misleading! The reviews with score 0 seem to be a large proportion for 1 and 2 star ratings, and certainly seem to comprise much smaller proportions for the other rating values. However, we see below that the differences are actually not as dramatic as they might look in the histograms: reviews with score 0 are approximately $4 %$ for negative ratings, $3 %$ for neutral ratings, and $2 %$ for positive ratings. . reviews_raw.groupby(&#39;rating&#39;).apply(lambda df: (df[&#39;polarity score&#39;] == 0).mean()) . rating 1 0.052295 2 0.035327 3 0.032470 4 0.025381 5 0.023363 dtype: float64 . 1.2 Clean Data . Some reviews appear to be spam, which is why we will remove reviews by users with suspiciously high review counts. We will also exclude some podcasts for kids because a majority of the &quot;reviews&quot; for those podcasts aren&#39;t actually reviews. Instead, children appear to be using the reviews as a forum in which to post jokes. . Additionally, we are writing two functions to convert both VADER polarity scores and ratings into sentiment classes. We will contemplate two possibilities: . Three classes: 0 (negative), 1 (neutral) and 2 (positive). | Binary case: 0 (negative) and 1 (positive). | . The functions below can handle either case. . kids_podcasts = [&#39;Wow in the World&#39;, &#39;Story Pirates&#39;, &#39;Pants on Fire&#39;, &#39;The Official Average Boy Podcast&#39;, &#39;Despicable Me&#39;, &#39;Rebel Girls&#39;, &#39;Fierce Girls&#39;, &#39;Like and Subscribe: A podcast about YouTube culture&#39;, &#39;The Casagrandes Familia Sounds&#39;, &#39;What If World - Stories for Kids&#39;, &#39;Good Night Stories for Rebel Girls&#39;, &#39;Gird Up! Podcast&#39;, &#39;Highlights Hangout&#39;, &#39;Be Calm on Ahway Island Bedtime Stories&#39;, &#39;Smash Boom Best&#39;, &#39;The Cramazingly Incredifun Sugarcrash Kids Podcast&#39;] def remove_spammers(reviews, max_reviews=135): &#39;Remove users with suspiciously high review count.&#39; mask = reviews.groupby(&#39;user_id&#39;)[&#39;podcast_id&#39;].transform(&#39;count&#39;) &lt;= max_reviews return reviews[mask] def rating_to_sentiment(ratings, neutral=True): sentiments = np.zeros(ratings.shape) sentiments[ratings == 3] = 1 if neutral else 0 sentiments[ratings &gt; 3] = 2 if neutral else 1 return sentiments def vader_score_to_sentiment(polarity_scores, neg_threshold=0.4, pos_threshold=0.75): assert neg_threshold &lt;= pos_threshold sentiments = np.zeros(polarity_scores.shape) sentiments[polarity_scores &gt; neg_threshold] = 1 if pos_threshold &gt; neg_threshold: # otherwise there is no neutral class sentiments[polarity_scores &gt; pos_threshold] = 2 return sentiments . reviews_raw[&#39;VADER sentiment&#39;] = vader_score_to_sentiment(reviews_raw[&#39;polarity score&#39;]) reviews_raw[&#39;sentiment&#39;] = rating_to_sentiment(reviews_raw[&#39;rating&#39;]) reviews_raw[&#39;binary sentiment&#39;] = rating_to_sentiment(reviews_raw[&#39;rating&#39;], neutral=False) . Note that in addition to cleaning the data we are taking a sample consisting of 100,000 reviews. This makes the data more manageable while still being a large enough dataset to be representative when we evaluate our sentiment classifiers. On top of that, we sample the data in such a way that each star rating is represented equally, to make sure that classification accuracy isn&#39;t skewed in favor of positive ratings, which constitute over $90 %$ of the original dataset. . reviews_raw[&#39;sentiment&#39;].value_counts() / reviews_raw[&#39;sentiment&#39;].count() . 2.0 0.905482 0.0 0.071179 1.0 0.023339 Name: sentiment, dtype: float64 . Now we are finally ready to do the cleaning and take a 100,000 reviews sample with equal ratings representation. . reviews = ( reviews_raw.query(&quot;name not in @kids_podcasts&quot;) .pipe(remove_spammers) .groupby(&#39;rating&#39;) .apply(lambda df: df.sample(n=20000)) .sample(frac=1) .reset_index(drop=True) ) . 1.3 Results for VADER Classification into Negative, Neutral, and Positive . We used the VADER score to classify reviews into those three classes based on two thresholds (which we tuned by hand to maximize accuracy). . The ratings were used as the ground truth sentiment, where 1 and 2 star ratings correspond to negative, 3 star ratings to neutral, and 4 and 5 star ratings to positive. . The following is the confusion matrix for the whole (raw) dataset. . pd.crosstab(reviews_raw[&#39;VADER sentiment&#39;], reviews_raw[&#39;sentiment&#39;]) . sentiment 0.0 1.0 2.0 . VADER sentiment . 0.0 45084 | 9247 | 79046 | . 1.0 11054 | 4525 | 116317 | . 2.0 13931 | 9203 | 695998 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; accuracy_score(reviews_raw[&#39;sentiment&#39;], reviews_raw[&#39;VADER sentiment&#39;]) . 0.757418948501887 . The accuracy is relatively high but this can be misleading because in the original dataframe reviews_raw over $90 %$ of ratings are positive. . The recall shows that the classification is no better than chance when restricted to neutral reviews (if we picked a rating at random we would get 3 stars, i.e. neutral, $20 %$ of the time, although the fact that the recall is $19.7 %$ is probably a coincidence). . recall_score(reviews_raw[&#39;sentiment&#39;], reviews_raw[&#39;VADER sentiment&#39;], average=None) . array([0.64342291, 0.19695321, 0.78082617]) . The accuracy on the cleaned data in reviews is less misleading because we made sure that all ratings are equally represented with 20,000 reviews each: . pd.crosstab(reviews[&#39;VADER sentiment&#39;], reviews[&#39;sentiment&#39;]) . sentiment 0.0 1.0 2.0 . VADER sentiment . 0.0 24394 | 8080 | 5497 | . 1.0 6680 | 3930 | 6007 | . 2.0 8926 | 7990 | 28496 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We see that on reviews the accuracy is much lower but the recall is similar (it is a little lower but that might change if we choose different thresholds for the VADER score). . accuracy_score(reviews[&#39;sentiment&#39;], reviews[&#39;VADER sentiment&#39;]) . 0.5682 . recall_score(reviews[&#39;sentiment&#39;], reviews[&#39;VADER sentiment&#39;], average=None) . array([0.60985, 0.1965 , 0.7124 ]) . 1.4 Optimal Threshold for VADER and Binary Sentiment . From now on we will consider a binary classification problem with the classes negative and positive, i.e. discarding the neutral category. We do this because the fine-tuned distilBERT model we are using is only a binary classifier. Note: In a separate notebook we will train distilBERT to predict the ratings, which would allow us to have a neutral class or even just 5 classes (the ratings themselves). . It seems clear that reviews with 1 or 2 stars should be considered negative and reviews with 4 and 5 stars positive. The question is how to classify the 3 star reviews. While VADER mostly gives them positive scores, we will see that the distilBERT model actually mostly classifies them as negative. From reading some of the 3 star reviews it does appear that the distilBERT model is right and we already noted that VADER has a positivity bias. . To classify the reviews into two classes using VADER we just have a single threshold: everything to the left of it is negative and everything to the right positive. With the following function we will find the threshold resulting in the highest possible classification accuracy, given a list of VADER scores and corresponding ground truth sentiments. This is just intended as a baseline for the distilBERT model and is not a principled way to tune VADER, since this threshold probably has a high variance and we are overfitting on our training set. . def find_best_split(reviews, score_col=&#39;polarity score&#39;, sentiment_col=&#39;binary sentiment&#39;): sorted_df = ( reviews.sort_values(by=score_col) [[score_col, sentiment_col]] ) scores = sorted_df[score_col] sentiments = sorted_df[sentiment_col] correct_class = max_correct = sentiments.sum() optimal_thresh = prev_score = -1 count = 0 for score, sentiment in zip(scores, sentiments): if sentiment == 0: correct_class += 1 else: if score != prev_score and correct_class &gt; max_correct: optimal_thresh = prev_score max_correct = correct_class correct_class -= 1 prev_score = score if correct_class &gt; max_correct: optimal_thresh = score max_correct = correct_class return {&#39;threshold&#39;: optimal_thresh, &#39;accuracy&#39;: max_correct / scores.size} . First we will define 3 star ratings as negative (in fact, we already did this when we computed the &#39;binary sentiment&#39; column above). . best_split = find_best_split(reviews) best_split . {&#39;threshold&#39;: 0.7945, &#39;accuracy&#39;: 0.71772} . Next let&#39;s think of 3 star ratings as positive instead (we call it alternative binary sentiment). . reviews[&#39;alt binary sentiment&#39;] = reviews[&#39;rating&#39;].map({1:0,2:0,3:1,4:1,5:1}) print(find_best_split(reviews, sentiment_col=&#39;alt binary sentiment&#39;)) reviews = reviews.drop(columns=&#39;alt binary sentiment&#39;); . {&#39;threshold&#39;: 0.296, &#39;accuracy&#39;: 0.70864} . Considering 3 star reviews to be positive instead of negative made virtually no difference to the accuracy. This is a little surprising because VADER tends to give 3 star reviews positive sentiment scores. The reason that the accuracy doesn&#39;t improve is that a lower threshold results in a lower recall for negative ratings, on which VADER also has a positivity bias. . Below we compute the recall. It isn&#39;t great but not terrible either considering the simplicity of the VADER method and the difficulty of the task. However, distilBERT will do better and without the need to fine-tune it on our data (although, as I mentioned, we will fine-tune it in a separate notebook and the accuracy will improve significantly). . recall_score(reviews[&#39;polarity score&#39;] &gt;= best_split[&#39;threshold&#39;], reviews[&#39;binary sentiment&#39;], average=None) . array([0.77318932, 0.6405552 ]) . 2. BERT for Sentiment Classification . As mentioned in the introduction, we are using a distilBERT model fine-tuned on the SST2 dataset consisting of sentences from movie reviews. . tokenizer = AutoTokenizer.from_pretrained(FINETUNED_SST) bert_model = AutoModelForSequenceClassification.from_pretrained(FINETUNED_SST) . Before being fed to the transformer we need to tokenize the text. Tokens often correspond to full words but can also correspond to parts of words (this happens for rare words) or symbols like punctuation. . The maximum length the transformer can handle is 512 so we will have to clip particularly long reviews. In fact, we will set a lower maximum than that to improve performance. A single long review would mean we have to make the whole batch longer (the lengths of the samples in the batch must agree and the shorter ones are filled with placeholder tokens) and this uses more memory on the GPU and requires more computations. . The cutoff should be larger than the length of the overwhelming majority of reviews, to make sure it has negligible effect on the precision of the model. To determine this cutoff we will plot the length distribution. . token_lengths = np.array([len(tokenizer.encode(s, truncation=True, max_length=512)) for s in reviews[&#39;demojized review&#39;]]) sns.histplot(token_lengths, kde=True) plt.xlabel(&#39;Token count for review&#39;); . f&#39;Just {(np.array(token_lengths) &gt;= 256).mean()*100:.2} percent of the reviews have a length over 256&#39; . &#39;Just 2.7 percent of the reviews have a length over 256&#39; . Now we take the demojized reviews from the reviews dataframe, tokenize them with a maximum length of 256 tokens and create a dataloader which will feed the tokenized samples in batches of size 32 to the distilBERT classifier. . def tokenize_function(data, tokenizer, max_length=256): return tokenizer(data[&#39;demojized review&#39;], truncation=True, max_length=max_length) dataset = Dataset.from_dict(reviews[[&#39;demojized review&#39;]]) tokenized_dataset = ( dataset.map(partial(tokenize_function, tokenizer=tokenizer), batched=True) .remove_columns([&#39;demojized review&#39;]) ) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) dataloader = DataLoader( tokenized_dataset, batch_size=32, collate_fn=data_collator ) . The distilBERT model outputs the logits for the targets 0 (negative) and 1 (positive). The following function evaluates the model on a dataloader and returns an array of probabilities for the reviews fed through the dataloader being positive. . def get_probs(model, dataloader): probs = [] model = model.to(device) model.eval() m = nn.Softmax(dim=1) for batch in tqdm(dataloader): batch = {k: v.to(device) for k, v in batch.items()} with torch.no_grad(): outputs = model(**batch) logits = outputs.logits probs += m(logits)[:,1].tolist() return np.array(probs) . reviews[&#39;BERT probs&#39;] = get_probs(bert_model, dataloader) . 3. Comparing VADER and BERT . We see in the following histograms that distilBERT classifies most 3 star ratings as negative. This is interesting because VADER does the complete opposite, assigning overwhelmingly positive scores to 3 star reviews. . Something else to note is that this model is very confident in its predictions, with two sharp peaks around 0 and 1 but very little in between. We can see a little less confidence for 3 star ratings. Those are the most mixed reviews in terms of sentiment and they do exhibit some more intermediate probability values than other star ratings. However, the VADER score does a much better job as a continuous measure of sentiment outside of 0 and 1. To be fair, the distilBERt classifier is intended to make correct binary predictions, not to quantify uncertainty. . plot_histograms_by_sentiment(reviews, &#39;BERT probs&#39;) . Most times when VADER and distilBERT disagree, the latter is right. This is not surprising because distilBERT is a much more complicated and computation intensive technique. . The following is a typical example which has a very high VADER score yet very low BERT probability of being positive (and BERT is right). . reviews.loc[945, [&#39;title&#39;, &#39;content&#39;, &#39;rating&#39;, &#39;polarity score&#39;, &#39;BERT probs&#39;]] . title Used to be great content Used to be a great comedy podcast, until, in s... rating 1 polarity score 0.9509 BERT probs 0.022963 Name: 945, dtype: object . reviews.loc[945, &#39;review&#39;] . &#39;Used to be great. Used to be a great comedy podcast, until, in someone &#39;s infinite wisdom, decided to replace the only talent on that &#34;network&#34;.&#39; . The reason the VADER score is so high for that review is that it contains many words with positive sentiment (great, wisdom, talent) and not really any words with negative sentiment (in isolation). The distilBERT model however is able to take into account the context of the whole sentence (&quot;used to be&quot;, &quot;the only talent&quot;). . Let&#39;s look at the reviews with high probability of being positive according to distilBERT but a very negative VADER score, and vice versa. . We see below that there are very few cases in the former category but many in the latter. The distilBERT model is usually right but certainly not every time. . Actually going through the reviews one gets the impression that those numbers underestimate how much better distilBERT is to VADER. In many cases the review sentiment is only loosely correlated with the rating. As such, some &quot;misclassifications&quot; by distilBERT could even be seen as additional signal to the ratings rather than mistakes. . reviews.loc[(reviews[&#39;BERT probs&#39;] &gt; 0.95) &amp; (reviews[&#39;polarity score&#39;] &lt; -0.9), &#39;rating&#39;].value_counts() . 4 17 3 17 1 16 2 13 5 13 Name: rating, dtype: int64 . reviews.loc[(reviews[&#39;BERT probs&#39;] &lt; 0.05) &amp; (reviews[&#39;polarity score&#39;] &gt; 0.9), &#39;rating&#39;].value_counts() . 3 1753 2 1731 1 871 4 769 5 74 Name: rating, dtype: int64 . Here are some 3 star reviews that distilBERT classifies as positive. . reviews[(reviews[&#39;BERT probs&#39;] &gt; 0.99) &amp; (reviews[&#39;rating&#39;] == 3)][[&#39;title&#39;, &#39;content&#39;, &#39;rating&#39;, &#39;polarity score&#39;, &#39;BERT probs&#39;]].head(10) . title content rating polarity score BERT probs . 50 ‚ò∫Ô∏è | Love the podcast!! You guys keep me entertained!! | 3 | 0.9015 | 0.999867 | . 54 Great content‚Ä¶ Please work on format | I love this podcast - the information is so va... | 3 | 0.9890 | 0.993949 | . 62 Cool, but... | Smart and funny pod. She gets a little condesc... | 3 | 0.8422 | 0.996883 | . 221 conservatives might pass on this | I&#39;ve been listening to this podcast for years ... | 3 | 0.3679 | 0.995914 | . 459 Hi it‚Äôs me and my family | Lion and cat cat dog cat cat | 3 | 0.0000 | 0.998568 | . 466 JUST WOW! | It‚Äôs like making a murderer REVERSED, this sou... | 3 | 0.9485 | 0.998971 | . 470 Another re-run? | Are you guys ever going to get back to creatin... | 3 | 0.3076 | 0.998778 | . 473 Keep it up | Hey, I&#39;m a follow controller I like where this... | 3 | 0.3612 | 0.998447 | . 542 Overuse of ‚Äúincredible‚Äù | I love these people but every other word Mallo... | 3 | 0.3818 | 0.996635 | . 584 my opinion | I love this podcast but you guys sidetrack way... | 3 | 0.3818 | 0.991521 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; On the other hand, here is an example of a 3 star review that distilBERT classifies as negative. . reviews[(reviews[&#39;BERT probs&#39;] &lt; 0.01) &amp; (reviews[&#39;rating&#39;] == 3)][[&#39;title&#39;, &#39;content&#39;, &#39;rating&#39;, &#39;polarity score&#39;, &#39;BERT probs&#39;]].head(10) . title content rating polarity score BERT probs . 6 Used to be great, now is just okay | I preferred the old format. Loved hearing from... | 3 | 0.8151 | 0.004110 | . 8 Okay... | Cool I guess but I can&#39;t download any songs cu... | 3 | 0.7391 | 0.007136 | . 15 Maz | Bring him back please. Not a good move. Los... | 3 | -0.1546 | 0.005427 | . 17 Decent content | Overall the content is good and timely, but th... | 3 | -0.7420 | 0.000644 | . 22 You should | Do an interview with Not The Worst Show, it co... | 3 | 0.7575 | 0.001318 | . 38 Addictive but trashy | I listened to most of the first season. It&#39;s a... | 3 | 0.9111 | 0.000854 | . 41 Basically two guys catching a buzz...... | These guys seem to have a grasp on what a beer... | 3 | 0.3498 | 0.000442 | . 76 not as good anymore | The episodes were never as long as I wanted th... | 3 | 0.8948 | 0.004767 | . 78 Inconsistent | The shows vary from top shelf to really weak d... | 3 | -0.5563 | 0.000442 | . 80 Enjoy...Except for the Recyling | I subscrobe to the show and enjoy it quite a b... | 3 | 0.3716 | 0.006824 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Looking at the reviews, there is a clear difference between the ones classified as positive and those classified as negative, even though all of them come with 3 star ratings. This exemplifies one way in which the review sentiment can give us additional signal of user preference. . Now let&#39;s look at 1 and 2 star rating reviews that distilBERT classifies as positive. We can see that many of them talk about how they used to love the show, which confuses the model. Others complain about politics. We will see that at least anecdotally the distilBERT we fine-tune on this dataset will do better on those types of reviews that are common within this dataset. . reviews[(reviews[&#39;BERT probs&#39;] &gt; 0.99) &amp; reviews[&#39;rating&#39;].isin([1, 2])][[&#39;title&#39;, &#39;content&#39;, &#39;rating&#39;, &#39;polarity score&#39;, &#39;BERT probs&#39;]].head(10) . title content rating polarity score BERT probs . 4 Why | Why would you trust anything from MSNBC? They ... | 2 | 0.7096 | 0.990558 | . 122 Lurched Right | Good folks. What happened to them at Evergreen... | 2 | 0.5794 | 0.992818 | . 239 Great Content but.... | I love the idea of this show and listen to oth... | 2 | 0.9259 | 0.998793 | . 255 No more Monica | I‚Äôm certain you are a good person, and I would... | 2 | 0.9603 | 0.998205 | . 285 Wanted to like this... | Yikes, combine cheerful ignorance with minimal... | 2 | 0.7331 | 0.996498 | . 418 hope Mr Black acknowledges the REAL creators o... | If you want to hear this done properly, listen... | 1 | 0.8689 | 0.993219 | . 492 More of the Same Now | I used to absolutely love Invisibilia. The ep... | 1 | 0.8858 | 0.993986 | . 523 Tip very intelligent butttttt!!! | Tip you have to let your guests speak to have ... | 1 | 0.9429 | 0.995483 | . 551 Well, So Much For That | When I first saw this podcast, I was VERY exci... | 2 | 0.9633 | 0.995017 | . 713 And That‚Äôs Why You‚Äôre Awesome | I have been listening to ATWWD for over a year... | 2 | 0.9714 | 0.999869 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Finally these are some 4 and 5 star rating reviews that distilBERt classifies as negative. We can see that it is mostly 4 star reviews and furthermore they all seem to be complaining. It makes sense that distilBERT would classify them as negative. . reviews[(reviews[&#39;BERT probs&#39;] &lt; 0.01) &amp; reviews[&#39;rating&#39;].isin([4, 5])][[&#39;title&#39;, &#39;content&#39;, &#39;rating&#39;, &#39;polarity score&#39;, &#39;BERT probs&#39;]].head(10) . title content rating polarity score BERT probs . 20 Great but.... | The interview with that Nolan guy was too anno... | 4 | 0.8916 | 0.001526 | . 58 Are u dumb | Yo flip release podcast audio already facts!!!... | 4 | -0.3348 | 0.001746 | . 61 New listener | People have tried to get me interested in podc... | 5 | 0.6908 | 0.001139 | . 106 Up speak?! | The show content is really interesting? But th... | 4 | 0.3981 | 0.000748 | . 222 sound issues | i love the words when the sound is good enough... | 4 | 0.1862 | 0.009301 | . 244 Great storytelling | I really like this podcast but I gotta admit t... | 4 | 0.8589 | 0.008216 | . 257 Weird music over talking | Jan. 7 episode has some weird loud music over ... | 4 | -0.5204 | 0.006214 | . 318 I do like this podcast... | I really like this podcast, however, John has ... | 4 | 0.6953 | 0.001644 | . 333 Bananas...NOOOOOwaaahhhh.... | Let me say, I absolutely love the MFM podcast ... | 4 | -0.8815 | 0.002038 | . 442 Hoping season 4 is a return to form | Seasons 1 &amp; 2 were utter brilliance. Season 3 ... | 4 | 0.3167 | 0.004957 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt;",
            "url": "david-recio.com/2022/10/21/vader-bert-podcast-reviews.html",
            "relUrl": "/2022/10/21/vader-bert-podcast-reviews.html",
            "date": " ‚Ä¢ Oct 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Training distilBERT to Predict Podcast Ratings",
            "content": "In a previous notebook we compared the performance of two methods to classify podcast reviews by sentiment. The VADER polarity score and a distilBERT transformer fine-tuned on the SST2 dataset, which consists of sentences from movie reviews. . In this notebook we will use the Hugging Face API and PyTorch to fine-tune the base distilBERT on the podcast reviews. We will train it to predict the rating given the title and body of the review. By converting the rating to sentiment, this also gives us a sentiment classifier. . Once we have trained our model, we will compare its performance with the &quot;ready to use&quot; model trained on SST2. Specifically we will compute accuracy and recall, and also visualize the distributions of predicted probabilities. . By base distilBERT we mean the model that has been pretrained only on two general language tasks (as opposed to sentiment analysis): predicting masked words in a sentence and predicting whether two sentences are adjacent. (Additionally the outputs of the BERT model from which it is distilled are used but we won&#39;t go into the details of knowledge distillation). Fine-tuning distilBERT for sentiment classification consists of adding a classification layer at the end of the transformer and then training this slightly modified transformer for sentiment classification (with a small learning rate). This is called transfer learning. . As part of the training process we will use Ray Tune to find good hyperparameters. . Finally, we will compare the predictions of our model with the model trained on SST2 on some reviews we will &quot;hold out&quot; of the training set. We picked those reviews because VADER was having a particularly hard time with them and they seemed interesting examples to test what the models have learned about podcast reviews. . Summary of results: . Training distilBERT for about two epochs on 80,000 podcast reviews results in a sentiment prediction accuracy of $0.883$ on a test set of 5000 reviews. The accuracy of the distilBERT fine-tuned on SST2 on the same test set is $0.815$. The training, evaluation and test sets were constructed in such a way that all ratings are represented equally. | Comparing the two models on some interesting reviews held out of the training set, it appears that our model learned to classify some difficult cases which are particular to the context of podcast reviews. For example, reviews of horror themed podcasts use language that would be indicative of negative sentiment in other contexts but are actually expressing approval of the show in this context. | We measured model learning beyond the accuracy and training/evaluation loss: One observation is that the recall for positive and negative reviews gets more balanced over time, even as the accuracy and loss plateau. Another aspect we note is that the model gets more confident over time, i.e. distribution of output probabilities became more and more concentrated. This is a symptom of overfitting. | . 1. Data Cleaning . In a previous notebook we processed the reviews data but it is still a noisy dataset! We will do the following: . Some reviews appear to be spam, which is why we will remove reviews by users with suspiciously high review counts. | We will also exclude some podcasts for kids because a majority of the reviews for those podcasts aren&#39;t really reviews. Instead, children appear to be using the reviews as a forum in which to post jokes. | Finally, will remove repeat reviews (reviews from the same user for the same podcast) to make sure there is no data leakage from the test set to the training set. I&#39;m not sure why there are repeat reviews but I suspect that they are edited reviews. The reason we need to exclude them is that the review content is often very similar and the rating is usually the same. | . Special holdout dataset: As mentioned, we will exclude a couple of reviews (on which we want to evaluate the models at the end) from the training set to make sure they haven&#39;t been memorized by the model (their indices are in holdout_ids). This is separate from the evaluation and test sets and not intended to be statistically significant, just to illustrate what the model has learned. . reviews_raw = pd.read_pickle(os.path.join(PATH, &#39;data/reviews_raw_sentiment.pkl&#39;)) . def remove_spammers(reviews, max_reviews=135): &#39;Remove users with suspiciously high review count.&#39; mask = reviews.groupby(&#39;user_id&#39;)[&#39;podcast_id&#39;].transform(&#39;count&#39;) &lt;= max_reviews return reviews[mask] def keep_only_latest_rating(ratings): &#39;Remove repeat reviews, keeping the latest. Also sorts the ratings by date.&#39; return ratings.sort_values(by=&#39;created_at&#39;, ascending=False).drop_duplicates(subset=[&#39;podcast_id&#39;, &#39;user_id&#39;]) . holdout_ids = = [956562, 49428, 15130, 212768, 123052, 283, 973, 1516, 2566, 14947, 922494, 9, 10, 76, 11204, 11211, 48339] kids_podcasts = [&#39;Wow in the World&#39;, &#39;Story Pirates&#39;, &#39;Pants on Fire&#39;, &#39;The Official Average Boy Podcast&#39;, &#39;Despicable Me&#39;, &#39;Rebel Girls&#39;, &#39;Fierce Girls&#39;, &#39;Like and Subscribe: A podcast about YouTube culture&#39;, &#39;The Casagrandes Familia Sounds&#39;, &#39;What If World - Stories for Kids&#39;, &#39;Good Night Stories for Rebel Girls&#39;, &#39;Gird Up! Podcast&#39;, &#39;Highlights Hangout&#39;, &#39;Be Calm on Ahway Island Bedtime Stories&#39;, &#39;Smash Boom Best&#39;, &#39;The Cramazingly Incredifun Sugarcrash Kids Podcast&#39;] . reviews = ( reviews_raw.query(&#39;name not in @kids_podcasts&#39;) .query(&#39;index not in @holdout_ids&#39;) .pipe(remove_spammers) .pipe(keep_only_latest_rating) ) . The classifier will expect the labels (targets) to start at 0, which is why we need to create a labels column which shifts the ratings by one. . reviews[&#39;labels&#39;] = reviews[&#39;rating&#39;] - 1 . Now we create validation and test sets, in such a way that they both have around 1000 reviews for each star rating (uniform distribution of star ratings). We do this to ensure that the accuracy metric treats all star ratings equally. . reviews_val_test = ( reviews.groupby(&#39;labels&#39;) .sample(n=2000) ) reviews_train = reviews.query(&#39;index not in @reviews_val_test.index&#39;) reviews_val, reviews_test = train_test_split(reviews_val_test, test_size=0.5) . reviews_val[&#39;labels&#39;].value_counts() . 2 1013 4 1011 1 1007 3 989 0 980 Name: labels, dtype: int64 . reviews_test[&#39;labels&#39;].value_counts() . 0 1020 3 1011 1 993 4 989 2 987 Name: labels, dtype: int64 . reviews_train[&#39;labels&#39;].value_counts() . 4 811106 0 43229 3 26008 2 19150 1 17149 Name: labels, dtype: int64 . The data has a very high skew towards 5 star ratings. We will create a training set which contains the same amount of reviews for each rating value, to make sure the model treats each rating class equally, so to speak. We did the same for the evaluation and test splits. . reviews_train_equal = ( reviews_train.groupby(&#39;labels&#39;) .sample(n=16_000) .sample(frac=1) #shuffle rows ) . Now we pickle the train, evaluation and test sets to ensure reproducibility. We took care to set seeds for NumPy and PyTorch at the beginning of the notebook but it is best to be careful, particularly in a notebook were cells could be run multiple times or out of order. . reviews_train_equal.to_pickle(os.path.join(PATH, &#39;data/reviews_train_equal.pkl&#39;)) reviews_val.to_pickle(os.path.join(PATH, &#39;data/reviews_val.pkl&#39;)) reviews_test.to_pickle(os.path.join(PATH, &#39;data/reviews_test.pkl&#39;)) . reviews_train_equal = pd.read_pickle(os.path.join(PATH, &#39;data/reviews_train_equal.pkl&#39;)) reviews_val = pd.read_pickle(os.path.join(PATH, &#39;data/reviews_val.pkl&#39;)) reviews_test = pd.read_pickle(os.path.join(PATH, &#39;data/reviews_test.pkl&#39;)) . reviews_train_equal[&#39;labels&#39;].value_counts() . 4 16000 0 16000 1 16000 3 16000 2 16000 Name: labels, dtype: int64 . Now we tokenize the datasets, which needs to be done before we feed them to the model. We saw in the previous notebook that under $3 %$ of reviews result in sequences of more than 256 tokens, which is why we set that as the max_length. . train_dataset_equal = Dataset.from_dict(reviews_train_equal[[&#39;demojized review&#39;, &#39;labels&#39;]]) val_dataset = Dataset.from_dict(reviews_val[[&#39;demojized review&#39;, &#39;labels&#39;]]) # We omit &#39;labels&#39; in the test_dataset because otherwise we would get an error # when evaluating the model fine tuned on SST2 with only 2 labels, instead of 5 test_dataset = Dataset.from_dict(reviews_test[[&#39;demojized review&#39;]]) dataset_dict = DatasetDict({&#39;train_equal&#39;:train_dataset_equal, &#39;validation&#39;:val_dataset, &#39;test&#39;:test_dataset}) . tokenizer = AutoTokenizer.from_pretrained(PRETRAINED) . def tokenize_function(data, tokenizer, truncation=True, max_length=256): return tokenizer(data[&#39;demojized review&#39;], truncation=truncation, max_length=max_length) . tokenized_datasets = ( dataset_dict.map(partial(tokenize_function, tokenizer=tokenizer), batched=True) .remove_columns([&#39;demojized review&#39;]) ) tokenized_datasets.set_format(&#39;torch&#39;) . 2. Hyperparameter Search . Now we are ready to do the hyperparameter search using Hugging Face and Ray Tune. We will perform a random search over the batch sizes 8, 16 and 32, as well as learning rates between $10^{-5}$ and $10^{-4}$. This roughly agrees with the recommended parameters in the original paper (Appendix A.3). They also recommend the epoch numbers 2, 3 and 4 but we will only use 2 epochs because that takes a long time already (and many Colab compute units üò¨). . We also use an ASHA scheduler to terminate less promising trials, although in retrospect I&#39;m not sure that is a good idea (see below). That said, with the scheduler it already took me 3 hours with a &quot;premium GPU&quot; on Colab and from the results it looks like the hyperparameter choice does not make a big difference (within a reasonable range). . The following function will evaluate the model during the training. It computes the accuracy and the recall. The recall is computed for every rating class and thus consists of 5 numbers. . def compute_metrics(eval_preds): logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) accuracy = accuracy_score(labels, predictions) recall = recall_score( y_true=labels, y_pred=predictions, labels=[0, 1, 2, 3, 4], average=None, ) metric_names = [f&#39;recall_{n}_stars&#39; for n in range(1, 6)] + [&#39;accuracy&#39;] return dict(zip(metric_names, list(recall) + [accuracy])) . training_args = TrainingArguments( output_dir=&#39;hugging-face-trainers&#39;, num_train_epochs=2, eval_steps=500, evaluation_strategy=&#39;steps&#39;, save_strategy=&#39;no&#39;, disable_tqdm=True, ) def get_model(): return AutoModelForSequenceClassification.from_pretrained( &#39;distilbert-base-uncased&#39;, num_labels=5, ignore_mismatched_sizes=True, ) trainer = Trainer( model=None, model_init=get_model, args=training_args, train_dataset=tokenized_datasets[&#39;train_equal&#39;], eval_dataset=tokenized_datasets[&#39;validation&#39;], tokenizer=tokenizer, compute_metrics=compute_metrics, ) scheduler = ASHAScheduler( metric=&quot;eval_accuracy&quot;, mode=&quot;max&quot;, grace_period=4, reduction_factor=4, ) def hp_space(trial): return { &#39;learning_rate&#39;: tune.loguniform(1e-5, 1e-4), &#39;per_device_train_batch_size&#39;: tune.choice([8, 16, 32]), } reporter = JupyterNotebookReporter( parameter_columns={ &#39;learning_rate&#39;: &#39;lr&#39;, &#39;per_device_train_batch_size&#39;: &#39;train_bs/gpu&#39;, }, metric_columns=[ &#39;eval_accuracy&#39;, &#39;eval_loss&#39;, &#39;epoch&#39;, &#39;eval_recall_1_stars&#39;, &#39;eval_recall_2_stars&#39;, &#39;eval_recall_3_stars&#39;, &#39;eval_recall_4_stars&#39;, &#39;eval_recall_5_stars&#39; ] ) . best_run = trainer.hyperparameter_search( hp_space=hp_space, backend=&#39;ray&#39;, direction=&#39;maximize&#39;, n_trials=15, resources_per_trial={ &#39;cpu&#39;: 1, &#39;gpu&#39;: 1/3, }, scheduler=scheduler, checkpoint_score_attr=&#39;training_iteration&#39;, progress_reporter=reporter, local_dir=os.path.join(PATH, &#39;models&#39;), name=&#39;hp_search_5class_uniform_ratings&#39;, log_to_file=True, ) . == Status ==Current time: 2022-10-18 15:20:26 (running for 02:04:20.59)Memory usage on this node: 9.4/83.5 GiBUsing AsyncHyperBand: num_stopped=9 Bracket: Iter 64.000: None | Iter 16.000: 0.5892 | Iter 4.000: 0.5764Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/49.81 GiB heap, 0.0/24.91 GiB objectsResult logdir: /content/drive/MyDrive/ml-projects/podcast-reviews/models/hp_search_5class_uniform_ratingsNumber of trials: 15/15 (15 TERMINATED) Trial name status loc lr train_bs/gpulr_scheduler eval_accuracy eval_loss epoch eval_recall_1_stars eval_recall_2_stars eval_recall_3_stars eval_recall_4_stars eval_recall_5_stars . _objective_0585f_00000 | TERMINATED | 172.28.0.2:907 | 2.36886e-05 | 8 | | 0.5982 | 0.972204 | 2 | 0.615306 | 0.480636 | 0.499506 | 0.579373 | 0.816024 | . _objective_0585f_00001 | TERMINATED | 172.28.0.2:947 | 6.02131e-05 | 8 | | 0.5778 | 0.992277 | 0.8 | 0.646939 | 0.409136 | 0.456071 | 0.521739 | 0.855589 | . _objective_0585f_00002 | TERMINATED | 172.28.0.2:949 | 1.43217e-05 | 32 | | 0.5962 | 0.967467 | 2 | 0.639796 | 0.46574 | 0.467917 | 0.55814 | 0.849654 | . _objective_0585f_00003 | TERMINATED | 172.28.0.2:1774 | 2.1563e-05 | 32 | | 0.597 | 0.961714 | 2 | 0.633673 | 0.474677 | 0.471866 | 0.564206 | 0.840752 | . _objective_0585f_00004 | TERMINATED | 172.28.0.2:2334 | 9.33061e-05 | 16 | | 0.539 | 1.06329 | 0.4 | 0.346939 | 0.481629 | 0.563672 | 0.408493 | 0.885262 | . _objective_0585f_00005 | TERMINATED | 172.28.0.2:2670 | 1.51993e-05 | 8 | | 0.543 | 1.06707 | 0.2 | 0.517347 | 0.543198 | 0.479763 | 0.471183 | 0.701286 | . _objective_0585f_00006 | TERMINATED | 172.28.0.2:2895 | 4.08934e-05 | 8 | | 0.5496 | 1.05635 | 0.2 | 0.577551 | 0.474677 | 0.454097 | 0.569262 | 0.673591 | . _objective_0585f_00007 | TERMINATED | 172.28.0.2:3032 | 1.95537e-05 | 32 | | 0.597 | 0.962354 | 2 | 0.635714 | 0.473684 | 0.46693 | 0.564206 | 0.844708 | . _objective_0585f_00008 | TERMINATED | 172.28.0.2:3170 | 1.11344e-05 | 32 | | 0.5756 | 1.00606 | 0.8 | 0.694898 | 0.457795 | 0.378085 | 0.465116 | 0.883284 | . _objective_0585f_00009 | TERMINATED | 172.28.0.2:3273 | 6.09784e-05 | 32 | | 0.5948 | 0.974191 | 2 | 0.614286 | 0.46574 | 0.501481 | 0.57634 | 0.816024 | . _objective_0585f_00010 | TERMINATED | 172.28.0.2:3751 | 9.62124e-05 | 8 | | 0.5222 | 1.10365 | 0.2 | 0.67449 | 0.288977 | 0.455084 | 0.488372 | 0.707221 | . _objective_0585f_00011 | TERMINATED | 172.28.0.2:3914 | 4.05084e-05 | 8 | | 0.556 | 1.058 | 0.2 | 0.610204 | 0.447865 | 0.479763 | 0.538928 | 0.704253 | . _objective_0585f_00012 | TERMINATED | 172.28.0.2:4076 | 1.03101e-05 | 8 | | 0.5434 | 1.0641 | 0.2 | 0.516327 | 0.529295 | 0.481737 | 0.462083 | 0.725025 | . _objective_0585f_00013 | TERMINATED | 172.28.0.2:4261 | 6.43276e-05 | 8 | | 0.5432 | 1.07166 | 0.2 | 0.697959 | 0.351539 | 0.465943 | 0.497472 | 0.706231 | . _objective_0585f_00014 | TERMINATED | 172.28.0.2:4370 | 1.70174e-05 | 32 | | 0.5968 | 0.964603 | 2 | 0.637755 | 0.471698 | 0.46693 | 0.560162 | 0.847676 | . 2022-10-18 15:20:26,481 INFO tune.py:759 -- Total run time: 7461.08 seconds (7460.56 seconds for the tuning loop). . best_run . BestRun(run_id=&#39;0585f_00000&#39;, objective=3.5890449331933936, hyperparameters={&#39;learning_rate&#39;: 2.368863950364079e-05, &#39;per_device_train_batch_size&#39;: 8}) . It seems that with higher batch size the accuracy is often better but at the cost of having higher recall for 1 and 5 stars and worse recall for the intermediate ratings. However, this might be an artifact of the ASHA early stopping, which appears to favor the larger batch sizes. This is probably because they converge more quickly at first given that the model gets to &quot;see&quot; more examples at each step. What makes me think this is indeed the case is: 1) all the completed trials except one have batch size 32 2) the only smaller batch trial that wasn&#39;t stopped early ended up having the highest accuracy (it was also the first trial and thus not subject to being stopped early). . In any case, the differences in accuracy are very small and it&#39;s not worth it to repeat the costly hyperparameter search. . 3. Fine-Tuning distilBERT . We will use the best parameters we found above: batch size 8 and learning rate $2.44 cdot10^{-5}$. Note that because we will train with 4 epochs instead of the 2 epochs we used in the hyperparameter search, the learning rate will actually be s little higher for longer at the beginning, given that we are using a linear learning rate scheduler (the default). . base_model = AutoModelForSequenceClassification.from_pretrained( PRETRAINED, num_labels=5, ignore_mismatched_sizes=True, ) . training_args = TrainingArguments( output_dir=os.path.join(PATH, &#39;models/best-run&#39;), learning_rate=2.4e-5, per_device_train_batch_size=8, per_device_eval_batch_size=32, num_train_epochs=4, evaluation_strategy=&#39;steps&#39;, ) trainer = Trainer( model=base_model, args=training_args, train_dataset=tokenized_datasets[&#39;train_equal&#39;], eval_dataset=tokenized_datasets[&#39;validation&#39;], tokenizer=tokenizer, compute_metrics=compute_metrics, ) trainer.train() . using `logging_steps` to initialize `eval_steps` to 500 PyTorch: setting up devices The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-). /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 80000 Num Epochs = 4 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 40000 . . [40000/40000 2:22:31, Epoch 4/4] Step Training Loss Validation Loss Recall 1 Stars Recall 2 Stars Recall 3 Stars Recall 4 Stars Recall 5 Stars Accuracy . 500 | 1.245400 | 1.128538 | 0.621429 | 0.371400 | 0.318855 | 0.506572 | 0.841741 | 0.531600 | . 1000 | 1.093900 | 1.104158 | 0.355102 | 0.348560 | 0.655479 | 0.425683 | 0.782394 | 0.515000 | . 1500 | 1.078600 | 1.058309 | 0.416327 | 0.629593 | 0.319842 | 0.556117 | 0.807122 | 0.546400 | . 2000 | 1.059700 | 1.082167 | 0.811224 | 0.345581 | 0.339585 | 0.427705 | 0.801187 | 0.544000 | . 2500 | 1.051500 | 1.042632 | 0.665306 | 0.434955 | 0.320829 | 0.562184 | 0.812067 | 0.558400 | . 3000 | 1.033800 | 1.027814 | 0.750000 | 0.400199 | 0.388944 | 0.485339 | 0.791296 | 0.562400 | . 3500 | 1.048100 | 1.018655 | 0.706122 | 0.328699 | 0.543929 | 0.564206 | 0.717112 | 0.571400 | . 4000 | 1.009200 | 1.007908 | 0.629592 | 0.470705 | 0.384995 | 0.577351 | 0.802176 | 0.572600 | . 4500 | 1.027100 | 1.057035 | 0.359184 | 0.422046 | 0.571570 | 0.593529 | 0.796241 | 0.549600 | . 5000 | 0.993200 | 1.007861 | 0.609184 | 0.384310 | 0.527147 | 0.549039 | 0.818002 | 0.577600 | . 5500 | 1.004700 | 1.021505 | 0.647959 | 0.555114 | 0.429418 | 0.412538 | 0.809100 | 0.571000 | . 6000 | 1.016400 | 0.996876 | 0.578571 | 0.509434 | 0.529121 | 0.430738 | 0.850643 | 0.580400 | . 6500 | 0.990500 | 1.001491 | 0.620408 | 0.606753 | 0.366239 | 0.529828 | 0.786350 | 0.581800 | . 7000 | 1.022000 | 0.974803 | 0.677551 | 0.448858 | 0.399803 | 0.566229 | 0.852621 | 0.588600 | . 7500 | 0.989900 | 0.978257 | 0.700000 | 0.477656 | 0.432379 | 0.444894 | 0.873393 | 0.585600 | . 8000 | 0.971000 | 0.997515 | 0.747959 | 0.423039 | 0.385982 | 0.419616 | 0.881306 | 0.571200 | . 8500 | 1.010300 | 0.978720 | 0.729592 | 0.400199 | 0.478776 | 0.467139 | 0.871414 | 0.589200 | . 9000 | 0.987800 | 0.982950 | 0.735714 | 0.433962 | 0.444225 | 0.492417 | 0.843719 | 0.589600 | . 9500 | 1.009900 | 0.971602 | 0.758163 | 0.390268 | 0.434353 | 0.566229 | 0.837784 | 0.596600 | . 10000 | 0.993400 | 0.987826 | 0.552041 | 0.414101 | 0.462981 | 0.669363 | 0.815035 | 0.582600 | . 10500 | 0.861500 | 1.013859 | 0.531633 | 0.578947 | 0.483712 | 0.539939 | 0.806133 | 0.588600 | . 11000 | 0.885600 | 1.003105 | 0.734694 | 0.397219 | 0.482725 | 0.486350 | 0.863501 | 0.592600 | . 11500 | 0.878500 | 0.998439 | 0.530612 | 0.507448 | 0.475814 | 0.588473 | 0.833828 | 0.587600 | . 12000 | 0.843300 | 0.981679 | 0.600000 | 0.504469 | 0.478776 | 0.479272 | 0.898121 | 0.592600 | . 12500 | 0.872100 | 1.021082 | 0.726531 | 0.443893 | 0.365252 | 0.553084 | 0.855589 | 0.588200 | . 13000 | 0.864900 | 0.984879 | 0.598980 | 0.450844 | 0.461994 | 0.597573 | 0.848665 | 0.591600 | . 13500 | 0.867700 | 0.968538 | 0.574490 | 0.454816 | 0.502468 | 0.585440 | 0.827893 | 0.589200 | . 14000 | 0.865500 | 0.996079 | 0.586735 | 0.510427 | 0.527147 | 0.442872 | 0.883284 | 0.590800 | . 14500 | 0.860200 | 0.987961 | 0.653061 | 0.436941 | 0.505429 | 0.606673 | 0.751731 | 0.590400 | . 15000 | 0.889400 | 0.988690 | 0.574490 | 0.409136 | 0.545903 | 0.575329 | 0.830861 | 0.587400 | . 15500 | 0.851700 | 0.996287 | 0.610204 | 0.478649 | 0.463968 | 0.569262 | 0.826904 | 0.589800 | . 16000 | 0.862600 | 0.978930 | 0.636735 | 0.508441 | 0.440276 | 0.550051 | 0.821958 | 0.591400 | . 16500 | 0.881000 | 0.985724 | 0.555102 | 0.573982 | 0.440276 | 0.595551 | 0.800198 | 0.593200 | . 17000 | 0.838600 | 0.989725 | 0.674490 | 0.440914 | 0.502468 | 0.550051 | 0.829871 | 0.599400 | . 17500 | 0.886200 | 0.974624 | 0.635714 | 0.520357 | 0.464956 | 0.510617 | 0.838773 | 0.594200 | . 18000 | 0.870900 | 0.985520 | 0.644898 | 0.527309 | 0.441264 | 0.505561 | 0.855589 | 0.595000 | . 18500 | 0.873500 | 0.990966 | 0.595918 | 0.451837 | 0.438302 | 0.644085 | 0.818991 | 0.589600 | . 19000 | 0.842400 | 1.015863 | 0.495918 | 0.555114 | 0.513327 | 0.589484 | 0.794263 | 0.590200 | . 19500 | 0.861000 | 0.984092 | 0.674490 | 0.425025 | 0.513327 | 0.554095 | 0.803165 | 0.593800 | . 20000 | 0.853900 | 0.994874 | 0.670408 | 0.511420 | 0.443238 | 0.526795 | 0.800198 | 0.590200 | . 20500 | 0.738000 | 1.033550 | 0.562245 | 0.549156 | 0.472853 | 0.524772 | 0.832839 | 0.588800 | . 21000 | 0.711800 | 1.087762 | 0.568367 | 0.425025 | 0.553801 | 0.570273 | 0.792285 | 0.582200 | . 21500 | 0.723800 | 1.099150 | 0.552041 | 0.448858 | 0.510365 | 0.652174 | 0.767557 | 0.586200 | . 22000 | 0.706500 | 1.131904 | 0.700000 | 0.482622 | 0.377098 | 0.500506 | 0.837784 | 0.579200 | . 22500 | 0.716100 | 1.097652 | 0.605102 | 0.516385 | 0.462981 | 0.595551 | 0.724036 | 0.580600 | . 23000 | 0.711500 | 1.042393 | 0.619388 | 0.471698 | 0.512340 | 0.521739 | 0.841741 | 0.593600 | . 23500 | 0.665200 | 1.120767 | 0.505102 | 0.628600 | 0.455084 | 0.561173 | 0.727992 | 0.576000 | . 24000 | 0.717600 | 1.072822 | 0.635714 | 0.463754 | 0.519250 | 0.496461 | 0.850643 | 0.593400 | . 24500 | 0.701500 | 1.091549 | 0.598980 | 0.554121 | 0.446199 | 0.525784 | 0.790307 | 0.583200 | . 25000 | 0.689600 | 1.085112 | 0.624490 | 0.444886 | 0.490622 | 0.537917 | 0.841741 | 0.588000 | . 25500 | 0.725500 | 1.109724 | 0.603061 | 0.550149 | 0.455084 | 0.452983 | 0.858556 | 0.584400 | . 26000 | 0.695800 | 1.102224 | 0.591837 | 0.508441 | 0.501481 | 0.507583 | 0.830861 | 0.588400 | . 26500 | 0.697100 | 1.079746 | 0.673469 | 0.385303 | 0.510365 | 0.507583 | 0.842730 | 0.583800 | . 27000 | 0.693500 | 1.112715 | 0.611224 | 0.499503 | 0.449161 | 0.608696 | 0.781405 | 0.589800 | . 27500 | 0.700400 | 1.105307 | 0.632653 | 0.447865 | 0.479763 | 0.563195 | 0.826904 | 0.590000 | . 28000 | 0.693200 | 1.099970 | 0.513265 | 0.598808 | 0.449161 | 0.559151 | 0.762611 | 0.577000 | . 28500 | 0.733600 | 1.059791 | 0.565306 | 0.545184 | 0.494571 | 0.583418 | 0.748764 | 0.587600 | . 29000 | 0.697600 | 1.076711 | 0.546939 | 0.530288 | 0.489635 | 0.589484 | 0.776459 | 0.586800 | . 29500 | 0.687600 | 1.077929 | 0.607143 | 0.459782 | 0.492596 | 0.580384 | 0.826904 | 0.593400 | . 30000 | 0.706100 | 1.084087 | 0.540816 | 0.493545 | 0.501481 | 0.582406 | 0.789318 | 0.581800 | . 30500 | 0.542400 | 1.207177 | 0.639796 | 0.460775 | 0.473840 | 0.546006 | 0.796241 | 0.583200 | . 31000 | 0.556300 | 1.225795 | 0.576531 | 0.466733 | 0.509378 | 0.590495 | 0.745796 | 0.577800 | . 31500 | 0.529400 | 1.255818 | 0.602041 | 0.464747 | 0.511352 | 0.585440 | 0.739862 | 0.580600 | . 32000 | 0.553200 | 1.246235 | 0.583673 | 0.493545 | 0.481737 | 0.544995 | 0.784372 | 0.577800 | . 32500 | 0.570700 | 1.228383 | 0.570408 | 0.511420 | 0.469891 | 0.553084 | 0.797230 | 0.580600 | . 33000 | 0.555200 | 1.267651 | 0.539796 | 0.507448 | 0.503455 | 0.603640 | 0.702275 | 0.571400 | . 33500 | 0.552200 | 1.268231 | 0.623469 | 0.435948 | 0.476802 | 0.563195 | 0.789318 | 0.577600 | . 34000 | 0.556900 | 1.260661 | 0.605102 | 0.478649 | 0.452122 | 0.568251 | 0.778437 | 0.576400 | . 34500 | 0.548100 | 1.275731 | 0.588776 | 0.441907 | 0.505429 | 0.582406 | 0.742829 | 0.572200 | . 35000 | 0.576100 | 1.266743 | 0.590816 | 0.474677 | 0.477789 | 0.577351 | 0.763600 | 0.576800 | . 35500 | 0.550400 | 1.259426 | 0.634694 | 0.428004 | 0.490622 | 0.565217 | 0.773492 | 0.578200 | . 36000 | 0.557200 | 1.279438 | 0.581633 | 0.498510 | 0.471866 | 0.567240 | 0.767557 | 0.577400 | . 36500 | 0.546600 | 1.275166 | 0.579592 | 0.485601 | 0.480750 | 0.565217 | 0.787339 | 0.579800 | . 37000 | 0.549500 | 1.283615 | 0.570408 | 0.492552 | 0.483712 | 0.583418 | 0.771513 | 0.580400 | . 37500 | 0.546800 | 1.264037 | 0.568367 | 0.510427 | 0.463968 | 0.567240 | 0.781405 | 0.578400 | . 38000 | 0.526100 | 1.276842 | 0.621429 | 0.452830 | 0.469891 | 0.563195 | 0.782394 | 0.577800 | . 38500 | 0.547500 | 1.275661 | 0.576531 | 0.494538 | 0.479763 | 0.577351 | 0.771513 | 0.580000 | . 39000 | 0.534700 | 1.273110 | 0.574490 | 0.494538 | 0.484699 | 0.575329 | 0.776459 | 0.581200 | . 39500 | 0.561400 | 1.266741 | 0.581633 | 0.492552 | 0.478776 | 0.573306 | 0.778437 | 0.581000 | . 40000 | 0.547000 | 1.265490 | 0.581633 | 0.493545 | 0.473840 | 0.563195 | 0.787339 | 0.580000 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; For all the models we trained so far the recall is higher for the extreme ratings (1 and 5 stars) than for the intermediate ratings. It is especially high for 5 star ratings. Could we make the model better by having a higher proportion of &quot;harder&quot; ratings in the training set? We will test that idea now by creating a new training set with unequal star rating proportions. . train_proportions = { 0: 16000, 1: 17000, 2: 19000, 3: 17000, 4: 10000, } # In case we loaded val and test with Pickle reviews_train = ( reviews.query(&#39;index not in @reviews_val.index&#39;) .query(&#39;index not in @reviews_test.index&#39;) ) reviews_train_unequal = ( reviews_train[[&#39;demojized review&#39;, &#39;labels&#39;]] .groupby(&#39;labels&#39;, group_keys=False) .apply(lambda x: x.sample(n=train_proportions[x.name])) .sample(frac=1) ) train_dataset_unequal = Dataset.from_dict(reviews_train_unequal) tokenized_train_dataset_unequal = ( train_dataset_unequal.map(partial(tokenize_function, tokenizer=tokenizer), batched=True) .remove_columns([&#39;demojized review&#39;]) ) . base_model = AutoModelForSequenceClassification.from_pretrained( PRETRAINED, num_labels=5, ignore_mismatched_sizes=True, ) . training_args = TrainingArguments( output_dir=os.path.join(PATH, &#39;models/best-run-unequal-ratings&#39;), learning_rate=2.4e-5, per_device_train_batch_size=8, per_device_eval_batch_size=32, num_train_epochs=2, evaluation_strategy=&#39;steps&#39;, ) trainer = Trainer( model=base_model, args=training_args, train_dataset=tokenized_train_dataset_unequal, eval_dataset=tokenized_datasets[&#39;validation&#39;], tokenizer=tokenizer, compute_metrics=compute_metrics, ) trainer.train() . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, ***** Running training ***** Num examples = 79000 Num Epochs = 2 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 19750 You&#39;re using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding. . . [19750/19750 1:34:17, Epoch 2/2] Step Training Loss Validation Loss Recall 1 Stars Recall 2 Stars Recall 3 Stars Recall 4 Stars Recall 5 Stars Accuracy . 500 | 1.253800 | 1.130906 | 0.546939 | 0.423039 | 0.576505 | 0.311426 | 0.693373 | 0.511000 | . 1000 | 1.160400 | 1.073716 | 0.424490 | 0.494538 | 0.401777 | 0.491405 | 0.847676 | 0.532800 | . 1500 | 1.120700 | 1.062113 | 0.583673 | 0.397219 | 0.514314 | 0.493428 | 0.757666 | 0.549400 | . 2000 | 1.100400 | 1.043622 | 0.667347 | 0.312810 | 0.442251 | 0.562184 | 0.798220 | 0.556000 | . 2500 | 1.082100 | 1.078694 | 0.767347 | 0.211519 | 0.526160 | 0.633974 | 0.559842 | 0.538200 | . 3000 | 1.076700 | 1.028931 | 0.598980 | 0.474677 | 0.503455 | 0.470172 | 0.779426 | 0.565600 | . 3500 | 1.058500 | 1.041818 | 0.668367 | 0.347567 | 0.590326 | 0.518706 | 0.655786 | 0.555800 | . 4000 | 1.080900 | 1.041594 | 0.748980 | 0.280040 | 0.507404 | 0.584429 | 0.684471 | 0.560000 | . 4500 | 1.060900 | 1.022272 | 0.539796 | 0.618669 | 0.403751 | 0.600607 | 0.681503 | 0.568800 | . 5000 | 1.030700 | 1.036904 | 0.490816 | 0.391261 | 0.593287 | 0.611729 | 0.707221 | 0.559200 | . 5500 | 1.047100 | 0.994701 | 0.460204 | 0.539225 | 0.556762 | 0.497472 | 0.821958 | 0.576200 | . 6000 | 1.033200 | 0.990816 | 0.646939 | 0.415094 | 0.461007 | 0.666330 | 0.716123 | 0.580400 | . 6500 | 1.021000 | 1.000840 | 0.623469 | 0.378352 | 0.524186 | 0.515672 | 0.870425 | 0.582600 | . 7000 | 1.034700 | 1.009722 | 0.575510 | 0.606753 | 0.455084 | 0.488372 | 0.768546 | 0.579200 | . 7500 | 1.023800 | 1.005465 | 0.724490 | 0.352532 | 0.434353 | 0.731041 | 0.632047 | 0.573400 | . 8000 | 1.009200 | 0.996092 | 0.637755 | 0.479643 | 0.537019 | 0.590495 | 0.653808 | 0.579400 | . 8500 | 1.015800 | 1.052649 | 0.564286 | 0.627607 | 0.480750 | 0.441860 | 0.688427 | 0.561000 | . 9000 | 1.011900 | 1.008287 | 0.754082 | 0.417080 | 0.437315 | 0.632963 | 0.659743 | 0.579000 | . 9500 | 1.033500 | 1.014436 | 0.491837 | 0.503476 | 0.560711 | 0.676441 | 0.603363 | 0.567200 | . 10000 | 0.982700 | 1.003126 | 0.557143 | 0.455809 | 0.608095 | 0.547017 | 0.709199 | 0.575800 | . 10500 | 0.922200 | 0.987351 | 0.596939 | 0.477656 | 0.518263 | 0.578362 | 0.798220 | 0.594000 | . 11000 | 0.884400 | 1.028805 | 0.608163 | 0.577954 | 0.461994 | 0.518706 | 0.751731 | 0.583800 | . 11500 | 0.907400 | 1.020639 | 0.503061 | 0.455809 | 0.595262 | 0.647118 | 0.650841 | 0.570600 | . 12000 | 0.915900 | 0.999687 | 0.571429 | 0.520357 | 0.549852 | 0.522750 | 0.776459 | 0.588600 | . 12500 | 0.889600 | 0.995653 | 0.597959 | 0.419067 | 0.519250 | 0.668352 | 0.719090 | 0.584400 | . 13000 | 0.886300 | 1.010592 | 0.634694 | 0.425025 | 0.587364 | 0.565217 | 0.713155 | 0.585000 | . 13500 | 0.862700 | 1.004916 | 0.527551 | 0.469712 | 0.608095 | 0.572295 | 0.732938 | 0.582600 | . 14000 | 0.890900 | 1.003140 | 0.663265 | 0.465740 | 0.530109 | 0.559151 | 0.668645 | 0.577000 | . 14500 | 0.897200 | 0.990828 | 0.648980 | 0.522344 | 0.453110 | 0.561173 | 0.807122 | 0.598400 | . 15000 | 0.883700 | 1.015247 | 0.642857 | 0.424032 | 0.512340 | 0.680485 | 0.667656 | 0.584800 | . 15500 | 0.876600 | 1.002395 | 0.532653 | 0.506455 | 0.535044 | 0.608696 | 0.722057 | 0.581200 | . 16000 | 0.885100 | 0.986371 | 0.586735 | 0.494538 | 0.472853 | 0.651163 | 0.765579 | 0.594000 | . 16500 | 0.877400 | 0.989928 | 0.612245 | 0.466733 | 0.528134 | 0.602629 | 0.751731 | 0.592200 | . 17000 | 0.887300 | 0.999306 | 0.570408 | 0.520357 | 0.498519 | 0.602629 | 0.755687 | 0.589600 | . 17500 | 0.874400 | 0.999116 | 0.605102 | 0.451837 | 0.551826 | 0.602629 | 0.731949 | 0.588600 | . 18000 | 0.871900 | 0.997107 | 0.582653 | 0.463754 | 0.541955 | 0.617796 | 0.754698 | 0.592200 | . 18500 | 0.870700 | 1.002845 | 0.551020 | 0.493545 | 0.536032 | 0.625885 | 0.732938 | 0.588000 | . 19000 | 0.896300 | 0.995333 | 0.592857 | 0.486594 | 0.531096 | 0.617796 | 0.714144 | 0.588400 | . 19500 | 0.865900 | 0.991305 | 0.603061 | 0.480636 | 0.518263 | 0.604651 | 0.752720 | 0.591800 | . &lt;/div&gt; &lt;/div&gt; TrainOutput(global_step=19750, training_loss=0.9750066875988924, metrics={&#39;train_runtime&#39;: 5661.6116, &#39;train_samples_per_second&#39;: 27.907, &#39;train_steps_per_second&#39;: 3.488, &#39;total_flos&#39;: 7133320516097280.0, &#39;train_loss&#39;: 0.9750066875988924, &#39;epoch&#39;: 2.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Changing the rating proportions in the training set definitely improved the recall score for 3 and 4 star ratings but at the cost of the recall for 1 and 5 star ratings. The accuracy stayed the same. Depending on the use case and what our objective function or cost function is, one might want to use such a training set with hand tuned proportions of star ratings. We will not pursue this further. . 4. Evaluating on the Test Set . Now we will compare various checkpoints of the models we fine-tuned with the distilBERT which was fine-tuned on the SST2 dataset. . We will write a function which computes the accuracy and recall, and also plots the distributions of the probabilities the model assigns to each class, conditional on the actual ground truth class. We will do this both for the classification into star ratings (5 classes) and for the classification into sentiment (2 classes). The latter will allow us to compare our models to the SST2 distilBERT, which is a binary classifier. . We will use the test set for the evaluation. For that purpose we will create test_dataloader and the get_probs function, which takes the model and the dataloader and returns the predicted probabilities. . data_collator = DataCollatorWithPadding(tokenizer=tokenizer) test_dataloader = DataLoader( tokenized_datasets[&#39;test&#39;], batch_size=32, collate_fn=data_collator ) . def get_probs(model, dataloader): probs = [] model = model.to(device) model.eval() m = nn.Softmax(dim=1) for batch in tqdm(dataloader): batch = {k: v.to(device) for k, v in batch.items()} with torch.no_grad(): outputs = model(**batch) logits = outputs.logits probs += m(logits).tolist() return np.array(probs) . We define the sentiment as: . negative or 0 if the rating is 3 or lower | positive or 1 if the rating is 4 or 5. | . The reason we make 3 negative is that the distilBERT fine-tuned on SST2 classifies most 3 star ratings as negative. Looking at the reviews they do seem mostly negative. . reviews_test[&#39;sentiment&#39;] = (reviews_test[&#39;rating&#39;] &gt; 3).astype(int) . The following function computes the accuracy and recall (for each class). If the sentiment argument is set to True, the classes are the binary sentiment. If a model predicts ratings, they will be converted to sentiments inside the function. If the model predictions are binary, the sentiment argument passed is ignored and set to True. . The function also plots histograms of probabilities assigned by the model in a grid. The rows in the grid correspond to the true classes (rating or sentiment for the review) and the columns correspond to the probability predictions. Each histogram consists of the probabilities that the model assigns to the class for that column when restricting to the reviews with true class given by the row. If the model is performing well, we expect the probabilities on the diagonal of the grid to concentrate at 1 and for the remaining histograms to concentrate at 0. . def plot_rating_hists(probs, targets_df): fig, axs = plt.subplots(5, 5, figsize=(16, 8), constrained_layout=True) fig.suptitle( &#39;Distribution of the predicted probabilities conditional on the true rating&#39;, size=&#39;x-large&#39;, ) for y_true, ax_row in enumerate(axs): for y_pred, ax in enumerate(ax_row): sns.histplot( probs[targets_df[&#39;labels&#39;] == y_true, y_pred], ax=ax, kde=True, bins=20, ) if y_true == 0: ax.set_title( f&#39;Probability that rating is {y_pred + 1}&#39; ) if y_pred == 0: ax.set_ylabel(f&#39;True rating is {y_true + 1}&#39;, size=&#39;large&#39;) def plot_sentiment_hists(probs, targets_df): fig, axs = plt.subplots(1, 2, figsize=(12, 6), constrained_layout=True) fig.suptitle( &#39;Distribution of the probabilities that reviews are positive&#39;, size=&#39;x-large&#39;, ) for sentiment in [0, 1]: sns.histplot( probs[targets_df[&#39;sentiment&#39;] == sentiment], ax=axs[sentiment], kde=True, bins=20, ) axs[sentiment].set_title( f&quot;True sentiment is {[&#39;negative&#39;, &#39;positive&#39;][sentiment]}&quot; ) def evaluate_and_plot(checkpoint, dataloader, targets_df, sentiment=False): &quot;&quot;&quot;targets_df must contain columns &#39;sentiment&#39; (binary) and &#39;labels&#39;&quot;&quot;&quot; model = AutoModelForSequenceClassification.from_pretrained(checkpoint) probs = get_probs(model, dataloader) n_classes = probs.shape[1] assert n_classes in {2, 5}, &#39;Model must predict either rating or binary sentiment&#39; if not sentiment and n_classes == 2: sentiment = True print( &#39;Setting sentiment to True because the model is a binary classifier&#39; ) preds = np.argmax(probs, axis=1) if sentiment and n_classes == 5: preds = preds &gt; 2 probs = probs[:, 3:].sum(axis=1) if n_classes == 2: probs = probs[:, 1] if sentiment: plot_sentiment_hists(probs, targets_df) else: plot_rating_hists(probs, targets_df) target_col = &#39;sentiment&#39; if sentiment else &#39;labels&#39; return{ &#39;accuracy&#39;: accuracy_score(targets_df[target_col], preds), &#39;recall&#39;: recall_score(targets_df[target_col], preds, average=None), } . Now we will use this function to evaluate what looks like it might be the best checkpoint during training, at 17,000 steps. This is a little under 2 epochs. After 17,000 steps, it appears the model is starting to overfit because at that point the evaluation slowly increases again while the training loss keeps going down. At the start of the fourth epoch we are clearly overfitting: the evaluation loss increases dramatically and the training loss just keeps going down. . evaluate_and_plot( os.path.join(PATH, &#39;models/best-run/checkpoint-17000&#39;), test_dataloader, reviews_test, ) . {&#39;accuracy&#39;: 0.5886, &#39;recall&#39;: array([0.66078431, 0.44008056, 0.46909828, 0.52818991, 0.84428716])} . We see both in the recall and in the histograms that the model has a particularly hard time with 2 and 3 star ratings. . For example, for 3 star reviews, the model is more likely than not ($53 %$ of the time) to predict a different rating (mostly 2 or 4, but sometimes 1) than to predict the correct 3 star rating. Beyond that, even if it identifies the 3 star reviews correctly almost half of the time, it is very rarely &quot;confident&quot; in its prediction. To be fair, when it gets 3 star reviews wrong it&#39;s almost always predicting them to be 2 star reviews and sometimes 4 star reviews. . For 5 star ratings the model does pretty well, only occasionally mistaking them for 4 star reviews. Similarly for 1 star reviews. . The histograms for 4 star reviews are interesting: The probabilities in histogram (4,4) have twin peaks at 0 and 1. When it misclassifies 4 star reviews, it is mostly as 3 and 5 stars. But the probabilities it assigns to those mistaken predictions are different. For 5 star misclassifications there is a small peak of very confident predictions, whereas the 3 star misclassifications are less confident (mostly under 0.7). See the histograms at (4,3) and at (4,5). . To compare our model to the one fine-tuned on SST2 we need to evaluate the prediction of sentiment (rather than ratings). We do this in the next cell and get an accuracy of $0.883$. We also get a recall of $0.895$ for negative ratings and $0.864$ for positive ratings. . This prediction accuracy is similar to those in the literature for BERT models on similar datasets. For example, the authors of the distilBERT fine-tuned on SST2 which we are using Report a $0.913$ accuracy. See also Table 6 in the original BERT paper. . Our model has an accuracy which is 2 or 3 percent lower than the ones I referenced, but our dataset is also significantly noisier. Taking that into account $0.883$ is a good result. . evaluate_and_plot( os.path.join(PATH, &#39;models/best-run/checkpoint-17000&#39;), test_dataloader, reviews_test, sentiment=True, ) . {&#39;accuracy&#39;: 0.8828, &#39;recall&#39;: array([0.89533333, 0.864 ])} . Now we evaluate the model which was fine-tuned on SST2 and is available on Hugging Face. . evaluate_and_plot( FINETUNED_SST, test_dataloader, reviews_test, sentiment=True, ) . {&#39;accuracy&#39;: 0.8148, &#39;recall&#39;: array([0.81466667, 0.815 ])} . The model fine-tuned on SST has a lower accuracy $0.815$ and also lower recall scores of $0.815$ and $0.815$. It is remarkable how similar the recall scores of positive and negative ratings are for the distilBERT model fine-tuned on SST. After rounding they are actually identical. . The model we fine-tuned does have significantly higher accuracy and recall than the one fine-tuned on the SST2 dataset. However, this is to be expected because of two main reasons: Firstly, podcast and movie reviews have pretty different distributions and so it is not unexpected that a model trained on one would do worse on the other. But an additional factor is that the sentiment labeling for SST2 was done on individual sentences by human judges, whereas our labels are for entire reviews consisting of multiple sentences (which might have different sentiments even within the same review) and the labels are the ratings, which are certainly a noisier signal of sentiment than labels given by multiple people for the specific purpose of training a classifier. . All things considered, it is actually impressive how well the distilBERT fine-tuned on SST2 does on this data! . 5. Evaluating the Models on some Interesting Reviews . Let&#39;s see how our model does compared to the SST2 distilBERT on the special examples we held out of the training set. We found those examples in a previous notebook by looking at misclassifications coming from VADER, and saved them because they are interesting and some of them seem to encapsulate peculiarities of podcast reviews. The idea is that a model trained directly on podcast reviews might do better on those. . holdout_reviews = reviews_raw.query(&#39;index in @holdout_ids&#39;) holdout_dataset = Dataset.from_dict(holdout_reviews[[&#39;demojized review&#39;]]) tokenized_holdout = ( holdout_dataset.map(partial(tokenize_function, tokenizer=tokenizer), batched=True) .remove_columns([&#39;demojized review&#39;]) ) holdout_dataloader = DataLoader( tokenized_holdout, batch_size=16, collate_fn=data_collator ) . mymodel = AutoModelForSequenceClassification.from_pretrained( os.path.join(PATH, &#39;models/best-run/checkpoint-17000&#39;) ) myprobs = get_probs(mymodel, holdout_dataloader) holdout_reviews[[f&#39;{k} star prob&#39; for k in range(1, 6)]] = myprobs holdout_reviews[&#39;star pred&#39;] = myprobs.argmax(axis=1) pos_prob = myprobs[:, 3:].sum(axis=1) holdout_reviews[&#39;positive prob mymodel&#39;] = pos_prob holdout_reviews[&#39;sentiment pred mymodel&#39;] = (pos_prob &gt; 0.5).astype(int) sstmodel = AutoModelForSequenceClassification.from_pretrained(FINETUNED_SST) sstprobs = get_probs(sstmodel, holdout_dataloader) holdout_reviews[&#39;positive prob sstmodel&#39;] = sstprobs[:, 1] holdout_reviews[&#39;sentiment pred sstmodel&#39;] = (sstprobs[:, 1] &gt; 0.5).astype(int) . holdout_reviews[&#39;sentiment&#39;] = (holdout_reviews[&#39;rating&#39;] &gt; 3).astype(int) . pd.crosstab(holdout_reviews[&#39;sentiment pred mymodel&#39;], holdout_reviews[&#39;sentiment&#39;]) . sentiment 0 1 . sentiment pred mymodel . 0 11 | 1 | . 1 1 | 4 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pd.crosstab(holdout_reviews[&#39;sentiment pred sstmodel&#39;], holdout_reviews[&#39;sentiment&#39;]) . sentiment 0 1 . sentiment pred sstmodel . 0 7 | 4 | . 1 5 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Clearly the results are way better for our model than the model fine-tuned on SST2. The latter does much worse on these held out reviews than for the generic reviews. I swear I didn&#39;t cherry pick them to make our model look good! But I did pick some of them because they seemed like interesting examples that are particular to the context of podcast reviews. . Let&#39;s go over some of the reviews to see why they are interesting examples. . First there are two reviews for two different horror themed podcasts. I wondered if the distilBERT model would learn to classify them as positive even though they use what would be considered negative language in other context, and it appears to have worked! . holdout_reviews.loc[[11204, 11211], [&#39;review&#39;, &#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]] . review rating positive prob mymodel positive prob sstmodel polarity score . 11204 The real stuff.... Genuinely disturbing horror... | 5 | 0.948132 | 0.424419 | -0.9390 | . 11211 Best scare ever!. It sounds strange but I alwa... | 5 | 0.984157 | 0.065295 | -0.9027 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; holdout_reviews.loc[11204, &#39;review&#39;] . &#39;The real stuff.... Genuinely disturbing horror! nThese are &#34;take out one of the earbuds&#34; scary, the narration kills any disconnect you may have started with, SN makes you THERE! nMOAR!&#39; . holdout_reviews.loc[11211, &#39;review&#39;] . &#34;Best scare ever!. It sounds strange but I always listen to horror stories through my headphones to help me fall asleep at night. But Knifepoint stories have literally kept me up all night hiding under the covers. I can&#39;t get enough of these terrifying stories!!&#34; . On the next review both models agree but it really illustrates one of the issues with VADER. While the two distilBERT models are confident the review is negative, VADER gives it a high positive score because it contains the word &quot;like&quot; many times: . holdout_reviews.loc[956562, &#39;review&#39;] . &#39;Like like like like like like like like like. I love the concept of this podcast - but just listening to 15 minutes I really couldn‚Äôt stand how many times all of the participants said LIKE. Literally unlistenable unless you want to hear a podcast that is 50% LIKE and 50% actual content.&#39; . holdout_reviews.loc[956562, [&#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]] . rating 1 positive prob mymodel 0.001567 positive prob sstmodel 0.008399 polarity score 0.9702 Name: 956562, dtype: object . Next there are two reviews discussing sound issues. Because this is a common complaint with podcasts, one might hypothesize that our model has learned that discussing the sound is usually associated with a negative rating. The results were mixed. The second review is arguably the harder case and our model gets it right (and the other model is extremely confident in its incorrect prediction). However, for some reason our model predicts that the first review is positive, albeit not with high confidence. Maybe &quot;sound&quot; is usually used in these critical reviews rather than &quot;volume&quot;. We can&#39;t draw conclusions from just two reviews, of course. . holdout_reviews.loc[[9, 123052], [&#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]] . rating positive prob mymodel positive prob sstmodel polarity score . 9 1 | 0.655020 | 0.004309 | 0.4749 | . 123052 1 | 0.209684 | 0.933054 | 0.9515 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; holdout_reviews.loc[9, &#39;review&#39;] . &#39;Volume???. Great podcast, but the editors turn the volume down for the talks. The intros are loud, then you have to crank up the volume for the talk.&#39; . holdout_reviews.loc[123052, &#39;review&#39;] . &#34;Want to love it. I love Colbert. And I really wanted to love this podcast. But I don&#39;t always listen to podcasts in a library where it&#39;s super quiet. The sound needs to be fixed so I can hear it while I&#39;m going about my daily life. If they fix the sound I will definitely download it again.&#34; . Then there is a review complaining about something to do with politics. This is a very common theme in 1 star ratings. As we might have expected, our model is much more confident of the review being negative than the one fine-tuned on SST2. The latter model usually assigns probabilities very close to 0 or 1, so 0.2 is pretty low confidence for that model. . VADER gets it completely wrong, presumably because of the word &quot;best&quot; and despite the word &quot;too&quot;. . holdout_reviews.loc[2566, [&#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]] . rating 1 positive prob mymodel 0.004463 positive prob sstmodel 0.20247 polarity score 0.6369 Name: 2566, dtype: object . holdout_reviews.loc[2566, &#39;review&#39;] . &#39;Too Political. Talk about food. That‚Äôs what you do best.&#39; . The following review contains mostly positive language (talking about how great the podcast used to be) but the word &quot;unsubscribe&quot;. Sure enough, our model was very confident that it is a negative review whereas the SST2 distilBERT and VADER predicted positive. . holdout_reviews.loc[14947, [&#39;review&#39;, &#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]] . review Unsubscribe. Was a huge supporter of the pod a... rating 1 positive prob mymodel 0.001306 positive prob sstmodel 0.506458 polarity score 0.6937 Name: 14947, dtype: object . Finally, the following positive review seems like a really hard one to classify and it&#39;s impressive that our fine-tuned distilBERT got it right! By contrast, the SST2 distilBERT and VADER were very confident that it is negative. See for yourself: . holdout_reviews.loc[48339, [&#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]] . rating 5 positive prob mymodel 0.932284 positive prob sstmodel 0.005084 polarity score -0.944 Name: 48339, dtype: object . holdout_reviews.loc[48339, &#39;review&#39;] . &#34;This episode...all about failure.... Wow! I‚Äôm in tears! My first podcast review but it had to be done because this podcast spoke to me! n n I‚Äôve spent a lot of time lately feeling bad about things I‚Äôve missed because I didn&#39;t lose this weight sooner, didn‚Äôt devote more time to my business sooner, didn‚Äôt figure out a way to get my irritability in check so my kids don‚Äôt have a mom that yells - could‚Äôve, would&#39;ve should‚Äôve...these thoughts rotate through my head daily and make me feel terrible about myself and my life. But you know what bows the time. I‚Äôm not going to sit with regrets any longer!&#34; . Here is the whole holdout dataframe. I mostly went over the reviews in which the distilBERT models disagree but you can see that they also agree in many cases. They are both generally superior to VADER. . holdout_reviews[[&#39;review&#39;, &#39;rating&#39;, &#39;positive prob mymodel&#39;, &#39;positive prob sstmodel&#39;, &#39;polarity score&#39;]].head(17) # Making sure all 17 rows are shown . review rating positive prob mymodel positive prob sstmodel polarity score . 9 Volume???. Great podcast, but the editors turn... | 1 | 0.655020 | 0.004309 | 0.4749 | . 10 America‚Äôs Forgotten Working Class. This episod... | 5 | 0.939482 | 0.995457 | -0.7269 | . 76 One-dur-ful. **Warning** nIf you listen to the... | 5 | 0.108636 | 0.335130 | -0.1779 | . 283 Rebroadcasts after rebroadcasts. This used to ... | 1 | 0.008035 | 0.625919 | 0.7447 | . 973 Everything else is better. I listen to a few c... | 1 | 0.007977 | 0.003618 | 0.7311 | . 1516 How does it work?. Worked great on my old Ipod... | 1 | 0.060916 | 0.001340 | 0.6249 | . 2566 Too Political. Talk about food. That‚Äôs what yo... | 1 | 0.004463 | 0.202470 | 0.6369 | . 11204 The real stuff.... Genuinely disturbing horror... | 5 | 0.948132 | 0.424419 | -0.9390 | . 11211 Best scare ever!. It sounds strange but I alwa... | 5 | 0.984157 | 0.065295 | -0.9027 | . 14947 Unsubscribe. Was a huge supporter of the pod a... | 1 | 0.001306 | 0.506458 | 0.6937 | . 15130 What happened??. Used to love it, but now it‚Äôs... | 1 | 0.009618 | 0.018876 | 0.9773 | . 48339 This episode...all about failure.... Wow! I‚Äôm ... | 5 | 0.932284 | 0.005084 | -0.9440 | . 49428 Entertainment. Not quality.. I think many woul... | 1 | 0.000944 | 0.001380 | 0.9100 | . 123052 Want to love it. I love Colbert. And I really ... | 1 | 0.209684 | 0.933054 | 0.9515 | . 212768 Good show that needs a professional narrator. ... | 1 | 0.494779 | 0.998852 | 0.9200 | . 922494 A different show now.. I loved to old podcast ... | 1 | 0.005331 | 0.884350 | 0.9913 | . 956562 Like like like like like like like like like. ... | 1 | 0.001567 | 0.008399 | 0.9702 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 6. On Model Confidence . Something that jumps out when looking at the distributions of predicted probabilities is that the distilBERT fine-tuned on SST2 is more confident of its predictions than our model. The former mostly assigns probabilities close to 0 and 1 whereas the latter outputs more probabilities in between. . Over time our model also gets more confident but never reaches that level of sharpness, which might be due to our data being more noisy. Below we see the results for a relatively early model, at 6000 steps (0.6 epochs), and the last model at 40,000 steps (4 epochs). . The 6000 steps model is actually not that different to the 17,000 steps model we saw above in terms of the accuracy. However, the recall for negative reviews is significantly higher than for positive reviews, mostly because it classifies many 4 star reviews as 3 star reviews. Another difference is that the histograms are much less concentrated (less &quot;confident&quot;) at 6000 steps. . The 40,000 steps model is clearly overfitting as at that point the evaluation loss has been going up and the training loss went down dramatically. One of the symptoms of this overfitting is the high confidence. The accuracy is actually worse than it was at 17,000 steps, but the histograms are much more concentrated at 0 and 1. . evaluate_and_plot( os.path.join(PATH, &#39;models/best-run/checkpoint-6000&#39;), test_dataloader, reviews_test, ) . {&#39;accuracy&#39;: 0.5662, &#39;recall&#39;: array([0.60588235, 0.50151057, 0.51874367, 0.36597428, 0.84226491])} . evaluate_and_plot( os.path.join(PATH, &#39;models/best-run/checkpoint-40000&#39;), test_dataloader, reviews_test, ) . {&#39;accuracy&#39;: 0.5708, &#39;recall&#39;: array([0.59117647, 0.48539778, 0.44883485, 0.53610287, 0.79271992])} . evaluate_and_plot( os.path.join(PATH, &#39;models/best-run/checkpoint-6000&#39;), test_dataloader, reviews_test, sentiment=True, ) . {&#39;accuracy&#39;: 0.8744, &#39;recall&#39;: array([0.93233333, 0.7875 ])} . evaluate_and_plot( os.path.join(PATH, &#39;models/best-run/checkpoint-40000&#39;), test_dataloader, reviews_test, sentiment=True, ) . {&#39;accuracy&#39;: 0.8762, &#39;recall&#39;: array([0.895, 0.848])} . &lt;/div&gt; .",
            "url": "david-recio.com/2022/10/21/bert-fine-tune-podcast-reviews.html",
            "relUrl": "/2022/10/21/bert-fine-tune-podcast-reviews.html",
            "date": " ‚Ä¢ Oct 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "The Titanic has a Leak",
            "content": "from collections import defaultdict import pandas as pd import numpy as np from sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold, StratifiedGroupKFold from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.pipeline import Pipeline from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import IterativeImputer from sklearn.preprocessing import OrdinalEncoder import xgboost as xgb import networkx as nx import matplotlib.pyplot as plt . . What is leakage? . Training a machine learning model to make predictions is tricky (especially about the future!). One of the main issues is overfitting: If left unchecked, models will tend to fit the training data too specifically in a way that doesn&#39;t actually generalize to &quot;future data&quot;. . This is why we always set aside a subset of the data (the test set) to evaluate the model predictions. The model never gets to see the test data during training, to simulate new data like the data the model will have to deal with when in real deployment. At first this seems like a foolproof method to gauge how well the model will do in practice, assuming that the &quot;future data&quot; in the context the model needs to operate arises from the same probability distribution as our current data (that is a whole other issue, see data drift). . However, in practice there might be unintended correlations between the test set and the data we used to train the model (the training set). Those correlations might allow us to make predictions based on information which we wouldn&#39;t actually have access to at prediction time in reality. We call this phenomenon data leakage, because &quot;future&quot; information is accidentally leaking from the test set to the training set. This can lead to dramatically overestimating the true model performance. Even worse, the model could end up mostly relying on the leakage for predictions, to the detriment of legitimate signals. This would make it essentially useless in a real deployment. . This somewhat abstract description will become clearer once we look at a specific instance of leakage. . Where is the leakage? . In the case of the famous Titanic competition there is a major source of information leakage. Groups of women and children traveling together tend to either all live or all die, simply because they tended to stay together. Women and children were famously prioritized on the lifeboats, while adult men were separated from their families. . This is an instance of data leakage because we wouldn&#39;t have known which families were going to survive before the Titanic sank, yet it provides us with a lot of information on passenger survival in our data. Strictly speaking, it is open to debate what exactly constitutes leakage in a one-time event such as in the Titanic disaster. However, if we imagine having to make a prediction about another ship sinking in similar circumstances, it seems unreasonable to assume that we would have information on which families would survive beyond the information on which passengers would survive. In contrast to survival per passenger class, for instance, family survival is seems to be subject to random events in a way that is not generalizable (what could be called random noise). . Why does the leakage matter? . I am not the first one to point out that family survival is a major predictor of individual passenger survival. As far as I know, however, the extent and importance of the leakage has not been thoroughly investigated yet. . There is no doubt that using the leakage gives an important advantage in the Kaggle Titanic competition. Most passenger groups (which we will specify below) have in fact been separated in the train-test split. This is not a particular characteristic of the Kaggle test set. Instead, it is statistically a near-certainty given the number of groups and the group sizes, as long as the test set makes up a third of the data. . We can distinguish three ways in which the leakage has been used on the Kaggle competition over the years: . Chris Deotte and others have already observed that one can get very good results (better than many sophisticated approaches) just by making predictions with a very simple rule using family survival directly (but not explicitly identifying it as an example of data leakage, to the best of my knowledge). The rule is the following: Just predict that all males die and all females live, except for boys whose family survived (who are predicted to live instead) and females whose family died (who are predicted to die instead). We will implement this model below and compare its accuracy with other approaches. | People like Erik Bruin have also noticed that adding &quot;family survival&quot; as an engineered feature helps train models. This basically amounts to target encoding of an engineered &quot;family group feature&quot;, where the groups are replaced by the mean survival of the group within the training set (or NA if the group is not represented in the training set). | Finally, beyond the explicit uses of the data leakage listed above, it is conceivable that many classifiers use it implicitly and inadvertently. Groups traveling together can be identified from features such as passenger class, port of embarkation, ticket number, cabin number, number of family members on board (children, parents, siblings, spouses). The &#39;Age&#39; feature as well as the engineered &quot;title feature&quot; can be used to identify children. | . I want to make clear that I am not suggesting any of the above constitutes cheating. It doesn&#39;t go against the rules of the competition (I believe) and, in any case, the Titanic competition is intended for people who are relatively new to machine learning (such as myself) to practice with an interesting problem and a manageable data set. . That said, I think it is interesting to investigate how pervasive this leakage is. That is to say, how much do classifiers rely on this data leakage, whether explicitly or implicitly? And how well can we do without it? . How do we plug the leak? . There is a simple solution. We just need to create an alternative test set which, unlike the Kaggle test set, doesn&#39;t contain any partial passenger groups. In other words, groups are either fully in the test set or fully in the training set. We will call such a train-test split leak-proof. . Getting slightly more sophisticated, we want to perform multiple train-test leak-proof splits. Taking the mean accuracy over multiple splits allows us to reduce the error in estimating the model performance. This process is called cross-validation (often abbreviated &quot;CV&quot;). . We will go into more detail below. . Warning: To do our custom train-test splits we need the full survival outcomes for all passengers. While this is publicly available data, I was reluctant to make it even easier to find it for people competing in the Kaggle competition. However, I think that on balance it is worth exploring the questions considered here, at the risk of potentially making the full data easier to access. Frankly, it seems that the cat is out of the bag, considering the large number of perfect scores on the leaderboard. In fact, the place where I found the full data is actually on Kaggle! There was an external link to the data surreptitiously hidden at the end of a long notebook, where it was quietly used to make a submission (with a perfect score, needless to say). . To avoid &quot;spoilers&quot; about the Kaggle test set, I will try to reveal as little specifics on the test set as possible. The overall survival rate does appear later on, but that can be figured out easily by submitting an &quot;everyone dies&quot; prediction. . # This is the full data with the survival outcomes of all passengers full_data = pd.read_csv(&#39;full.csv&#39;) X = full_data.drop(columns=&#39;Survived&#39;) y = full_data[&#39;Survived&#39;].copy() kaggle_train_idx = pd.RangeIndex(891) kaggle_test_idx = pd.RangeIndex(891, 1309) . . Here is what we will do . We will address two questions: . If we prevent leakage using leak-proof train-test splits as mentioned above, how well can we predict passenger survival? More concretely, could we do better than a simple baseline if we can&#39;t use the leakage? | How do we estimate to which extent a specific classifier is &quot;secretly&quot; using the leakage? | . For the first point, we will create an &quot;exemplary&quot; classifier and evaluate it using a leak-proof cross-validation. We will compare its accuracy to that of some simple baselines, to see how well it does once the leakage has been prevented. For the classifier we will use the popular XGBoost classifier. To get the most out of the classifier, we will also perform feature engineering and hyperparameter tuning. . The second point is actually hard to answer definitively. We will compare the accuracy of a classifier under a leak-proof cross-validation and a &quot;leaky&quot; cross-validation (i.e. a regular cross-validation in which the groups can be divided in the train-test split). If a classifier does worse under the former than under the latter, this suggests that the classifier was using the leakage. However, we need to be cautious about drawing conclusions just from that, because the leak-proof train-test splits might make it &quot;harder&quot; to generalize from the training set to the test set in more subtle ways. To dig a little deeper, we will compare the prediction accuracy on the solo travelers, i.e. passengers who are traveling alone (to the best of our knowledge). . Outline: . We will start by implementing the baseline classifiers and the XGBoost classifier. | Then, we will implement the cross-validation function, which will involve a nested cross-validation because of the hyperparameter search. | Finally, we will compute the accuracy of our classifier and of the baselines under various cross-validation schemes and discuss the results. | . Baselines . Enhanced gender model . Is is well-known that &#39;Sex&#39; is the most important feature, given that over $70 %$ of females survived, while less than $20 %$ of males did. This suggests the following rule: females should be predicted to survive and males to perish. This results in the gender model, the most commonly used baseline and provided by Kaggle as the example submission. . However, looking at the following table we can easily improve upon that baseline. (Note that I restricted the table to the Kaggle training set to avoid &quot;spoilers&quot; about the test set.) . full_data.loc[kaggle_train_idx].groupby([&#39;Sex&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;])[&#39;Survived&#39;].agg([&#39;mean&#39;, &#39;count&#39;]) . mean count . Sex Pclass Embarked . female 1 C 0.976744 | 43 | . Q 1.000000 | 1 | . S 0.958333 | 48 | . 2 C 1.000000 | 7 | . Q 1.000000 | 2 | . S 0.910448 | 67 | . 3 C 0.652174 | 23 | . Q 0.727273 | 33 | . S 0.375000 | 88 | . male 1 C 0.404762 | 42 | . Q 0.000000 | 1 | . S 0.354430 | 79 | . 2 C 0.200000 | 10 | . Q 0.000000 | 1 | . S 0.154639 | 97 | . 3 C 0.232558 | 43 | . Q 0.076923 | 39 | . S 0.128302 | 265 | . We see that there is one glaring exception to the rule: Females who traveled in 3rd class and who boarded in Southampton have a survival rate of under $40 %$ (and thus should be predicted to die). That actually applies to a pretty large number of passengers, namely 129 in the whole data set. . We will slightly modify the gender baseline by adding the exception that females are predicted not to survive if they are in 3rd class and they embarked in Southampton (&#39;Embarked&#39; is &#39;S&#39;). Let&#39;s call this the enhanced gender model. . Note that this model does not make use of the leakage and thus we will think of it as the leakage-free baseline. . Group survival model . On the other hand, what we will call the group survival model makes full use of the leakage. It is in fact an extension of the gender model which has already been implemented by Chris Deotte. The rule here is to predict that females survive unless all women and children in their group died (within the training set) and that boys die unless all women and children in their group survived (within the training set). . We will implement all three baseline models mentioned above as custom classifiers following the scikit-learn API. . class GenderClassifier(BaseEstimator, ClassifierMixin): def fit(self, X, y, groups): pass def predict(self, X): return (X[&#39;Female/Boy/Man&#39;] == 0).astype(int) class EnhancedGenderClassifier(BaseEstimator, ClassifierMixin): def fit(self, X, y, groups): pass def predict(self, X): is_woman = X[&#39;Female/Boy/Man&#39;] == 0 # This assumes that 2 corresponds to &#39;S&#39;! exceptions = (X[&#39;Embarked Encoded&#39;] == 2) &amp; (X[&#39;Pclass&#39;] == 3) return (is_woman &amp; ~exceptions).astype(int) class GroupSurvivalClassifier(BaseEstimator, ClassifierMixin): def __init__(self): self.survival_rates_ = None def fit(self, X, y, groups): is_man = X[&#39;Female/Boy/Man&#39;] == 2 self.survival_rates_ = y[~is_man].groupby(X.loc[~is_man, &#39;Group&#39;]).agg([&#39;mean&#39;, &#39;count&#39;]) def predict(self, X): if self.survival_rates_ is None: raise Exception(&#39;The model needs to be fitted first.&#39;) is_female = (X[&#39;Female/Boy/Man&#39;] == 0).reset_index(drop=True) is_boy = (X[&#39;Female/Boy/Man&#39;] == 1).reset_index(drop=True) family_survived = pd.merge(X, self.survival_rates_, how=&#39;left&#39;, on=&#39;Group&#39;)[&#39;mean&#39;] female_survivors = is_female &amp; (family_survived != 0) boy_survivors = is_boy &amp; (family_survived == 1) return (female_survivors | boy_survivors).astype(int) . Our &quot;exemplary&quot; model . Feature engineering and categorical encoding . Before feeding the data to our classifier we will engineer some new features, which are inspired by what I have read in various public notebooks on Kaggle. . The categorical features need to be encoded into numerical features. We will use ordinal encoding rather than one-hot encoding, given that this seems to be preferable for tree-based models. . embarked_encoder = OrdinalEncoder() X[&#39;Embarked Encoded&#39;] = embarked_encoder.fit_transform(X[[&#39;Embarked&#39;]]) . As we have discussed, male survival depends largely on being a boy or being an adult male. To be precise, by boy we mean anyone with the title &#39;Master&#39;. We create a new ordinal feature in which female is encoded as 0, boy as 1, and adult male as 2. Note that we ordered the categories in terms of descending survival rates. . X[&#39;Is Boy&#39;] = X[&#39;Name&#39;].str.contains(&#39;Master.&#39;).astype(int) X[&#39;Female/Boy/Man&#39;] = 2 * (X[&#39;Sex&#39;] == &#39;male&#39;) - X[&#39;Is Boy&#39;] . For shared tickets it seems that the ticket price is the total for all ticket holders. It seems more meaningful to compare the price per passenger than the total, which is why we engineer a &#39;Fare per Passenger&#39; feature. . X[&#39;Passengers per Ticket&#39;] = X.groupby(&#39;Ticket&#39;)[&#39;Ticket&#39;].transform(&#39;count&#39;) X[&#39;Fare per Passenger&#39;] = X[&#39;Fare&#39;] / X[&#39;Passengers per Ticket&#39;] . We combine the number of siblings and spouses with the number of parents and children in one &quot;Family Size&quot; feature. . X[&#39;Family Size&#39;] = X[&#39;SibSp&#39;] + X[&#39;Parch&#39;] + 1 . We extract the &#39;Deck&#39; from the &#39;Cabin&#39; feature. Note that the deck is given by the letter at the beginning of the cabin number. When encoding the Deck feature we make sure that NA gets encoded as its own category, rather than filled in later on. This is because a significant number of cabin numbers are missing (which makes the imputation harder) and also because the fact that the cabin number is missing might contain some information in itself. . X[&#39;Deck&#39;] = X[&#39;Cabin&#39;].str[0] deck_encoder = OrdinalEncoder() X[&#39;Deck Encoded&#39;] = deck_encoder.fit_transform(X[[&#39;Deck&#39;]].fillna(&#39;missing&#39;)) . We truncate the ticket number by removing the last two digits. This is essentially a form of binning, in that it reduces the cardinality of the &#39;Ticket&#39; feature and groups together similar tickets. . X[&#39;Truncated Ticket&#39;] = X[&#39;Ticket&#39;].str[:-2] ticket_encoder = OrdinalEncoder() X[&#39;Truncated Ticket Encoded&#39;] = ticket_encoder.fit_transform(X[[&#39;Truncated Ticket&#39;]]) . Finally, we select those features we will use for the classification. . X = X[[&#39;Female/Boy/Man&#39;, &#39;Age&#39;, &#39;Family Size&#39;, &#39;Fare per Passenger&#39;, &#39;Pclass&#39;, &#39;Deck Encoded&#39;, &#39;Truncated Ticket Encoded&#39;, &#39;Embarked Encoded&#39;]] . XGBoost classifier and hyperparameter tuning . We will use the popular classifier from the library XGBoost. . To tune the hyperparameters we will use the GridSearchCV function from scikit-learn, which evaluates all hyperparameter combinations in a &quot;parameter grid&quot; using cross-validation and selects the hyperparameters resulting in the highest mean accuracy. . Before feeding the data to the classifier we need to fill in the missing values. We will use an iterative imputer, which fills in the missing values of each feature based on the remaining features. Because the imputation step depends on the training data, it needs to be repeated for each new train-test split. This is why we will concatenate the imputer with the classifier in a scikit-learn pipeline. . iterative_imputer = IterativeImputer() xgbclassifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric=&#39;error&#39;, booster=&#39;gbtree&#39;) xgb_pipe = Pipeline(steps=[(&#39;iterative_imputer&#39;, iterative_imputer), (&#39;xgbclassifier&#39;, xgbclassifier)]) . We will use the following hyperparameter grid. . param_grid = { &#39;xgbclassifier__gamma&#39;: [0, 0.1, 0.2], &#39;xgbclassifier__subsample&#39;: [0.9, 1], &#39;xgbclassifier__max_depth&#39;: [4, 5, 6], &#39;xgbclassifier__learning_rate&#39;: [0.05, 0.1, 0.2], &#39;xgbclassifier__n_estimators&#39;: [40, 50, 60], &#39;xgbclassifier__max_delta_step&#39;: [0, 1, 5], &#39;xgbclassifier__reg_alpha&#39;: [1, 2, 3], } . Cross-validation . Now that we have implemented the classifiers, we need to set up the cross-validation schemes. . Recall that the rationale behind cross-validation is to reduce the error in estimating model performance with a single train-test split, by averaging the scores across multiple train-test splits. Intuitively, in a single train-test split we might get unlucky and have a particularly &quot;easy&quot; (or &quot;hard&quot;) test set, which results in a misleading accuracy score. Taking multiple splits both mitigates this risk and actually allows us to estimate the error (by looking at the variation across splits). . In k-fold cross-validation a model is trained and evaluated $k$ times on different train-test splits, where the different test sets have the same size (or at most differ by 1), are disjoint and together make up the whole data set. Each of these test sets is called a fold. . As explained above, to prevent the data leakage we need to perform splits which don&#39;t break up any of the groups of passengers traveling together (mostly families). In general, this is called group k-fold cross-validation. In this context we will refer to it as leak-proof cross-validation. In contrast, we will refer to regular (group agnostic) cross-validation as leaky cross-validation because it enables leakage. . First of all we need to figure out, to a sufficient approximation, which passengers are in fact traveling together. . Identifying the groups . This section was heavily influenced by the Kaggle notebooks of Chris Deotte, Erik Bruin, and others. . We want to identify the groups of passengers traveling together, who where likely to stay together during the sinking. This might require going beyond the closest family members (meaning those who have the same surname). One way to find extended families is to extract maiden names, which allows us to connect a married woman to her sister or mother, for instance. Even further, we can break apart double surnames (&quot;Kink-Heilmann&quot; or &quot;Penasco y Castellana&quot;) to discover as many family relations as possible. . Beyond the &#39;Name&#39; feature, we will also use the ticket numbers to capture groups of friends or more distant family members traveling together. . Because our aim is to minimize the leakage caused by groups of travelers being split across training set and test set, we want to make sure that we don&#39;t miss too many connections. This means that we want to catch as many real groups as possible, even at the cost of some false positives, while making sure the group sizes seem reasonable. To reduce the risk of false positives, we will only allow groups in which all passengers are traveling in the same class. This seems like a reasonable assumption for passengers sticking together. . With all this in mind, this is how we will form the groups: . If the surname of one passenger is the same as the surname, middle name, or any part of a double surname of another passenger, we put those passengers in the same group, provided they are in the same passenger class. | If the ticket numbers of two passengers agree, the passengers are in the same group. (Passenger with the same ticket number are automatically in the same passenger class.) | . Following the rules above, some groups will contain passengers who are only indirectly connected through a chain of direct connections. In more mathematical language: the passengers are vertices in a graph) and the rules above determine when two vertices are connected by an edge. From this perspective, the passenger groups are the connected components of the graph. We will use the networkx library to find the passenger groups (connected components). . Note: The approach outlined above seems to lead to pretty sensible groups. There are probably a couple of false connections but nothing dramatic. That said, it would be interesting to explore the effects that changes in how the connections are formed have on the results below. There is the option of a much more targeted approach, where we painstakingly go over the groups and discover false connections by taking into account the &#39;Parch&#39; (number of parents and children on board) and &#39;SibSp&#39; (number of siblings and spouses on board) features and even by searching some information on the passengers online. Furthermore, we decided to compare the whole ticket numbers of passengers, but some people have pointed out that the last digits of the ticket numbers sometimes vary for families traveling together. When I tried to compare the tickets numbers ignoring the last digit, this seemed to lead to too large groups (likely many false positives). However, in a more targeted approach it might be worth trying this out. . In the following we extract the surnames and maiden names from the &#39;Name&#39; feature. We also break apart the double surnames, being careful about false positives (names starting with de, del, Van, Vander, and so on). . full_data[&#39;Surname&#39;] = full_data[&#39;Name&#39;].str.extract(&#39;(.+),&#39;) # Maiden names are given in brackets in the &#39;Name&#39; column # Note that we require a space before the start of the maiden name # to exclude nicknames, which are usually a single word in brackets full_data[&#39;Maiden Name&#39;] = full_data[&#39;Name&#39;].str.extract(r&#39; ([A-Za-z]+) )&#39;) # Double surnames are separated by hyphens, spaces and &#39;y&#39; (some Spanish surnames). # We need to exclude some false positives in French, Dutch and Spanish names # starting with de, del, Van, Vander, and so on. has_double_surname = full_data[&#39;Surname&#39;].str.contains(&#39;-| &#39;) false_positives = full_data[&#39;Surname&#39;].str.contains(&#39;^(?i)va|^(?i)de&#39;) full_data[[&#39;First Surname&#39;, &#39;Second Surname&#39;]] = full_data[&#39;Surname&#39;].where(has_double_surname &amp; ~false_positives).str.split(&#39;-| y | &#39;, expand=True) full_data[&#39;First Surname&#39;] = full_data[&#39;First Surname&#39;].fillna(full_data[&#39;Surname&#39;]) . Now we use networkx to determine the groups according to the rules we outlined above. . connected_components = [] has_maiden_name = full_data[&#39;Maiden Name&#39;].notna() # for pclass in [1,2,3]: pairings = [] this_class = full_data[&#39;Pclass&#39;] == pclass # Note that for passengers with a simple surname (not a double/composite one) # we filled &#39;First Surname&#39; with the surname and left &#39;Second Surname&#39; as None # This is why we don&#39;t need the &#39;Surname&#39; feature itself in here pairings.append(full_data.loc[this_class &amp; has_maiden_name, [&#39;First Surname&#39;, &#39;Maiden Name&#39;]] .rename(columns={&#39;First Surname&#39;: &#39;first&#39;, &#39;Maiden Name&#39;: &#39;second&#39;}) ) pairings.append(full_data.loc[this_class &amp; has_double_surname &amp; ~false_positives, [&#39;First Surname&#39;, &#39;Second Surname&#39;]] .rename(columns={&#39;First Surname&#39;: &#39;first&#39;, &#39;Second Surname&#39;: &#39;second&#39;}) ) pairings.append(full_data.loc[this_class, [&#39;First Surname&#39;, &#39;Ticket&#39;]] .rename(columns={&#39;First Surname&#39;: &#39;first&#39;, &#39;Ticket&#39;: &#39;second&#39;}) ) edges = pd.concat(pairings) graph = nx.from_pandas_edgelist(edges, &#39;first&#39;, &#39;second&#39;) connected_components.extend(nx.connected_components(graph)) connected_components.sort(key=len, reverse=True) group_dict = dict(enumerate(connected_components)) full_data[&#39;Group&#39;] = full_data[&#39;Ticket&#39;].apply(lambda ticket: next(group_idx for group_idx, group in enumerate(connected_components) if ticket in group)) . Let&#39;s have a look at the sizes of the largest groups below: . full_data[&#39;Group&#39;].value_counts().values.tolist()[:20] . [16, 16, 13, 11, 11, 10, 10, 10, 10, 9, 8, 8, 8, 8, 7, 7, 7, 6, 6, 6] . A couple group sizes are a little large but overall, they are just a handful out of 232 groups total (this number excludes solo travelers; if each solo traveler counted as their own group, there would be a total of 768 groups). The results appear reasonable considering that we are grouping second degree relatives and even separate families traveling together (those sharing a ticket number). There are probably some false positives but our priority is to avoid leakage. . Cross-validation splits . Thankfully, there are two scikit-learn functions which we can use to create the splits, or folds. For the leaky cross-validation we will use StratifiedKFold and for the leak-proof cross-validation (the one that doesn&#39;t break apart groups) StratifiedGroupKFold. We use stratified splits in both cases. This is because the data is imbalanced ($38 %$ of passengers survived) and the stratified splits ensure that all folds have similar survival rates. . Before going further, we will check that the leak-proof folds are reasonably representative in terms of the various categorical features. This is to rule out other reasons why the accuracy might go down for leak-proof CV with respect to leaky CV. . X_raw = full_data.drop(columns=&#39;Survived&#39;) groups = full_data[&#39;Group&#39;].copy() sgkf = StratifiedGroupKFold(n_splits=5) ratios = defaultdict(list) for train, test in sgkf.split(X_raw, y, groups=groups): ratios[&#39;survival&#39;].append(y[test].mean()) ratios[&#39;female&#39;].append((X_raw.loc[test, &#39;Sex&#39;] == &#39;female&#39;).mean()) ratios[&#39;first class&#39;].append((X_raw.loc[test, &#39;Pclass&#39;] == 1).mean()) ratios[&#39;second class&#39;].append((X_raw.loc[test, &#39;Pclass&#39;] == 2).mean()) ratios[&#39;third class&#39;].append((X_raw.loc[test, &#39;Pclass&#39;] == 3).mean()) ratios[&#39;embarked S&#39;].append((X_raw.loc[test, &#39;Embarked&#39;] == &#39;S&#39;).mean()) ratios[&#39;embarked Q&#39;].append((X_raw.loc[test, &#39;Embarked&#39;] == &#39;Q&#39;).mean()) ratios[&#39;embarked C&#39;].append((X_raw.loc[test, &#39;Embarked&#39;] == &#39;C&#39;).mean()) pd.DataFrame(ratios) . . survival female first class second class third class embarked S embarked Q embarked C . 0 0.381679 | 0.351145 | 0.259542 | 0.213740 | 0.526718 | 0.709924 | 0.068702 | 0.221374 | . 1 0.381679 | 0.351145 | 0.263359 | 0.213740 | 0.522901 | 0.709924 | 0.091603 | 0.190840 | . 2 0.381679 | 0.381679 | 0.251908 | 0.206107 | 0.541985 | 0.656489 | 0.125954 | 0.217557 | . 3 0.381679 | 0.377863 | 0.221374 | 0.270992 | 0.507634 | 0.755725 | 0.083969 | 0.160305 | . 4 0.383142 | 0.318008 | 0.237548 | 0.153257 | 0.609195 | 0.659004 | 0.099617 | 0.241379 | . Each row corresponds to a fold (test set for each split). Fortunately, the folds appear quite similar to each other. . The one somewhat significant variation is for the &#39;Embarked&#39; feature. This is particularly noticeable for the least frequent value &#39;Q&#39;, because relatively few passengers embarked in Queenstown. This is something to keep in mind but probably not a big deal. . Nested cross-validation . To tune the hyperparameters we will use the GridSearchCV class from scikit-learn, which evaluates all hyperparameter combinations we provide (in the form of a &quot;parameter grid&quot;) using cross-validation. The hyperparameters resulting in the highest mean accuracy are selected for the final model. . When evaluating our classifier we need to repeat the hyperparameter search for each split, because the resulting hyperparameters depend on the training data to some extent. This means that the grid search cross-validation needs to be nested inside the main cross-validation, which we use to evaluate the model. For obvious reasons this is called a nested cross-validation. . Even though scikit-learn has support for group cross-validation, we hit a roadblock with the nested cross-validation. To the best of my knowledge, it is currently not possible to pass the groups to the inner cross-validation within an outer cross-validation using the function cross_validate. However, if we tune the hyperparameters of the XGBoost classifier using a leaky cross-validation and then evaluate the model accuracy using leak-proof cross-validation, we might underestimate the classifier&#39;s potential because the hyperparameters were specifically tuned for leaky train-test splits. To be able to have an inner leak-proof cross-validation will need to write a custom cross-validation function which can pass the groups through to the inner cross-validation. . The second reason we want a custom cross-validation function is to compute the accuracy for the solo travelers (in addition to the accuracy for all passengers). Solo travelers are those whose group has a size of &quot;1&quot;. The hope is that looking at the accuracy on those passengers (which should be unaffected by the leakage) will help shed some light on whether a particular classifier is relying on the leakage. . Now we implement the custom cross-validation function, which goes through the provided train-test splits (in the form of a StratiedKFold or StratifiedGroupKFold object), trains the classifier on each training set, computes the overall accuracy on each test set, and finally computes the accuracy on the solo travelers in each test set. When the classifier is a GridSearchCV object, the passenger groups are passed to it through its fit method. The function returns two NumPy arrays, one with the overall accuracy scores and one with the accuracy scores restricted to the solo travelers. . def custom_cross_validate(classifier, X, y, outer_cv, groups): &#39;&#39;&#39;Almost the same as sklearn&#39;s cross_validate, but when classifier is a GridSearchCV object, it passes the groups to it. This allows us to make the inner CV leak-proof. It also computes the accuracy of the predictions for solo passengers, as an additional &quot;solo score&quot;.&#39;&#39;&#39; solo_groups = groups.value_counts()[groups.value_counts() == 1].index scores = [] solo_scores = [] count = 1 total = outer_cv.n_splits for train_idx, test_idx in outer_cv.split(X, y, groups): X_train, X_test = X.loc[train_idx], X.loc[test_idx] y_train, y_test = y[train_idx], y[test_idx] classifier.fit(X_train, y_train, groups=groups[train_idx]) solo_travelers = groups.isin(solo_groups)[test_idx] score = classifier.score(X_test, y_test) solo_score = classifier.score(X_test[solo_travelers], y_test[solo_travelers]) scores.append(score) solo_scores.append(solo_score) # print(f&#39;Completed fold {count}/{total}&#39;) count += 1 return np.array(scores), np.array(solo_scores) . How well does our classifier do when we prevent leakage? . We will now try to do as well as we can in a leak-free CV with our XGBoost model. This will demonstrate how far we can go in the absence of leakage. . Because the (outer) evaluation CV is leak-free, we will also use a leak-free CV for the (inner) hyperparameter grid search CV. Additionally, we will drop the truncated ticket feature because it is unlikely to help in the absence of leakage, since it mostly identifies passenger groups. . group_inner_cv = StratifiedGroupKFold(n_splits=10) group_outer_cv = StratifiedGroupKFold(n_splits=5) X_woticket = X.drop(columns=&#39;Truncated Ticket Encoded&#39;) . Now let&#39;s evaluate the XGBoost classifier and the baseline models with a leak-free CV. . classifier = GridSearchCV(estimator=xgb_pipe, param_grid=param_grid, cv=group_inner_cv) xgb_scores, _ = custom_cross_validate(classifier, X_woticket, y, outer_cv=group_outer_cv, groups=groups) gender_scores, _ = custom_cross_validate(GenderClassifier(), X_woticket, y, outer_cv=group_outer_cv, groups=groups) enhanced_gender_scores, _ = custom_cross_validate(EnhancedGenderClassifier(), X_woticket, y, outer_cv=group_outer_cv, groups=groups) group_survival_scores, _ = custom_cross_validate(GroupSurvivalClassifier(), X_woticket.join(groups), y, outer_cv=group_outer_cv, groups=groups) . results = pd.DataFrame({&#39;XGBoost&#39;: xgb_scores, &#39;Gender Baseline&#39;: gender_scores, &#39;Enhanced Gender Baseline&#39;: enhanced_gender_scores}) results . XGBoost Gender Baseline Enhanced Gender Baseline . 0 0.809160 | 0.793893 | 0.770992 | . 1 0.862595 | 0.801527 | 0.832061 | . 2 0.797710 | 0.755725 | 0.809160 | . 3 0.847328 | 0.767176 | 0.812977 | . 4 0.793103 | 0.781609 | 0.777778 | . results.mean() . XGBoost 0.821979 Gender Baseline 0.779986 Enhanced Gender Baseline 0.800594 dtype: float64 . The XGBoost classifier does significantly better than the baselines most of the time, even when we prevent it from using group survival for prediction! . It is of course possible that we missed some passenger connections. Considering the low threshold we used to make the connections, it seems unlikely that we missed anything significant. But it would be worth investigating further. . Does our model rely on leakage if we give it the chance? . To that end we will compare the accuracy of our model under a leaky CV to its accuracy under a leak-free CV, as well as to the baselines. . In contrast to the last section, we will tune the hyperparameters with a leaky CV, even when we evaluate it with a leak-proof CV (in other words, the inner CV is leaky even when the outer CV is leak-proof). This is because now we want to &quot;allow&quot; our model to use the leakage during training (to see to which extent the model actually ends up relying on it), whereas before we just wanted to get the most out of our model in the leak-proof case. . Out of the features we are using, ticket and family size are the most likely ways in which the classifier could &quot;identify the groups&quot;. Because of this, we will train and evaluate the classifier with and without those features, in case it allows us to gain some insight into how the model uses the leakage. . We will add all the results to a dictionary and display them in a table (pandas DataFrame). The following helper functions add the results to a dictionary which we will turn into a DataFrame. . def add_leaky_results(results_dic, classifier, X): # print(f&#39;Computing leaky CV scores for n{classifier} with features n{list(X.columns)}&#39;) scores, scores_solo = custom_cross_validate(classifier, X, y, outer_cv=outer_cv, groups=groups) results_dic[&#39;Leaky CV mean&#39;].append(scores.mean()) results_dic[&#39;Leaky CV std&#39;].append(scores.std()) results_dic[&#39;Leaky CV solo mean&#39;].append(scores_solo.mean()) results_dic[&#39;Leaky CV solo std&#39;].append(scores_solo.std()) # print(&#39;- DONE -&#39;) def add_leak_proof_results(results_dic, classifier, X): # print(f&#39;Computing leak-proof CV scores for {classifier} with features {list(X.columns)}&#39;) scores, scores_solo = custom_cross_validate(classifier, X, y, outer_cv=group_outer_cv, groups=groups) results_dic[&#39;Leak-proof CV mean&#39;].append(scores.mean()) results_dic[&#39;Leak-proof CV std&#39;].append(scores.std()) results_dic[&#39;Leak-proof CV solo mean&#39;].append(scores_solo.mean()) results_dic[&#39;Leak-proof CV solo std&#39;].append(scores_solo.std()) # print(&#39;- DONE -&#39;) . . Now we will perform all the cross-validations. . inner_cv = StratifiedKFold(n_splits=10) outer_cv = StratifiedKFold(n_splits=5) results_dic = defaultdict(list) classifiers = [GenderClassifier(), EnhancedGenderClassifier(), GroupSurvivalClassifier()] + 3 * [GridSearchCV(estimator=xgb_pipe, param_grid=param_grid, cv=inner_cv)] X_variations = 2 * [X] + [X.join(groups)] + [X] + [X.drop(columns=&#39;Truncated Ticket Encoded&#39;)] + [X.drop(columns=[&#39;Truncated Ticket Encoded&#39;, &#39;Family Size&#39;])] for classifier, X_variation in zip(classifiers, X_variations): add_leaky_results(results_dic, classifier, X_variation) add_leak_proof_results(results_dic, classifier, X_variation) index = [&#39;Gender model&#39;, &#39;Enhanced gender model&#39;, &#39;Group survival model&#39;, &#39;XGBoost&#39;, &#39;XGBoost no ticket&#39;, &#39;XGBoost no ticket or familiy size&#39;] pd.DataFrame(results_dic, index=index)[[&#39;Leaky CV mean&#39;, &#39;Leaky CV solo mean&#39;, &#39;Leak-proof CV mean&#39;, &#39;Leak-proof CV solo mean&#39;]] . . Leaky CV mean Leaky CV solo mean Leak-proof CV mean Leak-proof CV solo mean . Gender model 0.779980 | 0.796510 | 0.779986 | 0.796434 | . Enhanced gender model 0.800611 | 0.797908 | 0.800594 | 0.798253 | . Group survival model 0.828867 | 0.796510 | 0.779986 | 0.796434 | . XGBoost 0.817393 | 0.799463 | 0.797555 | 0.792913 | . XGBoost no ticket 0.810526 | 0.785366 | 0.817405 | 0.800289 | . XGBoost no ticket or familiy size 0.803662 | 0.768720 | 0.804413 | 0.794888 | . We will discuss the table above and offer some (speculative) interpretations below. It needs to be noted that some of the observed effects might not be statistically significant and we need to be cautious about drawing conclusions. In the next section we will discuss the statistical significance of the observed effects. . When including the ticket feature, the leak-proof CV accuracy seems to be significantly worse than the leaky CV accuracy. This seems to confirm the conjecture that the XGBoost model would use the (truncated) ticket feature to identify groups and rely on leakage. Once we drop the ticket feature this is reversed, although the difference is no longer significant in that case (might be due to chance). Once again, this would be consistent with the conjecture that without the ticket feature our model doesn&#39;t make use of the leakage in any significant way. . The fact that the XGBoost model has a higher accuracy than the baselines is entirely due to its better predictions for passengers traveling in groups. For solo passengers the accuracy is either essentially the same as the baselines (under the leak-proof CV) or much worse than the baselines (under the leaky CV). Given that solo travelers make up $41 %$ of the passengers, any decrease in accuracy for solo passengers needs to be compensated by a similar increase in accuracy for non-solo passengers. . In the leaky CV, the performance on solo passengers degrades significantly when removing the ticket and family size features. In fact, it appears that the performance reduction can be entirely explained by the drop in accuracy for solo travelers (which, again, make up almost half of the passengers). . Conclusions . We need to take any conclusions with a grain of salt because the differences between the mean accuracy scores could at least partially be explained by random effects related to the train-test splits. The standard errors of the mean accuracy scores are roughly given by dividing the standard deviation of the results of each cross-validation by $ sqrt{5}$ (the square root of the number of folds). However, this would give us a somewhat downward biased estimate of the &quot;true&quot; standard error, given that the sample standard deviation is itself downward biased and the scores in a CV are not independent of each other (which is the underlying assumption for dividing by $ sqrt{5}$). Estimating the standard error for k-fold cross-validation is tricky. . The following table contains the standard deviations corresponding to the mean accuracy scores in the previous table. Recall that our rough rule of thumb is that the standard error of the mean accuracy is given by the standard deviation divided by 2 (which is slightly less than $ sqrt{5}$). . pd.DataFrame(results_dic, index=index)[[&#39;Leaky CV std&#39;, &#39;Leaky CV solo std&#39;, &#39;Leak-proof CV std&#39;, &#39;Leak-proof CV solo std&#39;]] . . Leaky CV std Leaky CV solo std Leak-proof CV std Leak-proof CV solo std . Gender model 0.014471 | 0.028917 | 0.016809 | 0.017170 | . Enhanced gender model 0.020251 | 0.024500 | 0.022864 | 0.016383 | . Group survival model 0.009656 | 0.028917 | 0.016809 | 0.017170 | . XGBoost 0.022923 | 0.043353 | 0.017577 | 0.026314 | . XGBoost no ticket 0.031544 | 0.046849 | 0.021893 | 0.012502 | . XGBoost no ticket or familiy size 0.034558 | 0.059913 | 0.016670 | 0.019164 | . Overall, I think the results point toward real effects but it has to be said that the observed differences seem to be mostly between 1 and 2 standard errors (according to the rule of thumb). The next step would be a careful analysis of the errors. Paired hypothesis tests are necessary to compare classifiers evaluated with the same CV (take the accuracy differences for each split and compute the sample standard deviation of the differences). . Keeping in mind the caveats above, we saw that classifiers can perform better than the enhanced gender baseline, even if we prevent the group survival leakage. Recall that XGBoost had a mean accuracy $0.822$ versus the baseline $0.801$ when evaluated with the leak-proof CV (using leak-proof grid search CV). It would be very interesting to see how other classifiers do compared to the baseline. . Assuming the difference in accuracy is statistically significant, this indicates that there is more predictive information in the data than: 1) What is given by splitting the passengers using the &#39;Sex&#39;, &#39;Pclass&#39;, and &#39;Embarked&#39; categories, and 2) The group survival leakage we identified. In fact, the $0.822$ mean accuracy achieved in the leak-proof CV is not far from the group survival baseline of $0.829$. This suggests that the amount of &quot;leakage-free&quot; information is comparable to the information we get from the leakage alone. The latter is highly speculative, of course. . Furthermore, the table of mean accuracy scores seems to show that the XGBoost model is using the leakage when we use the &quot;truncated ticket&quot; feature, but doesn&#39;t really use the leakage if that feature is dropped. However, we need to be cautious with these conjectures, considering the higher standard deviation when dropping those features. . Please let me know what you think! I welcome suggestions and I would be excited to see other people try this stuff out with other classifiers. There is still a lot to explore. . P.S. Is the Kaggle test set random? . Working with different train-test splits made me wonder if the Kaggle split was done completely at random or if the data was already ordered in some way before the split. In the latter case, the train and test set might have different distributions. . Below we will check how typical the train-test split is in two ways: . How often are groups with multiple members separated by the split? | What is the survival rate in the test set? | . In order to check if the observed quantities are typical we will make a histogram of both quantities over many random train-test split and see how central the values for the Kaggle split are within the histograms. This is essentially an informal hypothesis test (the null hypothesis being that the Kaggle test set was selected randomly, or more precisely by using a uniform distribution). . n_groups = (groups.value_counts() &gt; 1).sum() train_groups = set(X.join(groups).loc[kaggle_train_idx, &#39;Group&#39;].values) test_groups = set(X.join(groups).loc[kaggle_test_idx, &#39;Group&#39;].values) overlap = len(train_groups &amp; test_groups) / n_groups survival_rate = y[kaggle_test_idx].mean() print(f&#39;For the Kaggle split, {overlap * 100:.0f}% of proper groups (excluding solo travelers) have members in nboth train and test set, and the test set survival rate is {survival_rate:.2f}.&#39;) . . For the Kaggle split, 61% of proper groups (excluding solo travelers) have members in both train and test set, and the test set survival rate is 0.38. . overlaps = [] survival_rates = [] N = 10**4 for _ in range(N): groups_shuffled = groups.sample(frac=1).reset_index(drop=True) train_groups = set(groups_shuffled[kaggle_train_idx].values) test_groups = set(groups_shuffled[kaggle_test_idx].values) overlap = len(train_groups &amp; test_groups) / n_groups overlaps.append(overlap) y_shuffled = y.sample(frac=1).reset_index(drop=True) survival_rate = y_shuffled[kaggle_test_idx].mean() survival_rates.append(survival_rate) fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 4)) ax0.set_title(&#39;Fraction of groups divided into train and test&#39;) ax0.hist(overlaps); ax1.set_title(&#39;Survival rates in test set&#39;) ax1.hist(survival_rates); . . The Kaggle test set seems to be very typical, at least as far as the survival rate and passenger groups are concerned. In fact, both values (0.61 and 0.38) are close to the mean for both distributions (almost suspiciously close). This suggests that the test set was either drawn at random or specifically chosen to have a typical survival rate. .",
            "url": "david-recio.com/2022/04/11/titanic-leak.html",
            "relUrl": "/2022/04/11/titanic-leak.html",
            "date": " ‚Ä¢ Apr 11, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "The Latent Space of Podcasts",
            "content": "Nowadays we encounter recommender systems on a daily basis in search engines, streaming platforms, and social media. There exist many different mechanisms behind recommender systems, but we will focus on a a class of methods known as collaborative filtering. In a nutshell, this approach consists of taking the set of all known user preferences and using that to &quot;predict&quot; the user&#39;s preference for an item (movie, song, news article) that the user hasn&#39;t seen yet (or for which the user hasn&#39;t indicated a preference). The basis for establishing this preference depends on the context. Some examples include user ratings on Netflix, or how many times a user has listened to a song on Spotify. . Collaborative filtering relies on the assumption that similar users will like similar items. Furthermore, similarity is derived solely from the known user preferences, such as ratings, without any knowledge of the content of the items. Note that in practice only a tiny fraction of all user preferences are known. For example, Netflix users will only have watched a small fraction of all available content. . I find matrix-based collaborative filtering methods especially interesting. In those methods both the users and the items are represented by vectors in some high-dimensional space, called latent factors, which encapsulate both user preferences and item similarity: Vectors for two similar items (or for a user with a positive preference for an item) point in similar directions. . This latent space reflects patterns or structures in the set of items (for example movie genres), which we can visualize. For this we will need dimensionality reduction techniques, such as Principal Component Analysis, or PCA. It is interesting to see which structures emerge just from the set of user preferences, without providing any information about the items or users themselves. It is a useful check for our intuitions in regards to which items are more similar based on concepts like music style or movie genres. . Learning about this made me wonder which patterns the latent space of podcasts might reveal, given that I am a big fan of podcasts myself. This has likely already been studied internally by companies such as Apple and Spotify, but I haven&#39;t found any publicly available recommender system for podcasts. I imagine that part of the reason is the lack of large open access datasets, which do exist for movies, music, and books. This is probably because the mainstream appeal of podcasts is a relatively recent phenomenon. . Luckily I was able to find one pretty decent dataset of podcasts reviews on Kaggle. It consists of almost a million reviews for over 46,000 podcasts, stored in an SQLite database. Thanks to Stuart Axelbrooke for collecting the reviews and making them available for everyone! . We will use this dataset to create a recommender and visualize the latent factors for some popular podcasts. Before we can do that we will need to clean the data first. The data is a bit more raw than some more mainstream recommeder datasets like MovieLens. . import sqlite3 import pandas as pd import numpy as np from implicit.als import AlternatingLeastSquares from implicit.evaluation import precision_at_k, leave_k_out_split from scipy import sparse from sklearn.decomposition import PCA import matplotlib.pyplot as plt . . Import from SQLite . The whole data is in an SQLite file. The SQLite database contains three tables: . podcasts table containing the podcast ID, name and URL. | reviews table containing all the information associated with every review: the star rating, the review title and content, the date and time it was posted, and finally the author ID of the user who made the review as well as the podcast ID. | categories table, which simply contains a column with podcasts IDs and a column with categories into which to those podcasts have been classified. | . We will load the data from the SQLite file into a pandas DataFrame. Specifically, we will take a left join of the podcasts table and reviews table and select a subset of the columns. . For our purposes we will not need the review title and content. However, it would be interesting to do some NLP on the contents as a future project. Maybe some topic modeling which can be combined with collaborative filtering in a hybrid recommender system. . con = sqlite3.connect(&#39;data/database.sqlite&#39;) . get_ratings = &quot;&quot;&quot;SELECT author_id AS user_id, p.podcast_id, rating, p.title AS name, created_at FROM podcasts p INNER JOIN reviews r USING(podcast_id) &quot;&quot;&quot; ratings_raw = pd.read_sql(get_ratings, con, parse_dates=&#39;created_at&#39;) . ratings_raw . user_id podcast_id rating name created_at . 0 F7E5A318989779D | c61aa81c9b929a66f0c1db6cbe5d8548 | 5 | Backstage at Tilles Center | 2018-04-24 12:05:16-07:00 | . 1 F6BF5472689BD12 | c61aa81c9b929a66f0c1db6cbe5d8548 | 5 | Backstage at Tilles Center | 2018-05-09 18:14:32-07:00 | . 2 1AB95B8E6E1309E | ad4f2bf69c72b8db75978423c25f379e | 1 | TED Talks Daily | 2019-06-11 14:53:39-07:00 | . 3 11BB760AA5DEBD1 | ad4f2bf69c72b8db75978423c25f379e | 5 | TED Talks Daily | 2018-05-31 13:08:09-07:00 | . 4 D86032C8E57D15A | ad4f2bf69c72b8db75978423c25f379e | 5 | TED Talks Daily | 2019-06-19 13:56:05-07:00 | . ... ... | ... | ... | ... | ... | . 984400 4C3F6BE3495A23D | a23e18c73fd942fab41bcf6b6a1571da | 2 | Mile Higher Podcast | 2021-10-09 07:10:31-07:00 | . 984401 938F0A4490CE344 | ae44a724e94dcec1616d6e695e6198ba | 4 | Something Was Wrong | 2021-10-09 16:14:02-07:00 | . 984402 D6CBF74E9B2C7FE | ae44a724e94dcec1616d6e695e6198ba | 1 | Something Was Wrong | 2021-10-09 14:32:49-07:00 | . 984403 FED395AE84679C4 | ae44a724e94dcec1616d6e695e6198ba | 5 | Something Was Wrong | 2021-10-09 11:40:29-07:00 | . 984404 5058E0A92FF400B | ae44a724e94dcec1616d6e695e6198ba | 4 | Something Was Wrong | 2021-10-09 10:36:35-07:00 | . 984405 rows √ó 5 columns . Next we create a table of podcasts with some rating statistics: number of ratings, mean rating, and the years of the first and the last rating. . def extract_podcasts(ratings): &#39;Get the podcasts with rating count, rating mean and rating years.&#39; ratings_copy = ratings.copy() return (ratings_copy.groupby(&#39;podcast_id&#39;, as_index=False) .agg( name = (&#39;name&#39;, &#39;first&#39;), rating_count = (&#39;rating&#39;, &#39;count&#39;), rating_mean = (&#39;rating&#39;, &#39;mean&#39;), earliest_rating_year = (&#39;created_at&#39;, lambda c: c.min().year), latest_rating_year = (&#39;created_at&#39;, lambda c: c.max().year), ) ) . podcasts_raw = extract_podcasts(ratings_raw) podcasts_raw . podcast_id name rating_count rating_mean earliest_rating_year latest_rating_year . 0 a00018b54eb342567c94dacfb2a3e504 | Scaling Global | 1 | 5.000000 | 2017 | 2017 | . 1 a00043d34e734b09246d17dc5d56f63c | Cornerstone Baptist Church of Orlando | 1 | 5.000000 | 2019 | 2019 | . 2 a0004b1ef445af9dc84dad1e7821b1e3 | Mystery: Dancing in the Dark | 1 | 1.000000 | 2011 | 2011 | . 3 a00071f9aaae9ac725c3a586701abf4d | KTs Money Matters | 4 | 5.000000 | 2018 | 2018 | . 4 a000aa69852b276565c4f5eb9cdd999b | Speedway Soccer | 15 | 5.000000 | 2018 | 2020 | . ... ... | ... | ... | ... | ... | ... | . 46688 fffe3f208a56dfecfaf6d0a7f8399d63 | How Travel Writers Self-Publish | 4 | 5.000000 | 2019 | 2020 | . 46689 fffeb7d6d05f2b4c600fbebc828ca656 | TEDDY &amp; THE EMPRESS: Cooking the Queens | 43 | 4.837209 | 2017 | 2021 | . 46690 ffff5db4b5db2d860c49749e5de8a36d | Frankenstein, or the Modern Prometheus | 8 | 4.750000 | 2011 | 2021 | . 46691 ffff66f98c1adfc8d0d6c41bb8facfd0 | Who‚Äôs Bringing Wine? | 5 | 5.000000 | 2018 | 2018 | . 46692 ffff923482740bc21a0fe184865ec2e2 | TEFL Waffle | 2 | 5.000000 | 2018 | 2019 | . 46693 rows √ó 6 columns . Data Exploration and Cleaning . In this section we will deal with some issues in the data and prepare it for the recommender system below. . How far back do the reviews go? . A couple of ratings go all the way back to 2005 although most of them only go back to 2018. For many popular podcasts the reviews start in 2019. . When I asked the curator of the dataset on Kaggle why the reviews go much further back for some podcasts than for most others, he clarified that the reason is that the Apple API only gives access the latest 500 reviews of each podcast. This explains why for popular podcasts those 500 reviews only go back a couple of months, but for others they go back many years. . Inspecting the dates of the reviews of some popular podcasts, I found no gaps since 2019. This confirms that the reviews have been downloaded without interruption since then. . Curating the Ratings . We need to take care of the following complications: . Some users have left a suspiciously high number of reviews. Indeed, looking at the content of their reviews they do not look genuine at all: they repeat the same text hundreds of times, with slight variations. We will remove all the users with a rating volume beyond a specific threshold to weed out bots. We set the threshold at 135 reviews by inspecting the content of the reviews and making a judgment call. | It appears that some podcasts are no longer active, given that their latest review was made years ago. We need to decide whether we want to remove these seemingly inactive podcasts. While we don&#39;t want to recommend podcasts that don&#39;t exist anymore, their reviews can still help the collaborative filtering model. We will simply remove podcasts which have zero reviews made on or after 2020. Another option would be to include old podcasts in the training of the recommender system but skip them when making recommendations. | It turns out that there are repeat reviews in the data, meaning that some users left multiple reviews for the same podcast. They are probably just edited or updated reviews. Consequently, we will only consider the latest rating for each user-podcast pairing. | For the collaborative filtering approach to work, the users need to have rated multiple podcasts and, similarly, the podcasts need to have been rated by multiple users. To ensure this, we need to remove all users and podcasts with a number of reviews below a certain threshold. For example, we could remove all users with under 3 reviews and all podcasts with under 15 reviews. We have to be careful here: removing some users will reduce the number of reviews for some podcasts, which might push some podcasts below the threshold. In turn, removing those podcasts might push some users below the threshold. We need to keep doing this back and forth until the ratings DataFrame stops changing. | . We will write a separate function to deal with each point. . def remove_suspicious_users(ratings, max_reviews=135): &#39;Remove users with suspiciously high review count.&#39; mask = ratings.groupby(&#39;user_id&#39;)[&#39;podcast_id&#39;].transform(&#39;count&#39;) &lt;= max_reviews return ratings[mask] def remove_inactive_podcasts(ratings, latest_rating_year=2020): &#39;Remove podcasts with no reviews at or after latest_rating_year.&#39; active = (ratings.groupby(&#39;podcast_id&#39;)[&#39;created_at&#39;] .transform(lambda c: c.max().year) &gt;= latest_rating_year ) return ratings[active] def keep_only_latest_rating(ratings): &#39;Remove repeat reviews, keeping the latest. Also sorts the ratings by date.&#39; return ratings.sort_values(by=&#39;created_at&#39;, ascending=False).drop_duplicates(subset=[&#39;podcast_id&#39;, &#39;user_id&#39;]) def remove_low_rating_users_and_podcasts(ratings, min_user_reviews=3, min_podcast_reviews=15): &#39;Alternate between removing podcasts and users with insufficient reviews until there are none left.&#39; result = ratings.copy() while result.shape: previous_shape = result.shape mask = result.groupby(&#39;podcast_id&#39;)[&#39;user_id&#39;].transform(&#39;count&#39;) &gt;= min_podcast_reviews result = result[mask] mask = result.groupby(&#39;user_id&#39;)[&#39;podcast_id&#39;].transform(&#39;count&#39;) &gt;= min_user_reviews result = result[mask] if result.shape == previous_shape: return result . ratings = remove_suspicious_users(ratings_raw) ratings = remove_inactive_podcasts(ratings) ratings = keep_only_latest_rating(ratings) ratings = remove_low_rating_users_and_podcasts(ratings) . ratings . user_id podcast_id rating name created_at . 984386 2ED4C4FD5F1740E | a9bdaba5449189a4587793e36ce4f704 | 5 | Going West: True Crime | 2021-10-09 16:24:40-07:00 | . 984402 D6CBF74E9B2C7FE | ae44a724e94dcec1616d6e695e6198ba | 1 | Something Was Wrong | 2021-10-09 14:32:49-07:00 | . 984376 C0E30B0BB0AA10C | ddd451a18055f0108edf79f8c3c9bf15 | 5 | What If World - Stories for Kids | 2021-10-09 07:36:51-07:00 | . 984301 E220A79CBD5C5AD | b9a8d90ae43232769ecc68d7defb0c38 | 5 | Scared To Death | 2021-10-09 03:51:32-07:00 | . 984316 36B07D3E02D2516 | c3f080cc393035a81d4ac7c7bff1c6c1 | 5 | Smash Boom Best | 2021-10-09 03:45:35-07:00 | . ... ... | ... | ... | ... | ... | . 138992 4A06D17875D08A3 | fb3a6b9f12f4d0b050887e91684e68c0 | 5 | EconTalk | 2007-04-21 20:49:49-07:00 | . 50962 4A06D17875D08A3 | d2f31ced463510866c6adfbae95d3be2 | 2 | Money Girl&#39;s Quick and Dirty Tips for a Richer... | 2007-03-31 21:09:12-07:00 | . 479293 245BC52E7C82EA7 | b913c95d304bac8b6cddf9034294eab7 | 5 | PotterCast: The Harry Potter podcast since 2005 | 2006-10-18 14:14:46-07:00 | . 479291 39D9B7796AA0A6C | b913c95d304bac8b6cddf9034294eab7 | 5 | PotterCast: The Harry Potter podcast since 2005 | 2006-05-20 11:18:27-07:00 | . 202910 BF99560F0BB8F18 | cae7b4d90b57e8bae77907092d5e813e | 4 | All Songs Considered | 2005-12-15 04:23:27-07:00 | . 51015 rows √ó 5 columns . podcasts = extract_podcasts(ratings) . podcasts.sort_values(by=&#39;rating_count&#39;, ascending=False) . podcast_id name rating_count rating_mean earliest_rating_year latest_rating_year . 245 bc5ddad3898e0973eb541577d1df8004 | My Favorite Murder with Karen Kilgariff and Ge... | 688 | 3.228198 | 2019 | 2021 | . 228 bad6c91efdbee814db985c7a65199604 | Wow in the World | 649 | 4.648690 | 2019 | 2021 | . 807 f2377a9b0d9a2e0fb05c3dad55759328 | Story Pirates | 589 | 4.590832 | 2016 | 2021 | . 334 c3f080cc393035a81d4ac7c7bff1c6c1 | Smash Boom Best | 470 | 4.740426 | 2018 | 2021 | . 142 b1a3eb2aa8e82ecbe9c91ed9a963c362 | True Crime Obsessed | 434 | 3.679724 | 2019 | 2021 | . ... ... | ... | ... | ... | ... | ... | . 821 f3c3640687112903a1fa5869665682ae | American Ball Tales | 15 | 4.933333 | 2019 | 2020 | . 301 c0f3a638b96dd2482255211827f1a95a | Dreams In Drive | 15 | 5.000000 | 2016 | 2021 | . 306 c149f512e2e103090a77b32d42511479 | Cantina Cast | 15 | 4.400000 | 2013 | 2019 | . 202 b76c794eb4bb5b73748bd8e5dc8052b8 | Congeria | 15 | 4.200000 | 2018 | 2020 | . 800 f17f50670f7ba891f1b147687649da29 | OFFSHORE | 15 | 4.733333 | 2016 | 2018 | . 936 rows √ó 6 columns . Out of the 46,693 podcasts we started with, we are left with 936. Unfortunately, it is inevitable that we have to discard a large fraction of the podcasts because most of them have only a few reviews on Apple Podcasts. Consider the fact that more than a fourth of the podcasts (13,922 to be precise) had only a single review. More that half of the podcasts (a total of 25,104) had only up to 3 reviews! . That said, it&#39;s worth noting that there are actually as many as 8323 podcasts with at least 15 ratings. However, a sizable portion of the users leaving those ratings had to be removed because they only rated one or two podcasts in total (and of course removing some podcasts led to having to remove more users and so on). Thus, this is how we are left with just 936 podcasts. . The remaining ratings are still sufficient to yield interesting results, though! . The minimum threshold of ratings for users and podcasts is also reflected in the density of the ratings matrix. The so called ratings matrix contains all the ratings such that each row corresponds to one user and each column corresponds to one podcast. If there is a particular user hasn&#39;t rated a particular podcast, the corresponding entry (where the user row and podcast column meet) is simply $0$. Furthermore, the density of the ratings matrix is the percentage of non-zero entries. In other words, the density is the percentage of user-podcast pairs for which a rating exists in the dataset. . def compute_density(ratings): n_ratings = ratings.shape[0] n_podcasts = ratings[&#39;podcast_id&#39;].nunique() n_users = ratings[&#39;user_id&#39;].nunique() return n_ratings / (n_podcasts * n_users) . print(f&#39;The density of the curated rating matrix is {compute_density(ratings) * 100:.2f}%, while the density of the original rating matrix is {compute_density(ratings_raw) * 100:.4f}%.&#39;) . The density of the curated rating matrix is 0.45%, while the density of the original rating matrix is 0.0028%. . We went from 755,438 users to 12,212 users after cleaning up the data and discarding users and podcasts with too few reviews. . Unfortunately, the vast majority of users left only a single review (in this dataset at least). This is probably at least partly due to the fact that many popular podcasts are missing and even for those included the reviews go back only three years. However, even taking this into account, it is conceivable that most people listen to fewer podcasts than Netflix users watch different shows and movies, for example. There is also more friction (more time and steps involved) for leaving a review on Apple Podcasts than rating a show on Netflix, again as an example. . Implicit Recommender System . It turns out that the overwhelming majority of the ratings are 5 star ratings. It appears that most users do not go out of their way to give a negative rating unless they really dislike a show. The following bar chart shows the frequency of each star rating in the curated ratings table. The situation is even more skewed in favor of 5 star ratings in the raw ratings data. . fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 4)) ax0.set_title(&#39;Star Ratings Before Curating&#39;) ax0.bar(ratings_raw[&#39;rating&#39;].value_counts().index, ratings_raw[&#39;rating&#39;].value_counts().values) ax1.set_title(&#39;Star Ratings After Curating &#39;) ax1.bar(ratings[&#39;rating&#39;].value_counts().index, ratings[&#39;rating&#39;].value_counts().values); . Why Implicit? . When I started this project I intended to use a model which tries to predict the specific star rating a user would give to &quot;unseen&quot; items, in order to recommend the item with the highest predicted rating. This is how explicit recommender systems work, which are trained on explicit user feedback (in this case, star ratings). However, the extreme imbalance in the ratings suggests that the explicit recommender system approach might not be appropriate here. . First of all, there is a well-known issue with imbalanced data which can be illustrated as follows. The simple baseline model which predicts that every user will rate every podcast with 5 stars would have a high accuracy, because it would be right almost all of the time. That said, this is not a big deal and can be corrected by choosing a more appropriate metric than plain accuracy. . The deeper concern in this case is the reason behind the class imbalance. It appears that that most users simply stop listening to a podcast they don&#39;t like without bothering to leave a negative review. Not only that, but people clearly don&#39;t just pick podcasts at random to listen to. Instead, there is a pre-selection: they follow a friend&#39;s recommendation, seek out podcasts on a particular topic or featuring a particular guest, and so on. Needless to say, users are unlikely to leave a review for a podcast they never listened to (although I am sure that a handful of people do). . All of this is to say: . In explicit recommender systems missing ratings are viewed simply as missing information. However, it appears that there actually is some information given by the fact that a podcast wasn&#39;t rated by a user. Maybe we should think of missing ratings as suggesting a negative preference, but assigning low confidence to that preference. Some reasons why a missing rating potentially reveals a negative preference were given above. Namely, users are less likely to rate a podcast they don&#39;t like and many are even unlikely to listen to it in the first place. On the other hand, the confidence in this negative preference is low because the rating might be missing for a variety of other reasons. The most likely reason is that the user isn&#39;t even aware of that particular podcast&#39;s existence. | Focusing mostly on the precise ratings (1 to 5 stars) is of limited value because users seem to be using the stars mostly to give a &quot;thumbs up&quot; (5 stars). | . It turns out that there is an approach which seems to be perfectly suited to address the two issues above: implicit recommender systems. They are called implicit because they usually do not use (explicit) feedback given by the users. Instead they infer the preferences of users from their activity, such as how often a user has listened to a song on Spotify, or if a user has watched the entirety of a movie on Netflix. The fundamental change from explicit to implicit systems is that instead of giving ratings, users have preferences and those preferences are known to us with a certain confidence. What this allows us to do is to interpret the absence of activity (user didn&#39;t watch a particular movie) as a negative preference, but with low confidence. . Unfortunately, we don&#39;t have access to user activity, but the ratings (which are explicit feedback) can be made &quot;implicit&quot; with the following interpretation: high ratings (4 or 5 stars) correspond to positive preferences with high confidence, while missing ratings and all lower ratings (1 to 3 stars) correspond to negative preferences with low confidence. It is possible to treat low ratings separately from missing ratings but this doesn&#39;t seem to improve the results, maybe due to the low frequency of low ratings. . Alternating Least Squares . We will use the implicit, a very fast recommender library written in Cython by Ben Frederickson. Specifically, we will use the Alternating Least Squares algorithm, or ALS. The ALS algorithm for implicit recommenders was introduced in this paper by Hu, Koren and Volinsky. I will not go into too much detail here, but a general explanation is outlined below. In addition to the original paper, I recommend reading this blog post, in which the algorithm is implemented in Python (although the implicit library is actually used for speed). . Here is a brief overview of the model we will use: Each user $u$ is assumed to have a preference $p_{ui}$ for podcast $i$ and we want to find latent factors $x_u$ and $y_i$ such that their inner product approximates the preference: $p_{ui} approx x_u cdot y_i$. More precisely, we want to find $x_u$ and $y_i$ which minimize the following cost function: $$ sum_{u,i} c_{ui}(p_{ui} - x_u cdot y_i)^2 + lambda left( sum_u |x_u |^2 + sum_i |y_i |^2 right) $$ . The weights $c_{ui}$ are the confidence that we have in the respective preference $p_{ui}$. The higher the confidence, the more importance we give to approximating the particular preference $p_{ui}$ by $x_u cdot y_i$. The summands multiplied by $ lambda$ are there to avoid overfitting. . If we hold constant the user vectors $x_u$, the cost function is quadratic in the podcast vectors $y_i$ and can be minimized efficiently. The same is true swapping $x_u$ and $y_i$. This where the Alternating Least Squares trick comes in: First compute the $y_i$ which minimize the cost function with $x_u$ held constant. Then fix $y_i$ at that (provisional) minimum and in turn find $x_u$ minimizing the resulting cost function. Amazingly, simply doing this back and forth several times yields pretty good results. . The Implicit Matrix . In order to feed our data to the implicit ALS model, we need to transform our table of explicit ratings into a matrix of implicit data. The entries of the matrix need to incorporate both the confidence factors $c_{ui}$ and the preference factors $p_{ui}$. . In order to construct the matrix correctly, we need to know which input the model implicit.AlternatingLeastSquares expects. We feed the ALS model a single matrix, which then (internally) deduces preferences and confidence from that single matrix. If there is a positive entry at a position $(u,i)$, this is taken to mean that $p_{ui} = 1$ (positive preference), otherwise $p_{ui} = 0$ (negative entries). The precise values of the entries are also important: The element at position $(u,i)$ equals the confidence $c_{ui}$, after adding 1 to make sure that the confidence is at least 1 for all $(u,i)$ (if the confidence at some point were 0 the preference $p_{ui}$ would be irrelevant in the cost function, which we want to avoid in the implicit setting). . In light of the above, it&#39;s clear that our implicit matrix needs strictly positive entries for each pair $(u,i)$ for which the user $u$ gave the podcast $i$ a high ratings, and all other entries should be set to 0. Marking low ratings (1 or 2 stars, say) with negative entries in the matrix did not help much when I tried it, so we will avoid this. (That would mean a higher confidence in the negative preference for low ratings, as opposed to missing ratings.) . Here is what we will do: The implicit matrix will have a 1 at every position corresponding to a high rating (4 or 5 stars) and a 0 everywhere else. There is nothing special about the value 1, which can be changed later to any other number (by simply multiplying the matrix by that number). Note that most entries are 0, given that most users have not left reviews for most podcasts. In other words, the matrix will have a high sparsity (low density). This is why it makes sense to use a scipy sparse matrix instead of a NumPy array. . def make_implicit(ratings, threshold=4): &#39;&#39;&#39;Replace star rating (1 to 5) by a +1 if rating &gt;= threshold and if rating &lt; threshold either replace it by a -1 (if negative is True) or remove it (if negative is False). Return a csr sparse matrix with the ratings (users rows and podcasts cols) and two lists: one with the user_ids corresponding to the rows and one with the podcast names corresponding to the columns. &#39;&#39;&#39; positive = ratings[&#39;rating&#39;] &gt;= threshold implicit_ratings = ratings.loc[positive].copy() implicit_ratings[&#39;rating&#39;] = 1 # Remove low rating users and podcasts again implicit_ratings = remove_low_rating_users_and_podcasts(implicit_ratings, 2, 5) user_idx = implicit_ratings[&#39;user_id&#39;].astype(&#39;category&#39;).cat.codes podcast_idx = implicit_ratings[&#39;podcast_id&#39;].astype(&#39;category&#39;).cat.codes # The codes simply number the user_id and podcast_id in alphabetical order # We keep track of the order of the users and podcasts with the following arrays user_ids = implicit_ratings[&#39;user_id&#39;].sort_values().unique() podcast_names = implicit_ratings.sort_values(by=&#39;podcast_id&#39;)[&#39;name&#39;].unique() implicit_ratings = sparse.csr_matrix((implicit_ratings[&#39;rating&#39;], (user_idx, podcast_idx))) return implicit_ratings, user_ids, podcast_names . implicit_ratings, user_ids, podcast_names = make_implicit(ratings) implicit_ratings.shape . (10607, 933) . Training and Evaluation . At last, we are ready to train our recommender! . To evaluate the performance of a recommender we need to be able to decide if recommendations are relevant. However, if the system simply recommends podcasts that it already &quot;knows&quot; the user likes (positive entry in the implicit matrix), this doesn&#39;t reflect how well the system can make recommendations for podcasts the user hasn&#39;t shown a preference for yet (0 in the implicit matrix). . To address this, we will turn one positive entry into a 0 entry for each user. In other words, for each user we will forget one podcast the user rated highly. Then we train the recommender system on this modified implicit dataset (called the training set). Next, we let the model make one recommendation per user, but require that for each user the podcast recommended has not already been &quot;liked&quot; by that user in the training set. Finally, we compute the precision of the recommender: the fraction of the users for which the recommendation is precisely the podcast we &quot;forgot&quot; for that user when creating the training set. Recall that we know those &quot;forgotten&quot; podcasts to be relevant recommendations, because the user gave them a high rating (which we omitted in the training set). . Note that recommendations other than the one positive preference we omitted (for each user) might also be relevant, but there is no way for us to verify that with our data. In light of this, the precision might in fact underestimate how often the recommendations are relevant. . The (simple) precision is not the best metric. For example, it would be better to omit several ratings for each user and then compute the precision at k (denoted p@k), which consists of recommending $k$ podcasts for each user and determining which fraction of those recommendations is relevant. What we are doing above is effectively p@1 (precision at 1). There are other more sophisticated metrics, but they also require making multiple recommendations per user. The reason we cannot use these metrics is that most users only have 3 ratings and removing more than one would leave them with 1 rating, which is basically useless for collaborative filtering. If we instead only removed ratings from a subset of users who left many ratings, we would be biasing our metric in favor of a minority of very active users. . ratings_train, ratings_test = leave_k_out_split(implicit_ratings, K=1) . import os os.environ[&quot;MKL_NUM_THREADS&quot;] = &quot;1&quot; als_recommender = AlternatingLeastSquares(factors=50, regularization=0.1, random_state=42) als_recommender.fit(2 * ratings_train.T) . precision_at_k(als_recommender, ratings_train, ratings_test, K=1) . 0.09263038548752835 . As a baseline, we will also compute the precision for a simple recommender which recommends the most popular podcast to all users. To be precise, it recommends the most popular podcast among those not already liked by the user in the training set, because those recommendations are not scored as hits when computing the precision (we want the recommender to suggest &quot;new&quot; podcasts after all). . We write the baseline in such a way that it can also recommend multiple podcasts. It simply recommends the $N$ most popular podcasts, given some $N$. . class PopularityBaseline(): def __init__(self, implicit_ratings): podcast_ids, count = np.unique(implicit_ratings.tocoo().col, return_counts=True) self.top_podcasts = podcast_ids[np.argsort(-count)] def recommend(self, user_id, user_items, N=10): &#39;&#39;&#39;Recommend the most popular podcasts, but exclude podcasts which the users in user_ids have already interacted with according to user_items&#39;&#39;&#39; user_items = user_items.tocoo() this_user = user_items.row == user_id liked_podcasts = set(user_items.col[this_user]) recom = [] for podcast in self.top_podcasts: if podcast not in liked_podcasts: recom.append(podcast) if len(recom) == N: break else: raise Exception(&#39;Not enough podcasts remaining to recommend&#39;) return list(zip(recom, [0] * N)) # The implicit API expects a score for each podcast popularity_baseline = PopularityBaseline(implicit_ratings) . precision_at_k(popularity_baseline, ratings_train, ratings_test, K=1) . 0.02913832199546485 . Our recommender system is significantly better than the baseline recommender ($9.3 %$ versus $2.9 %$). It appears the recommender learned something! . Now we will train the recommender again but with the whole implicit rating set, not just the a smaller training set. We will use this recommender going forward. . als_recommender = AlternatingLeastSquares(factors=50, regularization=0.1, random_state=42) als_recommender.fit(2 * implicit_ratings.T) . Latent Factors . Recall that the our recommender works by finding latent factors for all podcasts and all users, such that the inner product of the user and podcast vectors is as close as possible to the corresponding user preferences. Another way of looking at this is that preference (of a user for a podcast) or similarity (of two podcasts, or two users, to each other) corresponds to vectors pointing in a similar direction (technically, having a high cosine similarity, or low cosine distance). . In light of the above, to introspect the recommender we must visualize the latent factors. We will do this for the most popular podcasts in the dataset. Because the latent space is 50-dimensional we will project it down to 2 dimensions. We will use Principal Component Analysis (PCA) to find the two directions in which the latent factors vary the most and project down to those. . podcast_ids, count = np.unique(implicit_ratings.tocoo().col, return_counts=True) top_podcasts = podcast_ids[np.argsort(-count)][:25] . pca = PCA(n_components=5) reduced_item_factors = pca.fit_transform(als_recommender.item_factors) . fig, ax = plt.subplots(figsize=(15, 15)) X = reduced_item_factors[top_podcasts].T[1] Y = reduced_item_factors[top_podcasts].T[0] ax.set_title(&#39;Latent Podcast Space&#39;, fontdict = {&#39;fontsize&#39; : 20}) ax.scatter(X, Y) for i, x, y in zip(podcast_names[top_podcasts], X, Y): ax.text(x, y, i, color=np.random.rand(3)*0.7, fontsize=14) . We must take the visualization with a grain of salt because obviously information is lost when we project a 50-dimensional space down to two dimensions. Specifically, podcasts that appear close in the projection might not be close at all in the full space. . That said, there appears to be some clear structure, which we will describe below. We must also remember that this is not some random 2D projection, but a projection to the two axes of highest variability (principal components). . Let&#39;s start with the horizontal direction (or x axis). Podcasts targeted at children are on the right and podcasts targeted at more mature audiences are to the left. The most extreme values are attained by &#39;Wow to the World&#39; and &#39;Story Pirates&#39;, which are the most popular podcasts for kids. Judging from the content of the reviews there seems to be a bit of a rivalry between those two podcasts, although they have a large overlap in preference. &#39;Smash Boom Best&#39; and &#39;Pants on Fire&#39; are for children as well. It is interesting that the two podcasts on stories for kids are so close to each other. . In the vertical direction (or y axis), the situation is not as clear-cut but we can recognize different genres bunch together. The podcasts at the top all focus on self-improvement or self-help. The tiles &#39;The Learning Leader Show&#39;, &#39;Discover Your Talent&#39;, and &#39;Mindulness Mode&#39; are self-explanatory. &#39;Confessions of a Terrible Husband&#39; is about relationship advice. As for &#39;Leveling Up&#39;, this is a (partial) quote from the official website: &quot;Leveling Up is a radical new perspective on achieving success [...]&quot;. On the other hand the podcasts at the bottom are all for pure entertainment (true crime themed and slightly above, pop culture and comedy). . Podcast Similarity . As a reality check, we will go through a couple of popular podcasts and inspect the 10 most similar podcasts according to our model. I find the results pretty impressive considering the limited information the model was trained on. Click on &quot;show output&quot; to view the list of similar podcasts. . def get_k_most_similar_podcasts(name, recommender, podcast_names, K=10): this_name = np.where(podcast_names == name)[0][0] return [podcast_names[idx] for idx, _ in recommender.similar_items(this_name, N=K+1)[1:]] . get_k_most_similar_podcasts(&#39;My Favorite Murder with Karen Kilgariff and Georgia Hardstark&#39;, als_recommender, podcast_names, 10) . [&#39;Scary Stories To Tell On The Pod&#39;, &#39;Puck Soup&#39;, &#39;Who Killed Theresa?&#39;, &#39;Murder Minute&#39;, &#39;Mother, May I Sleep With Podcast?&#39;, &#39;Manifestation Babe&#39;, &#39;Encyclopedia Womannica&#39;, &#39;Highest Self Podcast&#39;, &#39;The TryPod&#39;, &#39;Story Story Podcast: Stories and fairy tales for families, parents, kids and beautiful nerds.&#39;] . . get_k_most_similar_podcasts(&#39;The Joe Rogan Experience&#39;, als_recommender, podcast_names, 10) . [&#39;Bret Weinstein | DarkHorse Podcast&#39;, &#39;Artificial Intelligence (AI)&#39;, &#39;Cleared Hot&#39;, &#39;Ben Greenfield Fitness&#39;, &#39;The Bill Bert Podcast&#39;, &#39;Walk-Ins Welcome w/ Bridget Phetasy&#39;, &#34;Congratulations with Chris D&#39;Elia&#34;, &#39;Jocko Podcast&#39;, &#39;expediTIously with Tip &#34;T.I.&#34; Harris&#39;, &#39;Team Never Quit&#39;] . . get_k_most_similar_podcasts(&#39;Story Pirates&#39;, als_recommender, podcast_names, 10) . [&#39;By Kids, For Kids Story Time&#39;, &#39;Katie, The Ordinary Witch&#39;, &#39;Listen Out Loud with The Loud House&#39;, &#39;Minecraft Me - SD Video&#39;, &#39;Story Story Podcast: Stories and fairy tales for families, parents, kids and beautiful nerds.&#39;, &#39;Scary Story Podcast&#39;, &#39;Wow in the World&#39;, &#39;Highlights Hangout&#39;, &#39;The Casagrandes Familia Sounds&#39;, &#39;KiDNuZ&#39;] . . get_k_most_similar_podcasts(&#39;Best Real Estate Investing Advice Ever&#39;, als_recommender, podcast_names, 10) . [&#39;Real Estate Investing For Cash Flow Hosted by Kevin Bupp.&#39;, &#39;Jake and Gino: Multifamily Real Estate Investing &amp; More&#39;, &#39;Target Market Insights: Multifamily + Marketing&#39;, &#39;Before the Millions | Lifestyle Design Through Real Estate | Passive Cashflow Investing Tips and Str...&#39;, &#39;Lifetime Cash Flow Through Real Estate Investing&#39;, &#39;Dwellynn Show - Financial Freedom through Real Estate Investing&#39;, &#39;Investing In The U.S.&#39;, &#39;Accelerated Investor Podcast&#39;, &#39;Support is Sexy Podcast with Elayne Fluker | Interviews with Successful Women Entrepreneurs 5 Days a...&#39;, &#39;Simple Passive Cashflow&#39;] . . get_k_most_similar_podcasts(&#39;Mindfulness Mode&#39;, als_recommender, podcast_names, 10) . [&#39;Pregnancy Podcast&#39;, &#39;The Bitcoin Knowledge Podcast&#39;, &#39;Undone Redone&#39;, &#39;Recording Studio Rockstars&#39;, &#39;Take Up Code&#39;, &#39;Wedding Planning Podcast&#39;, &#39;Podcast Junkies&#39;, &#39;Tandem Nomads - From expat partners to global entrepreneurs! Build a successful business and thrive...&#39;, &#39;Fearless And Healthy Podcast|Holistic Health|Success Habits|Lifestyle&#39;, &#39;Play Your Position with Mary Lou Kayser&#39;] . . get_k_most_similar_podcasts(&#39;ADHD reWired&#39;, als_recommender, podcast_names, 10) . [&#39;People Behind the Science Podcast - Stories from Scientists about Science, Life, Research, and Scien...&#39;, &#39;Brilliant Business Moms with Beth Anne Schwamberger&#39;, &#39;The Inside Winemaking Podcast with Jim Duane&#39;, &#39;Top Traders Unplugged&#39;, &#39;Big Wig Nation with Darrin Bentley&#39;, &#39;Own It! For Entrepreneurs. Talking Digital Marketing, Small Business, Being Digital Nomads and Succ...&#39;, &#39;Maura Sweeney: Living Happy Inside Out | Encouragement | Inspiration | Empowerment | Leadership&#39;, &#39;Commercial Real Estate Elite: Broker to Brokers&#39;, &#39;Whistle and a Clipboard- the coaching communities resource&#39;, &#39;Podcast Junkies&#39;] . . get_k_most_similar_podcasts(&#39;Good Night Stories for Rebel Girls&#39;, als_recommender, podcast_names, 10) . [&#39;Fierce Girls&#39;, &#39;Short &amp; Curly&#39;, &#39;Big Life Kids Podcast&#39;, &#39;Dream Big Podcast&#39;, &#39;Book Club for Kids&#39;, &#39;Katie, The Ordinary Witch&#39;, &#39;Saturday Morning Theatre&#39;, &#39;KiDNuZ&#39;, &#39;Storynory - Stories for Kids&#39;, &#39;The Calm Kids Podcast&#39;] . . get_k_most_similar_podcasts(&#39;Leveling Up with Eric Siu&#39;, als_recommender, podcast_names, 10) . [&#39;Leaders Inspire Leaders | Koy McDermott - Millennial Leadership Consultant | Personal &amp; Professional...&#39;, &#34;Where There&#39;s Smoke&#34;, &#39;Tandem Nomads - From expat partners to global entrepreneurs! Build a successful business and thrive...&#39;, &#39;On Air with Ella&#39;, &#39;Everyday MBA&#39;, &#39;MLM Nation&#39;, &#34;Worldbuilder&#39;s Anvil&#34;, &#39;The Bitcoin Knowledge Podcast&#39;, &#39;Fearless And Healthy Podcast|Holistic Health|Success Habits|Lifestyle&#39;, &#39;Play Your Position with Mary Lou Kayser&#39;] . . get_k_most_similar_podcasts(&#39;Pants on Fire&#39;, als_recommender, podcast_names, 10) . [&#39;The Mayan Crystal&#39;, &#39;The Cramazingly Incredifun Sugarcrash Kids Podcast&#39;, &#39;Minecraft Me - SD Video&#39;, &#39;Saturday Morning Theatre&#39;, &#39;Short &amp; Curly&#39;, &#39;Smash Boom Best&#39;, &#39;Book Club for Kids&#39;, &#39;The Casagrandes Familia Sounds&#39;, &#39;The Past and The Curious: A History Podcast for Kids and Families&#39;, &#39;Listen Out Loud with The Loud House&#39;] . . get_k_most_similar_podcasts(&#39;Bachelor Happy Hour with Rachel &amp; Ali ‚Äì The Official Bachelor Podcast&#39;, als_recommender, podcast_names, 10)#collapse-output . [&#39;Logically Irrational&#39;, &#39;Mommies Tell All&#39;, &#39;The Ben and Ashley I Almost Famous Podcast&#39;, &#39;Hot Marriage. Cool Parents.&#39;, &#39;Miraculous Mamas&#39;, &#39;Off The Vine with Kaitlyn Bristowe&#39;, &#39;Scrubbing In with Becca Tilley &amp; Tanya Rad&#39;, &#39;Ringer Dish&#39;, &#39;Know Your Aura with Mystic Michaela&#39;, &#39;Another Bachelor Podcast&#39;] . . get_k_most_similar_podcasts(&quot;And That&#39;s Why We Drink&quot;, als_recommender, podcast_names, 10) . [&#39;Two Girls One Ghost&#39;, &#34;Dude, That&#39;s F****d Up&#34;, &#39;Wine &amp; Crime&#39;, &#39;Ghost Town&#39;, &#39;I Said God Damn! A True Crime Podcast&#39;, &#39;Beyond the Secret&#39;, &#39;Cult Podcast&#39;, &#39;Potterless&#39;, &#39;Death in the Afternoon&#39;, &#39;The Alarmist&#39;] . . Discussion . Despite the fact that the dataset was drastically reduced after curation (removing podcasts with insufficient reviews and so on), the recommender still has 933 podcasts and about 10,607 users to work with, with a total of 40,585 positive ratings. The density is around $0.4 %$, meaning that around $0.4 %$ of all possible ratings (in other words, of all user-podcast pairs) are are actually realized in the data. . While this is a relatively small dataset for collaborative filtering, our recommender did pretty well: . On our test set, the accuracy was $0.09$ which is three times as high as the baseline recommender (which simply recommends the most popular podcasts). Recall that we computed this number by training the recommender while omitting 1 rating per user and then checking how often the omitted podcast was the first recommendation for each user. Getting precisely the omitted podcast as the first recommendation for $9 %$ of users seems pretty good, considering that there are probably many relevant podcasts that the users just haven&#39;t rated yet (we consider those irrelevant by default because we cannot verify their relevance). | When we looked at recommendations of individual podcasts they were very compelling. | Finally, as we described above, there are clear patterns in the latent factors of the podcasts which can be visualized with PCA. We can summarize those findings as follows: The first principal component seems to correspond to a spectrum going from self-improvement to pure entertainment (with true crime at the very end). Along the second principal component the podcasts separate according to whether they are targeted at kids or adults. | . Closing Thoughts . It seems that it was a good choice to turn the star ratings into an implicit dataset, with preferences and confidences. Remember that we did this because the vast majority of ratings give 5 stars, which suggests that a lot of information lies in the podcasts a user did not rate. That information is lost in the explicit paradigm because missing ratings are ignored, unlike in the implicit paradigm, where they are taken into account as low confidence negative preferences. . I noticed that many popular podcasts are missing (based on this list of top 200 podcasts as of early 2022). When I brought this up with the curator of the dataset on Kaggle he confirmed that many podcasts are left out on purpose. However, he admitted that he hadn&#39;t realized how many popular podcasts were missing. This is unfortunate because if we do not know exactly how podcasts have been selected, we cannot correct for sampling bias. . On a related note: Joe Rogan&#39;s immensely popular and controversial podcast is not on Apple Podcasts since 2020, when it became a Spotify exclusive in a deal involving reportedly two hundred million dollars. Nonetheless, it appears many users were still able to leave reviews after the move, and some even wonder in their review why they aren&#39;t able to access the podcast anymore (and sometimes leave a negative rating in protest). This doesn&#39;t seem to have thrown off the recommender, judging by the list of podcasts most similar to &#39;The Joe Rogan Experience&#39;, which seems very appropriate. . The next steps would be to put together a larger dataset in which most popular podcasts are actually included. Then we would tune the hyperparameters of our model and evaluate the model with the best parameters using cross-validation. Note that a larger dataset is needed to properly carry out the parameter search and final evaluation. The parameter search requires cross-validation to evaluate the models with different parameters and this needs to be nested within a larger cross-validation to evaluate the performance of the best parameters found in the search. The nested cross-validation in this context requires removing one rating per user for the outer split and an additional rating per user for the inner split. In our data a majority of users only have 3 ratings, which would leave them with only a single rating in the training set (useless for collaborative filtering). If we wanted to use a better metric such as p@3, a total of 6 ratings per user would be omitted, needing even more ratings per user. .",
            "url": "david-recio.com/2022/03/19/podcasts-recommender.html",
            "relUrl": "/2022/03/19/podcasts-recommender.html",
            "date": " ‚Ä¢ Mar 19, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "What is the effect of flipped classroom groups on grades?",
            "content": "The Covid pandemic arrived in the US while I was a visiting professor at Lehigh University. Like many, we had to quickly adapt to a rapidly evolving situation and transition from a physical classroom one week to a fully virtual format the next. To capitalize on the situation, I implemented a flipped classroom with prerecorded lectures. This entailed the students working together on exercises over Zoom divided into breakout rooms. The breakout room groups were created at random for fairness and retained throughout the whole semester. The main rationale for not changing the groups is that the students needed time to get to know each other and figure out a team-based work flow (students in each group had to prepare shared answers on OneNote). However, the downside was that some groups worked together better than others. . This brings me to the central question motivating this notebook: Does the group each student was (randomly) assigned to make a noticeable difference to that student&#39;s final grade? More precisely, can we detect a statistically significant difference between the grade distributions of the groups? . This is a classic hypothesis testing question: Assuming the groups make no difference at all to the student grades (this assumption is called the null hypothesis), how extreme is the observed data (according to some specified measure called the statistic)? If the data is sufficiently extreme, it might be better explained by an alternative hypothesis, in this case that there actually is some difference in the grade distributions between the groups. . A more general null hypothesis would be that the group means are equal, even if the distributions might differ in other ways. In terms of the grades, testing this hypothesis corresponds to the question: Do the groups have an effect on the expected grade of a student (i.e. the group mean that student is a part of)? . We will use three different models belonging to three different paradigms: nonparametric, semiparametric, and parametric: . Permutation test: This test makes no assumptions on the shape of the probability distributions underlying the data in each group. However, the null hypothesis in this case is very broad (or weak): there is no difference at all between the groups, i.e. all grades come from the same distribution (which is unknown and we make no assumptions on). For instance, the null hypothesis does not include the case in which all groups have normal distributions with the same mean but different variances. That said, the statistic we are using is mostly sensitive to differences in means. | Semiparametric bootstrap: In this case we do make a few assumptions on the grade distributions in each group, but they are quite minimal. To put it simply, we assume that all the group distributions have the same &quot;shape&quot; but allow them to have different means and variances. Crucially, we do not make any assumptions on the &quot;distribution shape&quot;, which is instead approximated by bootstrap resampling. The null hypothesis in this case is more narrow: there is no difference in the means of the groups, i.e. the underlying grade distributions have the same mean but could have different variances. | F-test (One-way ANOVA): This is probably the most commonly used hypothesis test for comparing group means. It requires the strongest assumptions, but it comes with a narrow null hypothesis (the same as the bootstrap) and has higher power than the boostrap method (we will see later that the bootstrap method has low power due to the small group sizes). The assumptions will be explained further bellow, but basically the data is assumed to be normal with equal variances. We will also use the Welch F-test, which corrects for some deviations from these assumptions. | . Outline . This exposition is organized into the following parts: . Preamble: We load the grades and save in a dictionary of named tuples, each containing the final grade and group membership of one section. (I taught three sections of the same course.) | Implementation of the Tests: We motivate the three tests mentioned above and write functions which compute the p-values for each test. We implement the first two from scratch and write a wrapper for the F-test provided by statsmodels. We implement the permutations and bootstrap in a vectorized manner which makes them much faster than the library implementations we found, which are implemented using for loops. | Data Exploration: We visualize the data and test it for normality and heteroscedasticity. The data does not appear to be normal and it has a left skew consistent with an article cited below. We also have a look at the empirical distribution of the residuals used in the semiparametric bootstrap. That distribution does not appear to be a good approximation for the actual underlying grades distribution, which might explain the low power of the semiparametric bootstrap. | Compute p-values: We compute the p-values for our data. | Discussion: Discuss the results, considering issues like multiple comparisons and the power of the tests. | Takeaways | Simulations (APPENDIX): We approximate the power and size of the hypothesis tests with synthetic data. Specifically, we consider several realistic distributions underlying the grades and study the distribution of the p-values in each scenario. | . Preamble . from collections import namedtuple, defaultdict import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats as st from statsmodels.stats.diagnostic import kstest_normal from statsmodels.graphics.gofplots import qqplot from statsmodels.stats.oneway import anova_oneway as statsmodels_ftest from tqdm.notebook import tqdm %matplotlib inline . . First we load the grades. There are three sections, all of the same course. Each section has about 10 groups of around 4 students each. . Some groups have 5 students to limit the number of groups per section. There are several groups which had only 3 students by the end of the semester for two reasons. Firstly, some students dropped the course at some point during the semester. Also, after the semester had already started, I created a new group for students who had to remain in Asia because of travel restrictions. This group met at a different time due to their timezone. That group was excluded from this analysis so as not to interfere with the randomization principle. . In a separate notebook we already computed the final grades for all three sections, after extracting the homework and exam grades from a CSV exported from GradeScope (the platform we used for online submission and grading). In that same notebook we changed the group numbers and the section numbers for anonymity, before saving the final grades (together with the renamed sections and groups) in &#39;anonymized_grades.csv&#39;. . Here we load that CSV file and save the grades in a dictionary, where each item corresponds to a section. The grades and group sizes are stored in a named tuple. . all_grades = pd.read_csv(&#39;data/anonymized_grades.csv&#39;, usecols=[&#39;Section&#39;, &#39;Group&#39;, &#39;Percentage&#39;]) GradesWithGroups = namedtuple(&#39;GradesWithGroups&#39;, [&#39;grades&#39;, &#39;group_sizes&#39;]) section = {} for n in [1, 2, 3]: this_section = all_grades[all_grades[&#39;Section&#39;] == f&#39;Section {n}&#39;].sort_values(by=[&#39;Group&#39;]) grades = this_section[&#39;Percentage&#39;].to_numpy() group_sizes = this_section[&#39;Group&#39;].value_counts(sort=False).to_numpy() section[n] = GradesWithGroups(grades, group_sizes) . Implementation of the Tests . Permutation Test . The permutation test starts with a simple idea: Assuming it is the case that the student groups make no difference at all to the final grades, then those groups are essentially arbitrary. We call this assumption the null hypothesis. To estimate how unlikely our &quot;observed&quot; grades would be under the null hypothesis (that groups have no influence on grades), we repeatedly split the students into random groups (of the appropriate sizes): This allows us to estimate how extreme the differences in grades between the actual groups are relative to the differences in means between the random splits (we will see how to quantify the difference between the groups in a moment). Because under the null hypothesis each split is equally likely, the more extreme the differences between groups, the more unlikely those groups are (if we assume the null hypothesis is true). . How do we quantify how extreme the difference between the groups is? We need a statistic, which is a number that can be can be computed for any grades sample and correlates in some way with group differences. There is no single right choice because there are different ways in which the groups might differ. Since we are mostly interested in the group means, which are the expected grades for a student in each group, we will choose a statistic that is sensitive to differences in the group means. . If we only had two groups we could simply take the difference of the two group means. For several groups we will use the F-test statistic. Denote the grades in group $i$ by $y_{i,j}$ and the size of group $i$ by $n_i$. Furthermore, let $ bar y_i$ be the sample group mean of group $i$ and let $ bar y$ be the overall sample mean (of the pooled grades). We will use the following statistic: $$ F = frac{n_i sum_i ( bar y_{i} - bar y)^2}{ sum_{i,j}(y_{i,j} - bar y_{i})^2} $$ . We are dividing the variance between the groups (called explained variance) by the variance within the groups (called unexplained variance). It is clear the the larger the variance between the groups, the more likely it is that the groups have different true means. The reason we need to divide by the group variances is to control for the fact that more dispersed groups are more likely to give rise to larger variance in sample means due to chance. . Note that we left out some constants that are part of the usual definition of the F-test statistic. This is because a constant factor has no influence on the permutation test. . Before we can explain in detail how the test works, there is still one problem to resolve: Even for relatively small samples there is an astronomical number of permutations, which makes the permutation test intractable (impossible to actually compute except in very small examples). However, essentially the same results can be achieved by simply sampling permutations at random, provided sufficiently many samples are taken (tens of thousands, say). . The p-value under the randomized permutation test is computed as follows: . Resample a large number of random splits of the data (into groups of the right sizes). | Compute $F$ for each permuted sample. | The p-value is given by the fraction of $F$ values which are equal or larger than the value of $F$ for our original data. (Note that this fraction doesn&#39;t change if we multiply $F$ by a non-zero constant.) | . For example, if only $8 %$ of random splits result in a larger $F$ than the original data, the p-value is $0.08$. . The permutation test is nonparametric because it doesn&#39;t require us to postulate a parametric model for the distribution which underlies the observations. For example, a parametric model might be that the observations are drawn from a normal distribution, which is completely determined by two parameters: its mean and its variance. . This lack of parametric assumptions makes the permutation test more flexible and preferable whenever we don&#39;t have a good a priori model of our data and thus no reason to assume that the observations come from a specific distribution. . Implementing the Permutation Test . Because we are going to be taking ten thousand samples or even a hundred of thousand samples every time we perform a permutation test and then computing the statistic for each sampled permutation, we will want to leverage vectorized operations on NumPy arrays to speed the computations up considerably. This is especially important for the simulations at the end of the notebook, where we apply the hypothesis tests $10,000$ times to approximate the p-value distributions for each test. Even with the vectorized code some simulations take half an hour to run on my MacBook Pro. . Below we compare three implementations to sample permutations. . rng = np.random.default_rng() N = 10**5 sample = np.array([0] * 40) . First we use a for loop, which is the simplest for least optimal option. It doesn&#39;t take full advantage of NumPy arrays. . %%timeit permutations = [] for _ in range(N): permutations.append(rng.permuted(sample)) permuted = np.array(permutations) . 399 ms ¬± 23.5 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each) . Next we implement a vectorized approach. In this case that means that instead of using a Python for loop we perform the permutation on the rows of a 2D array, using the NumPy function permuted. The 2D array consists of copies of the sample stacked on top of each other. . %%timeit permuted = rng.permuted(np.stack((sample,) * N), axis=-1) . 129 ms ¬± 2.88 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each) . We see that the vectorized approach is almost 3 times faster in this case. . There is an alternative vectorized implementation which is faster for smaller samples, although it is a little hacky. . %%timeit reindex = np.random.rand(N, sample.size).argsort(axis=1) permuted = sample[reindex] . 161 ms ¬± 43.2 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each) . This approach is fast for small samples but the previous approach has a better time complexity over the sample length (sorting an array has time complexity $ mathcal{O}(n log n)$ but finding a permutation has time complexity $ mathcal{O}(n)$). For samples of size 40 the permuted approach has the upper hand but for size 32 the argsort approach is still clearly faster (the sections have sizes 40, 32, 41). . We will go with the implementation using permuted. . def permute(sample, n_resample=10**5): &#39;&#39;&#39;Take 1D array sample and return 2D array with random permutations of sample as its rows&#39;&#39;&#39; rng = np.random.default_rng() return rng.permuted(np.stack((sample,) * n_resample), axis=-1) . Now we need to implement the test, which involves computing the F-statistic, which in turn requires the group means and group variances. We will actually need to compute those again later on and it will be convenient to have it in a separate function. It will be helpful to store the group means and the group standard deviations using a named tuple. . MeansVars = namedtuple(&#39;MeansVars&#39;, &#39;group_means group_vars&#39;) def take_group_means_and_vars(resamplings, group_sizes): &#39;&#39;&#39;Take 1D/2D array (each row is a resampling) and a 1D array of group sizes. Take the means of slices of the specified group sizes along the rows. Return an array containing the group means (with the same dimensions as the input).&#39;&#39;&#39; left = 0 group_means, group_vars = [], [] for l in group_sizes: right = left + l group_mean = resamplings[..., left: right].mean(axis=-1, keepdims=True) group_var = resamplings[..., left: right].var(axis=-1, keepdims=True) group_means.append(group_mean) group_vars.append(group_var) left = right return MeansVars(np.hstack(group_means), np.hstack(group_vars)) def F_stat(samples, group_sizes, version=&#39;regular&#39;): &#39;&#39;&#39;Compute F-test statistic (up to a constant factor which doesn&#39;t matter for the permutation test) for every row of a 1D/2D array and return an array with the computed values&#39;&#39;&#39; sample_size = samples.shape[-1] group_means = take_group_means_and_vars(samples, group_sizes).group_means pooled_mean = (group_means @ group_sizes / sample_size) if len(samples.shape) &gt; 1: pooled_mean = pooled_mean.reshape(samples.shape[0], 1) explained_variance = np.square(group_means - pooled_mean) @ group_sizes unexplained_variance = np.square(samples - np.repeat(group_means, group_sizes, axis=-1)).sum(axis=-1) return explained_variance / unexplained_variance # This is the F-test statistic up to a constant (degrees of freedom factors) def permutation_test(sample, group_sizes, n_resample=10**5): &#39;&#39;&#39;Compute the p-value according to the permutation test using the F-test statistic&#39;&#39;&#39; resamplings = permute(sample, n_resample) original = F_stat(sample, group_sizes) resampled = F_stat(resamplings, group_sizes) return (original &lt; resampled).mean() . The Semiparametric Bootstrap . On a superficial level, the nonparametric bootstrap (when used in a hypothesis test) is almost the same as the randomized permutation test, in that it involves repeatedly resampling from the actual data and computing a statistic each time. The main difference is that in the bootstrap we resample with replacement, meaning that the same value can be sampled repeatedly. However, conceptually the bootstrap is actually a richer tool, with applications that extend beyond hypothesis testing. . The main idea motivating the bootstrap is to think of the resamplings as new observations: We are drawing samples from the empirical distribution function (ECDF) instead of the &quot;true&quot; distribution underlying our observations. If we have a sufficiently large sample (and this is an important limitation to keep in mind), the ECDF will be a good approximation of the true distribution. The aim of the bootstrap method is usually to approximate the distribution of some statistic over the observations by the distribution of the statistic over the resamplings. . In our situation we could apply the nonparametric bootstrap by: . Resampling from the pooled grades (if we assume that the groups are interchangeable). This would amount to the (randomized) permutation test we implemented above, except for resampling with replacement (which wouldn&#39;t make a big difference to the end result if we take sufficiently many resamplings). | In the latter case we are resampling from very small groups (sizes ranging from 3 to 5) and so the approximation of the distributions of each group given by the bootstrap would be inadequate. | . Instead, we will use the semiparametric bootstrap. This bootstrap relies on a minimal model that allows us to pool the &quot;residuals&quot; from all groups and resample from those. . The Model . We assume that the grades in group $i$ are given by $y_{i,j} = mu_i + sigma_i epsilon_{i,j}$, where we assume that the $ epsilon_{i,j}$ are drawn from the same distribution, but we do not make any assumptions on which distribution that is. In words, we assume that the data in all groups follows the same distribution, except for perhaps having a different mean $ mu_i$ and a different standard deviation $ sigma_i$. I took this model from Example 4.14 in &quot;Bootstrap Methods and Their Application&quot; by Davison and Hinkley. . Our null hypothesis is that all group means are equal: $ mu_1= mu_2= ldots= mu_0$. Thus, the null hypothesis is sharper than for the permutation test (where the null hypothesis is that the distributions of the groups are identical), but we pay the price of having to make assumptions about the distributions. . Bootstrap Resampling . Now we need to take bootstrap resamples to approximate the distributions of each subgroup. What makes the bootstrap (semi) parametric is that we need to estimate some parameters: the group means $ mu_i$ and the group variances $ sigma_i^2$. Furthermore, what distinguishes this method from a fully parametric bootstrap is that we make no assumption on the distribution of the $ epsilon_{i,j}$, other than the fact that it is the same for all groups. In the parametric bootstrap the resamplings come from a distribution such as the normal distribution, which is analytically defined by some parameters (such as the mean and variance in the case of the normal distribution). Instead, we pool all the studentized residuals $e_{i,j}$ (see bellow) and resample from those residuals just like with the nonparametric bootstrap. In other words, we (implicitly) compute an estimated ECDF of the $ epsilon_{i,j}$, under the (parametric) assumptions of our model. . We do the bootstrap resampling under the null hypothesis, i.e. that assuming all the group means are the same. In order to do the resampling, we need to estimate the pooled mean $ mu_0$ and the group variances $ sigma_j^2$, and then use those to compute the studentized residuals $e_{i,j}$. . Let $n_i$ denote the number of students in group $i$, let $ bar y_i$ denote the sample means of each group and let $s_i^2$ denote the sample variances. To estimate the mean under the null hypothesis we weight the sample means with the factors $w_i = n_i/s_i^2$ (which are the reciprocals of the squared standard errors of the sample means). This gives the groups with larger variance a smaller weight to reduce the standard error in the estimate of the pooled mean. The null estimate of the mean is: $$ hat mu_0 = frac{ sum_{i=1}^nw_i bar y_i}{ sum_{i=1}^nw_i}$$ . The null estimates of the variances are given by: $$ hat sigma_{i,0}^2 = frac{n_i-1}{n_i}s_i^2 + ( bar y_i - hat mu_0)^2$$ . Now we have the necessary estimates to compute the studentized residuals: $$e_{i,j} = frac{y_{i,j} - hat mu_0}{ sqrt{ hat sigma_{i,0}^2 - ( sum_i w_i)^{-1}}}$$ . The bootstrap now resamples $ epsilon_{i,j}^*$ from the $e_{i,j}$ and replicates the $y_{i,j}$ as $y_{i,j}^* = hat mu_0 + hat sigma_{i,0} epsilon_{i,j}^*$. . The Statistic . In order to determine how extreme the differences between the group means are in our data we need an appropriate statistic. Following the book I mentioned above, we will use the statistic $$ tilde F = sum_{i=1}^k w_i( bar y_i - hat mu_0)^2.$$ Note that this is similar to the F-test statistic (hence the notation). The larger the differences between the sample means, the more extreme the value $ tilde F$ is. We use the weights $w_i$ introduced above to give sample means from groups with higher variance lower weight. This makes sense because the sample means of groups with higher variance will naturally deviate more strongly from the true mean $ mu_0$ (under the null hypothesis). By multiplying by $w_i$ we are essentially normalizing the squared errors by dividing by the squared standard error of the sample means. . Implementation of the Bootstrap Test . def estimate_params(samples, group_sizes): &#39;&#39;&#39;Takes a 2D array where each row is a sample (or a 1D array with just one sample), as well as an array of group sizes. The grades in each sample are ordered by group. Returns a named tuple (Estimates) with statistics computed from those samples. The computations are performed for all rows in a vectorized fashion.&#39;&#39;&#39; epsilon = 10**-5 # To prevent division by 0 group_means, group_vars = take_group_means_and_vars(samples, group_sizes) weights = group_sizes / (group_vars + epsilon) est_mean = np.expand_dims((group_means * weights).sum(axis=-1) / weights.sum(axis=-1), -1) est_vars = (group_sizes - 1) / group_sizes * group_vars + (group_means - est_mean)**2 residuals = (samples - est_mean) / np.repeat(np.sqrt(est_vars - np.expand_dims(1 / weights.sum(axis=-1), -1) + epsilon), group_sizes, axis=-1) Estimates = namedtuple(&#39;Estimates&#39;, &#39;residuals est_vars est_mean group_means group_vars weights&#39;) return Estimates(residuals, est_vars, est_mean, group_means, group_vars, weights) def bootstrap(original_sample, group_sizes, n_resample=10**5): &#39;&#39;&#39;Takes the data and generates n_resample new samples based on the semiparametric bootstrap introduced above. They are computed in a vectorized fashion and returned as the rows of a 2D array&#39;&#39;&#39; original_estimates = estimate_params(original_sample, group_sizes) original_residuals = original_estimates.residuals rng = np.random.default_rng() resample = rng.choice(np.arange(original_residuals.shape[0]), original_residuals.shape[0] * n_resample) resampled_residuals = original_residuals[resample].reshape(n_resample, original_residuals.shape[0]) replicatations = original_estimates.est_mean + resampled_residuals * np.repeat(np.sqrt(original_estimates.est_vars), group_sizes) return replicatations def F_tilde(samples, group_sizes, estimates=None): &#39;&#39;&#39;Computes the F tilde statistic for a 2D array (where each row is a sample) or a 1D array for a single sample. The statistic is built on top of some other statistics which might be supplied with the estimates keyword argument, to avoid computing them twice.&#39;&#39;&#39; if estimates is None: estimates = estimate_params(samples, group_sizes) return np.sum((estimates.group_means - estimates.est_mean)**2 * estimates.weights, axis=-1) def bootstrap_test(original_sample, group_sizes, n_resample=10**5): &#39;&#39;&#39;Computes the p-value for the bootstrap test using F tilde.&#39;&#39;&#39; replicates = bootstrap(original_sample, group_sizes, n_resample) return np.mean(F_tilde(replicates, group_sizes) &gt; F_tilde(original_sample, group_sizes)) . ANOVA F-test . ANOVA stands for Analysis of Variance and it is a well-known parametric approach to testing for differences of group means. There are several tools within ANOVA. We will use both the basic F-test and the Welch corrected F-test, which is a variant that makes the test more robust. . We actually used the (basic, uncorrected) F-test statistic in the permutation test, but these two tests compute the p-value very differently. For the permutation test, we simply re-compute the statistic repeatedly for many permutations of the original data and see how often the statistic takes values higher than on the original data. This gives us a measure of how extreme the data is. . The F-test, on the other hand, is a parametric test, which means that we have a parametric model for the probability distribution of the statistic (over the data). In other words, if we repeatedly took data samples and computed the statistic for those samples, those values would follow a probability distribution we can compute. In this instance we assume that the F-test statistic follows the so called F-distribution (with the appropriate degrees of freedom). Because the CDF of this distribution is known, we can directly compute which quantile that value belongs to (i.e. how extreme it is) without needing to resample the data. . Of course, the F-test will only give sensible results if the statistic does in fact follow the F-distribution, at least approximately. Thankfully, it can be mathematically proven that the F-test statistic follows the F-distribution if the data satisfies some assumptions: the grades of different students need to be independent of each other and the grades within each group need to come from normal distributions which have the same variance for all groups. . Those assumptions are unlikely to be fully satisfied for our data. The grade distributions is unlikely to be normal and there might be some differences in the group variances (see the Data Exploration section below). However, the test is known to be quite robust against deviations from normality. The Welch correction we will also use makes the test more robust against deviations from heteroscedasticity (different variance between groups). . In the last sections we do some simulations using the empirical distribution function of our pooled grades and find that the F-test does very well, despite the distribution not being normal. Surprisingly, the Welch test doesn&#39;t do that well. This may be in part due to the small group sizes. . Implementation of the F-test . We will just write a wrapper around the F-test included in the scipy library, which does the Welch correction by default. We will compare the results when the variances are assumed to be equal (no correction is applied) and when this assumption is not made and the statistic is corrected to compensate for the possibility of different variances. . def anova(pooled, group_sizes, equal_vars=False): groups = [] left = 0 for size in group_sizes: right = left + size groups.append(pooled[left:right]) left = right if equal_vars: return statsmodels_ftest(groups, use_var=&#39;equal&#39;).pvalue return statsmodels_ftest(groups).pvalue . Data Exploration . In this section we will: . Visualize the grades distribution. | Visualize the residuals used in the bootstrap test with a view to understand its low power. | Check if there is a significant deviation from normality and equal variance between groups (which are both assumptions of the F-test). | . Grades Distribution . Let&#39;s plot a histogram of the pooled grades from all three sections. . all_sections = GradesWithGroups( np.concatenate([section[1].grades, section[2].grades, section[3].grades]), np.concatenate([section[1].group_sizes, section[2].group_sizes, section[3].group_sizes]), ) fig, ax = plt.subplots(figsize=(8, 5)) ax.hist(all_sections.grades, bins=&#39;auto&#39;); . The data appears to have a clear left skew and thus does not look normal (more on that below). This made wonder why the data would differ from normality in this way and if this is a typical grade distribution for an exam. I found a 2019 paper (citation below), in which the authors analyzed 4000 assignments graded on Gradescope and essentially determined that most of the grade distributions were too skewed to be normal. Interestingly, the skewness was usually negative, just as in our case. They found the logit-normal distribution to be a good fit for exam grade distributions. . The authors do not venture to speculate why exam grades tend to have a left skew. One guess I have is that the students with the top grades are not given the opportunity to differentiate more, by having more challenging exam questions. It is also conceivable that there is higher variability among students with lower grades for reasons beyond the specific exam design. This is all just speculation of course, but it would be interesting to investigate. . Full citation of the paper: &quot;Grades are not Normal&quot; by N. Arthurs, B. Stenhaug, S. Karayev, C. Piech, published in Proceedings of the 12th International Conference on Educational Data Mining, Montr√©al, Canada. 2019 . Residuals . We will see in the simulations at the end of this notebook that the bootstrap has very low power when the groups are small, as is the case with our data. This might be due to the fact that the bootstrap method works by taking samples from the empirical distribution function of the residuals, which might not actually be a good approximation of the actual probability distribution of the groups, due to the small sizes of the groups. . Below we visualize the empirical distribution functions of the residuals for section 10 (the results are similar for the other two sections). We also compare them to synthetic data in two scenarios: . All groups have the same normal distribution and the group sizes are the same as in Section 1. | All groups have the same normal distribution and there are 10 groups of size 8. (For comparison, there are 10 groups in Section 1, mostly of size 4). | . fig, axs = plt.subplots(3, 2, figsize=(12, 8)) fig.tight_layout() axs[0, 0].hist(section[1].grades, bins=&#39;auto&#39;) axs[0, 0].set_title(&#39;Section 1 grades&#39;) axs[0, 1].hist(estimate_params(*section[1]).residuals, bins=&#39;auto&#39;) axs[0, 1].set_title(&#39;Section 1 residuals&#39;) synthetic_normal_small = GradesWithGroups(st.norm.rvs(size=section[1].group_sizes.sum()), section[1].group_sizes) synthetic_normal_large = GradesWithGroups(st.norm.rvs(size=100), np.array([10] * 10)) axs[1, 0].hist(synthetic_normal_small.grades, bins=&#39;auto&#39;) axs[1, 0].set_title(&#39;Synthetic normal data with small groups&#39;) axs[1, 1].hist(estimate_params(*synthetic_normal_small).residuals, bins=&#39;auto&#39;) axs[1, 1].set_title(&#39;Synthetic normal data with small groups residuals&#39;); axs[2, 0].hist(synthetic_normal_large.grades, bins=&#39;auto&#39;) axs[2, 0].set_title(&#39;Synthetic normal data with large groups&#39;) axs[2, 1].hist(estimate_params(*synthetic_normal_large).residuals, bins=&#39;auto&#39;) axs[2, 1].set_title(&#39;Synthetic normal data with large groups residuals&#39;); . The histograms seem to make clear that the empirical distributions of the residuals (pictured on the right) are not a good approximation of the underlying distribution (pictured on the left). This holds also for the synthetic data. These small sample effects might explain the low power of the bootstrap we will observe in the simulations below and why the power of the bootstrap converges with the other tests for larger group sizes. . Normality . We observed that the grade distribution looks too skewed to be normal. One way to quantify this is to compute the sample skewness and sample kurtosis. Of course, we expect these two quantities to be close to 0 if the data is sampled from a normal distribution (note that by default the scipy kurtosis function subtracts 3 from the fourth moment, which makes the kurtosis of the normal distribution 0). We will compare them to a random normal sample of the same size as our data to get an idea of how close to 0 the sample skewness and sample kurtosis usually are. . Another common method to visualize the data to see if it looks normal is the QQ plot, where the quantiles of the sample distribution are plotted against the quantiles of the normal distribution. We will again do this for both our grades data and for comparable synthetic data from a normal distribution. . qqplot(all_sections.grades) print(f&#39;The sample skew is {st.skew(all_sections.grades)} and the sample kurtosis is {st.kurtosis(all_sections.grades)}.&#39;) . The sample skew is -1.0902692480611051 and the sample kurtosis is 1.3753992488399467. . Now we compute compute the sample skew and kurtosis for a random sample of a normal distribution, as a reference for our data. We also look at the QQ plot. . normal_sample = st.norm.rvs(size=110) qqplot(normal_sample) print(f&#39;The sample skew is {st.skew(normal_sample)} and the sample kurtosis is {st.kurtosis(normal_sample)}.&#39;) . The sample skew is -0.1817125808235642 and the sample kurtosis is -0.34271547675743186. . After running the cell above a couple of times, it is evident that a sample skew of -1 and a sample kurtosis of 1.4 are very unlikely for data sampled from a normal distribution. What this means is that the the grades distribution is skewed to the left (as we observed above) and has a longer tails than the normal distribution (a long left tail, really). . This can also be seen in the QQ plots, from the fact that the graph is steeper at the beginning than towards the end. For a normal sample the QQ plot is expected to be close to a line of slope 1. . For a more principled way to determine significant deviation from normality we apply the Kolmogorov‚ÄìSmirnov test. This test directly measures how different the observed distribution is from the normal distribution. First it computes the maximum distance between the empirical cumulative distribution function and the cumulative distribution of the normal distribution and then it determines how extreme this distance would be if we assume that the data does come from a normal distribution. The p-value it outputs is the probability of observing a difference as large as the one observed or larger for samples from a normal distribution. . kstest_normal(estimate_params(*all_sections).residuals)[1] . 0.044937074410551656 . The deviation from normality is significant, under the usual level for significance of 0.05. Specifically, what this means is the only in 4.5% of the cases would a sample from a normal distribution lead to such an extreme difference between the distribution of the data and the underlying normal distribution. This reinforces our analysis above. . Equal Variances . The Levene test is more suitable than than Bartlett test because the data significantly deviates from normality. . def test_equal_variances(pooled, group_sizes): groups = [] left = 0 for size in group_sizes: right = left + size groups.append(pooled[left:right]) left = right return st.levene(*groups)[1] for n in [1, 2, 3]: levene = test_equal_variances(*section[n]) print(f&#39;For Section {n} we have Levene p-value {levene}&#39;) . For Section 1 we have Levene p-value 0.8941346714392719 For Section 2 we have Levene p-value 0.9712056919863893 For Section 3 we have Levene p-value 0.9326385373075483 . The test does not detect significant differences in the variances. However, this should be taken with a grain of salt because the groups are so small that the test probably has a very small power. One indication that the test is not working well, at least in the case of comparisons between groups within each section, is how all three p-values are so high. This is because a well-behaved test should yield p-values which are uniformly distributed under the null hypothesis and which tend to have small values under the alternative hypothesis. . Compute p-values . In the following we compare the p-values from four hypothesis tests: . the (non-parametric) permutation test. | the semiparametric bootstrap. | the (parametric) ANOVA F-test. | the (parametric) ANOVA Welch F-test. | . We use all four tests to check for significant differences between the groups in each of the three sections. . index = [] pvalues = {&#39;Permutation Test&#39;: [], &#39;Semiparametric Bootstrap&#39;: [], &#39;ANOVA&#39;: [], &#39;ANOVA (Welch)&#39;: []} for n in [1, 2, 3]: pvalues[&#39;Permutation Test&#39;].append(permutation_test(*section[n])) pvalues[&#39;Semiparametric Bootstrap&#39;].append(bootstrap_test(*section[n])) pvalues[&#39;ANOVA&#39;].append(anova(*section[n], equal_vars=True)) pvalues[&#39;ANOVA (Welch)&#39;].append(anova(*section[n])) index.append(f&#39;Section {n} Groups&#39;) . pd.DataFrame(pvalues, index=index) . Permutation Test Semiparametric Bootstrap ANOVA ANOVA (Welch) . Section 1 Groups 0.18275 | 0.36078 | 0.188831 | 0.210221 | . Section 2 Groups 0.05562 | 0.48233 | 0.053237 | 0.129899 | . Section 3 Groups 0.38250 | 0.87914 | 0.392885 | 0.851698 | . Discussion . The p-values . The first thing that jumps out at us is that the p-values of the permutation test are almost identical to the p-values of the F-test without Welch correction. In the simulations done at the end of this notebook these two tests have persistently similar p-values. While we used the same (uncorrected) F-test statistic for the permutation test, it cannot be assumed that the p-values will be so close, considering that the tests compute the p-values in very different ways: the former simply recomputed the statistic for many permutations of the data, while the latter compares the value of the statistic to a theoretical distribution (called the F-test distribution). This seems to indicate that F-test is working well despite the data not being normally distributed. . As for the statistical significance of differences in grades between the groups: Based on the permutation test (or equivalently the uncorrected F-test) alone, there are indications of a small effect, especially in Section 2, but unfortunately it is not statistically significant. The usual convention is to set the rejection threshold at $0.05$. Section 2 comes close but we need to consider that we made multiple comparisons (see below). . Furthermore, the results suggest that the semiparametric bootstrap has low power, probably due to the small group sizes. At the end of this notebook we investigate the size and the power of the tests with various simulations. The results there show that the bootstrap does indeed have very low power when the groups are small (such as in our data). . Multiple Comparisons . Recall that the p-value tells us what the probability is of observing data as extreme or more extreme than the data we actually observed, assuming the null hypothesis is true (or perhaps an approximation of this probability). For example, according to the permutation test the probability that the distributions of the different groups in Section 2 are exactly the same is only $5.5 %$. Usually results are considered statistically significant if the p-value is under $0.05$, although this just a convention which is intended to keep the rate of false discoveries in scientific publications low (this threshold has been often criticized, as has the over-reliance on p-values more generally, but we will not get into that here). . However, that is only true for one test in isolation. Unlikely results are more likely to occur if we make multiple observations and it is very important to take this into account. . We will go through two different methods for handling so called multiple comparisons. To be concrete, let&#39;s say we set the significance level at $0.1$. Recall that the permutation test p-values for each of the three sections are $0.18$, $0.05$ and $0.38$. . Bonferroni: Divide the level $0.1$ by the number of comparisons to get the rejection threshold. In this case we would only reject the null if a p-value is under $0.1/3 = 0.0 overline{3}$. Because none of our p-values is under that threshold, we cannot reject the null in any of the three cases. This method ensures the probability of a single false positive will be no higher than $0.1$. The problem is that this also limits the true positives, i.e. rejections of null hypothesis which is false and should be rejected. We say that it increases the type II error rate and that it decreases the power. The next method strikes a balance between keeping false positives down while not reducing true positives too much. . Benjamin-Hochberg: This one is a little more involved than Bonferroni. It ensures that on average at most $10 %$ of all rejections will be false positives. Another way of putting it is that BH ensures that most of the rejections (discoveries) are in fact correct. To be clear, it does not ensure that the probability of making even a single type I error is at most $0.1$. In fact, BH (or variations of it) is often used in cases where thousands of comparisons are made (such as in genomics) where many true positives are expected and a small fraction of false positives is a price worth paying. . It is worth emphasizing that BH works regardless how many of the null hypotheses tested are in fact true. If all null hypotheses happen to be false, then of course $100 %$ of rejections will be false positives. However, BH ensures that this happens only $10 %$ of the time (if we set the level to $0.1$). Thus the expected false positive ratio is less than $0.1$. . Let&#39;s demonstrate BH with our p-values above. Sort the p-values: $p_{(1)} = 0.05$, $p_{(2)} = 0.18$ and $p_{(3)} = 0.38$. To find the rejection threshold using BH, we look for the largest p-value $p_{(i)}$ satifying $p_{({i})} le 0.1 cdot i/m$ ($m$ is the number of p-values, in this case 3). To put it in words, we need the p-value not just to be bounded above by the level of the test (in this case $ alpha=0.1$), but in addition to this to be bounded above by a fraction of the level $ alpha$ which is equal to the fraction of p-values smaller than or equal to $p_{(i)}$. As a hypothetical example, if we had a p-value $p_{(i)}= alpha/2$, then we could only use it as the rejection threshold if at least half of the p-values are smaller than this potential threshold $ alpha/2$. In our example, there is no such p-value because even $p_{(1)} = 0.05$ does not satisfy the condition: $0.05 &gt; 0.1 cdot1/3$. . Actually, in this case Bonferroni and BH give similar results regardless of how we set $ alpha$ because one p-value is so much smaller than the others. If we let the level be $0.15$ both would reject one null hypothesis. We would need to raise the level all the way up to $0.27$ for BH to reject two null hypotheses and to $0.38$ for BH to reject all three. Bonferroni would reject two at $ alpha=0.54$ and there is no level at which all three would be rejected. . Such high p-values would not really be very sensible in practice, of course. It should also be made clear that the level needs to be set in advance and not adapted to the experiment. The previous paragraph is just intended to clarify the differences between the two methods. . To consider an extreme example, if all p-values happen to be less than $ alpha$, then every single hypothesis is rejected because the largest p-value would be chosen as the threshold. The main intuition behind BH is that the more concentrated the p-values from multiple tests are closer to 0, the more null hypotheses must be true (or are at least expected to be true). A logically equivalent way of saying this is that p-values would be expected to be spread out if many null hypotheses are true since p-values should be uniformly distributed (between 0 and 1) under the null hypothesis. . Conclusions . Unfortunately, the differences in mean grades between the groups are not statistically significant. One of the p-values is 0.05, but taking into account the multiple comparisons we would have had to put the rejection threshold at 0.15 (this is true for Benjamin-Hochberg, not just for the more conservative Bonferroni). That threshold would be quite high. To be clear, the rejection threshold needs to be set before looking at the results for it to work as intended. The whole situation is also complicated by the fact that we did several tests. This certainly increases the likelihood of false positives but it is tricky to say by how much because the p-values of different tests are not independent of each other. We did this more as an exercise, rather than to get significant results for the group differences (it became clear early on that the group sizes are too small for significant results). . Of course, the results do not necessarily imply that there is a high probability that the group means are the same. As can be seen in the simulations below, in the more realistic scenarios all four hypothesis tests have pretty low power, due to the small sample sizes. This means that even if there were a meaningful effect it would be unlikely to be discovered by these tests (or probably any reasonable tests) unless it were very large. The only scenario in which the power of the tests is above $0.8$ is when we assume that the true group means vary as much as the sample group means (i.e. if there is a pretty large effect size). . To actually find the probability of a particular effect size in a principled way we would need to turn to Bayesian statistics. In a nutshell, we would need to come up with a prior (the credence we would we give to each scenario before we even look at the data) and then update this prior using the likelihood function (probability of seeing our data under each scenario). This would be an interesting direction to explore in the future. . One big takeaway is that the semiparametric bootstrap has especially low power, even with large effect sizes. In simulations with larger groups (size 10 specifically) the power of the bootstrap is similar to the other tests, which confirms that the small group sizes are the problem. Evidently, larger groups are needed for the bootstrap resamplings to adequately approximate the underlying distribution of the residuals. Recall that we avoided a purely nonparametric bootstrap which resamples from the individual groups because of their small sizes. We had hoped that combining the residuals of all groups using the semiparametric bootstrap would mitigate this, but this clearly failed. To salvage the bootstrap approach would need to either increase the group sizes or modify the underlying model in some way. . APPENDIX: Simulations of Power and Size . In this last section we simulate alternative hypotheses to investigate the size and power of the three tests through simulation. . Power and Size . The size of a test is easy to explain. It is simply the probability of rejecting the null hypothesis when it is in fact true. We want to keep the size as small as possible, of course. Crucially, the size is not supposed exceed the level of the test. Similarly, the power of a test is the probability of rejecting the null hypothesis if the alternative hypothesis is true. Just like we want to minimize the size, we also want to maximize the power and those two aims are always in tension. . To be fair, the concepts are a little trickier to make precise than the previous paragraph makes it seem. In the following two points we go into the weeds a bit more: . The alternative hypothesis is usually defined as the logical negation of the null hypothesis, but not always. For example, if the null hypothesis is that the means of all groups are equal, then the logical negation would yield an alternative hypothesis consisting of all situations in which the means differ in any way. However, maybe we want to restrict the alternative hypothesis to include only differences in means that would be significant in a certain context (that is, maybe we are only interested in rejecting a hypothesis when there is a minimal effect size). See the Neyman-Pearson Lemma for an example of an alternative hypothesis which is not the negation of the null hypothesis. | Another technicality is that both the the null hypothesis and the alternative hypothesis usually are usually composite hypotheses, meaning that they don&#39;t simply say that the data comes from a specific probability distribution, but rather from a set of possible probability distributions. For example, if the null hypothesis is that the means of the groups are equal, that doesn&#39;t fully determine the probability distribution at all. In truth, the size of a test is the supremum (which can be roughly thought of as the maximum) of the probability to reject the null hypothesis under a specific probability distribution which is part of the null hypothesis, where the supremum is taken over all those probability distributions. The same thing goes for the power and the alternative hypothesis. | . In the simulations we can&#39;t consider all scenarios and take the supremum to actually compute the power and size over the whole space of possibilities. Instead, we will contemplate some realistic scenarios for our situation. We do this by drawing samples from probability distributions which seem reasonable models for our data, apply our tests and finally inspect the distribution of the resulting p-values. If the tests work well, they will be uniformly distributed whenever the null hypothesis is true and concentrated towards small values when the null hypothesis is false. . Simulations Setup . In all simulations we will assume that the grade distributions have the same &quot;shape&quot; for all groups, meaning that they are the same up to shifting and scaling. We will use both the empirical distribution function (ECDF) of the pooled grades and the normal distribution. . Beyond this, the three parameters we can tune are the means, variances and group sizes. We will consider cases in which the true group means are equal, differ slightly and differ strongly. Similarly, we will consider cases in which the true group variances are equal, differ slightly or differ strongly. Finally, we will do simulations with small group sizes and with large group sizes. Specifically, we will use the exact group sizes of Section 1 (which happens to have 10 groups) or 10 groups of size 10. . All in all, there are 36 possible scenarios. To avoid clutter we will only include a subset of those. That will allow us to compute thousands of p-values in each case, which can take a long time because we are doing bootstrap and permutation resamplings. . To ensure the simulations are relevant to our actual grades data, we use the sample group means and sample group standard deviations. . Constant mean and variance: median group mean and average group variance over all groups from all three sections. | Weakly varying means and variances: a random sample of 10 out of middle $33 %$ of the group means and group variances over all groups (i.e. excluding the top third and bottom third). | Strongly varying means and variances: a random sample of 10 out of middle $80 %$ of the group means and group variances over all groups (i.e. excluding the top $10 %$ and bottom $10 %$). | Extremely varying means: a random sample of 10 among all group means. | . Simulation Results Overview . All tests have low power, especially the bootstrap: As we had anticipated above, the semiparametric bootstrap has a very low power when the groups are small (around size 4, as is the case in our data). It does similarly to the other tests when we increase the group size to 10. Something else we see in the simulations is that all the tests have low power, unless we let the group means vary a lot. . How much the true means vary matters a lot: In the following we assume that the group standard deviations are equal. . For weakly varying means (chosen from middle $33 %$ of sample group means, ranging from $85.0$ to $87.4$) the rejection rate is $7 %$, almost the same as under the null hypothesis. | For strongly varying means (chosen from middle $80 %$ of sample group means, ranging from $82.8$ to $90.2$) the rejection rate is $17 %$, which is still way too low. | For very strongly varying means (chosen from $100 %$ of sample group means, ranging from $73.6$ to $92.5$) the rejection rate is $85 %$, which is reasonable. | . Poor performance of the Welch F-test: It is surprising (at least to me) that the uncorrected F-test did better than the Welch F-test, considering that it is widely recommended to use the Welch test in all circumstances. Especially considering that we violated the normality and equal variances assumptions of the F-test and we have unequal group sizes, which is precisely when the Welch F-test should be doing better. My guess is that the Welch correction is not working well with the very small groups in our data. For larger groups it does better. . The group sizes are very important: Unsurprisingly, increasing the group sizes to 10 increases the power of all tests significantly. This is especially the case for the bootstrap test. . Close enough to normal: Even though the empirical distribution function is too skewed to be normal, it seems to be close enough for the F-tests. In the simulations it made almost no difference whether we used the empirical distribution or the normal distribution (not all simulations are included below). . Bimodal p-value distributions: Interestingly, when we have equal means and let the group standard deviations vary strongly, the p-value distributions for the permutation test and uncorrected F-test become bimodal: there is a peak at 0 and a peak at 1. The peak at 0 corresponds to samples with very small variation between sample group means (the groups form one big cluster), while the peak at 1 corresponds to sampled with very large variation between sample group means (the groups are spread into separate clusters). Remember that this case (equal means and unequal variances) is not included in the null hypothesis for the permutation test. It also violates the assumptions of the non-corrected F-test (ANOVA). Thus, a uniform p-value distribution was definitely not expected in those cases. What we might have expected for the permutation test is that the p-value distribution has only one peak, namely at 0. However, the statistic we used in the permutation test (the F-test statistic) is meant to capture differences between group means, not group standard deviations. This is presumably why we see this double peak and consequent low power. The bootstrap test and the Welch F-test did not result in bimodal p-value histograms in any of our simulations. For large group sizes the p-values are close to uniformly distributed and the rejection rate is close to the level. This is as it should be because the former are precisely designed to handle variation in the standard distributions. . Simulation Results . First we need to compute the group means and standard deviations, as well as the empirical distribution, to be used in the simulations below. . all_group_means = take_group_means_and_vars(*all_sections).group_means all_group_stds = np.sqrt(take_group_means_and_vars(*all_sections).group_vars) central_mean = np.median(all_group_means) central_std = np.median(all_group_stds) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4)) fig.tight_layout() ax1.hist(all_group_means, bins=&#39;auto&#39;) ax1.set_title(&#39;Group means&#39;) ax2.hist(all_group_stds, bins=&#39;auto&#39;) ax2.set_title(&#39;Group stds&#39;) print(f&#39;The median of the group means is {central_mean} and the median of the group standard deviations is {central_std}.&#39;) . The median of the group means is 86.5875 and the median of the group standard deviations is 6.331982950879348. . section1_group_sizes = section[1].group_sizes section1_n_groups = section1_group_sizes.size large_group_sizes = np.array([10] * section1_n_groups) # Truncated lists of group means and group stds total_number_of_groups = all_group_means.size third_group_means = np.sort(all_group_means)[total_number_of_groups // 3: 2 * total_number_of_groups // 3] third_group_stds = np.sort(all_group_stds)[total_number_of_groups // 3: 2 * total_number_of_groups // 3] most_group_means = np.sort(all_group_means)[total_number_of_groups // 10: 9 * total_number_of_groups // 10] most_group_stds = np.sort(all_group_stds)[total_number_of_groups // 10: 9 * total_number_of_groups // 10] # Sample of means and stds for use in the simulations rng = np.random.default_rng(42) constant_means = np.array([central_mean] * section1_n_groups) constant_stds = np.array([central_std] * section1_n_groups) weakly_varying_means = rng.choice(third_group_means, size=section1_n_groups) weakly_varying_stds = rng.choice(third_group_stds, size=section1_n_groups) strongly_varying_means = rng.choice(most_group_means, size=section1_n_groups) strongly_varying_stds = rng.choice(most_group_stds, size=section1_n_groups) very_strongly_varying_means = rng.choice(all_group_means, size=section1_n_groups) # Normalized empirical distribution normalized_ecdf = (all_sections.grades - all_sections.grades.mean()) / all_sections.grades.std() . Finally, we write two functions for the simulations to do the following: . Draw samples from some given group distributions and compute the p-value distributions of each hypothesis test. | Plot the p-value histograms for all the tests in a grid. | . def simulate_pvalues(group_sizes, group_means, group_stds, dist, n_sims, level): &#39;&#39;&#39;Take n_sims samples with distribution dist and the given group means, group variances and group sizes. For each sample, compute the p-value for each of the four hypothesis tests. &#39;&#39;&#39; coverage, pvalues = defaultdict(int), defaultdict(list) rng = np.random.default_rng() for _ in tqdm(range(n_sims)): group_samples = [] for size, mean, std in zip(group_sizes, group_means, group_stds): if isinstance(dist, np.ndarray): # dist is the normalized ECDF sample = mean + rng.choice(dist, size) * std group_samples.append(sample) elif dist == &#39;normal&#39;: sample = st.norm.rvs(loc=mean, scale=std, size=size, random_state=rng) group_samples.append(sample) else: raise Exception(f&#39;Invalid dist argument: {dist}&#39;) pooled_sample = np.concatenate(group_samples) pvalue = permutation_test(pooled_sample, group_sizes, n_resample=10**4) pvalues[&#39;permutation&#39;].append(pvalue) coverage[&#39;permutation&#39;] += pvalue &lt;= level pvalue = bootstrap_test(pooled_sample, group_sizes, n_resample=10**4) pvalues[&#39;bootstrap&#39;].append(pvalue) coverage[&#39;bootstrap&#39;] += pvalue &lt;= level pvalue = anova(pooled_sample, group_sizes, equal_vars=True) pvalues[&#39;ANOVA&#39;].append(pvalue) coverage[&#39;ANOVA&#39;] += pvalue &lt;= level pvalue = anova(pooled_sample, group_sizes) pvalues[&#39;ANOVA (Welch)&#39;].append(pvalue) coverage[&#39;ANOVA (Welch)&#39;] += pvalue &lt;= level for key in coverage: coverage[key] /= n_sims return coverage, pvalues . def plot_histograms(pvalues_dict): fig, axs = plt.subplots(2, 2, figsize=(10, 6)) positions = [(0, 0), (0, 1), (1, 0), (1, 1)] for position, key in zip(positions, pvalues_dict): axs[position].hist(pvalues_dict[key], bins=&#39;auto&#39;) axs[position].set_title(key) . Groups Have Equal Distributions . In the following we simulate the case in which the grade distributions are exactly the same for all groups. . This is precisely the null hypothesis of the permutation test and it is contained in the null hypothesis of the other two tests (equal means). Therefore, we would expect the p-values to be uniformly distributed if the tests are working well. . We start with the normal distribution, which satisfies the assumptions of the F-test. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, constant_stds, &#39;normal&#39;, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0463 bootstrap 0.0110 ANOVA 0.0469 ANOVA (Welch) 0.1073 dtype: float64 . Firstly, the permutation test and the uncorrected F-test (ANOVA) have almost identical p-values and they are also performing the best by far. Their rejection rates are just under the level $0.05$, which is perfect. The distributions look uniformly distributed. . The p-values for the semiparametric bootstrap and the Welch corrected F-test are clearly not uniformly distributed. . For the bootstrap the actual size turns out to actually be smaller than the level. This is not in itself undesirable but it indicates that the power will be very low. . Let&#39;s see if the bootstrap and Welch F-test do better for larger groups: . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, constant_means, constant_stds, &#39;normal&#39;, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0474 bootstrap 0.0428 ANOVA 0.0475 ANOVA (Welch) 0.0578 dtype: float64 . From what we have seen above, it seems that the bootstrap and Welch F-test are not performing well for the small group sizes present in our data (around 4), while doing reasonably well for size 10 groups. The permutation test and the regular uncorrected F-test (one-way ANOVA) are still the best but not by much. . Next, let&#39;s use the actual empirical grades distribution instead of the normal distribution. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0551 bootstrap 0.0127 ANOVA 0.0543 ANOVA (Welch) 0.1454 dtype: float64 . Now let&#39;s make the groups larger again. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, constant_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0486 bootstrap 0.0499 ANOVA 0.0480 ANOVA (Welch) 0.0758 dtype: float64 . We can see that the empirical distribution and the normal distribution yield pretty results. The biggest difference is for the Welch F-test, which has an even worse rejection rate for the empirical distribution. This might be due to the long left tail. We will only use the empirical distribution function from now on. . Groups Have Equal Means but Varying Standard Deviations . Now we are strictly speaking stepping out of the null hypothesis of the permutation test, which requires standard deviations to be equal too (although the statistic we chose makes it mostly sensitive to the means). We are still within the null hypothesis of the other tests. Again, we would expect uniformly distributed p-values if the tests are working as they should. . First let us vary the standard deviations only slightly. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, weakly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0512 bootstrap 0.0145 ANOVA 0.0509 ANOVA (Welch) 0.1438 dtype: float64 . The p-values are almost identical to the case with constant standard deviation. They are very slightly larger in this case (as was expected) but not significantly. The same is true for larger group sizes: . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, constant_means, weakly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0493 bootstrap 0.0508 ANOVA 0.0472 ANOVA (Welch) 0.0730 dtype: float64 . If we let the standard deviation vary strongly between the groups something interesting happens: some p-value distributions have a peak at 1 as well as the usual peak at 0. This is the case also for very large group sizes. For instance, we let the groups be of size 50 below, which is much larger than the groups we have been considering. The peaks do not seem to change much as we vary the group sizes from 4 to 10 to 50. . When the groups are this large and the group standard deviations differ strongly the bootstrap and the Welch F-test do better than the permutation test and regular F-test. This is as it should be because the former are precisely designed to handle variation in the standard distributions. It is worth noting that this is the only case in which the bootstrap and the Welch F-test seem to be doing better than the other two, assuming our goal is to detect differences in means (which, to be fair, is not really the aim of the permutation test). . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, strongly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0672 bootstrap 0.0174 ANOVA 0.0654 ANOVA (Welch) 0.1576 dtype: float64 . n_sims = 10**4 level = 0.05 very_large_group_sizes = np.array([50] * 10) result = simulate_pvalues(very_large_group_sizes, constant_means, strongly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0679 bootstrap 0.0535 ANOVA 0.0680 ANOVA (Welch) 0.0598 dtype: float64 . Unequal means . Finally, we have arrived at the arguably more important alternative hypothesis: there is a difference in the expected grade (mean) of the groups. . First, we will assume that the means vary weakly to see if the tests would pick up on that. Later, we will let the means vary more strongly. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, weakly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0727 bootstrap 0.0185 ANOVA 0.0715 ANOVA (Welch) 0.1585 dtype: float64 . We see that the tests are very weak in this case, due to the small effect size (weakly varying means) and the small group sizes. The bootstrap is still performing terribly and the permutation tests and ANOVA rejection rates are barely higher than the level of the test. . The Welch F-test has the highest power, but this is useless given that the rejection rate is pretty much the same as it was when the null hypothesis was true (making it impossible to distinguish between true and false positives). . Let&#39;s see if the power increases with larger group sizes: . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, weakly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.1241 bootstrap 0.1120 ANOVA 0.1218 ANOVA (Welch) 0.1531 dtype: float64 . Leaving the Welch F-test aside, the power has increased for larger groups, but it is still not great. It is worth noting that the bootstrap now yields almost the same p-value distribution as the permutation test and regular F-test, underscoring how the bootstrap does fine for larger groups, but just can&#39;t deal with the small groups in our data. . Next we will let the group means vary strongly. Recall that we randomly picked 10 the means from the middle $80 %$ sample group means. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, strongly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.1710 bootstrap 0.0329 ANOVA 0.1704 ANOVA (Welch) 0.2581 dtype: float64 . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, strongly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.4901 bootstrap 0.3989 ANOVA 0.4847 ANOVA (Welch) 0.4882 dtype: float64 . As we can see, going from the middle $33 %$ group means to the middle $80 %$ group means made an enormous difference. At around $0.4$, the power is still not great. . One way to increase the power is increasing the effect size even more. In the following, we will randomly choose from $100 %$ of the group means (what we called very strongly varying means). . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, very_strongly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.8508 bootstrap 0.2242 ANOVA 0.8502 ANOVA (Welch) 0.7863 dtype: float64 . We see that in the extremely varying means scenario the power of all tests other than the bootstrap is reasonably good ($0.85$). Surprisingly, the Welch corrected F-test has a lower power that the uncorrected F-test in this case, even though in most other cases it had a higher power (even too high, for equal means). .",
            "url": "david-recio.com/2022/03/19/grades-analysis.html",
            "relUrl": "/2022/03/19/grades-analysis.html",
            "date": " ‚Ä¢ Mar 19, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Math PhD, self-taught in Python, stats, and machine learning while my Green Card application was pending. Currently I am a Software Engineer at Mobi. Before that I was a postdoc at Lehigh University doing research on Topological Robotics. . Grew up in Spain üá™üá∏, studied math and physics in Germany üá©üá™, and did my PhD in the UK üá¨üáß. . Now a US permanent resident üá∫üá∏ based in Boston. . You can view my publications here. Note that I used my double surname Recio-Mitter on my articles. (In Spain everyone has a double surname but now that I am in the US I use my paternal surname to avoid confusion.) .",
          "url": "david-recio.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Contact",
          "content": "Email address Name Message Submit",
          "url": "david-recio.com/contact/",
          "relUrl": "/contact/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "david-recio.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}