{
  
    
        "post0": {
            "title": "The Latent Space of Podcasts",
            "content": "Nowadays we encounter recommender systems on a daily basis in search engines, streaming platforms, and social media. There exist many different mechanisms behind recommender systems, but we will focus on a a class of methods known as collaborative filtering. In a nutshell, this approach consists of taking the set of all known user preferences and using that to &quot;predict&quot; the user&#39;s preference for an item (movie, song, news article) that the user hasn&#39;t seen yet (or for which the user hasn&#39;t indicated a preference). The basis for establishing this preference depends on the context. Some examples include user ratings on Netflix, or how many times a user has listened to a song on Spotify. . Collaborative filtering relies on the assumption that similar users will like similar items. Furthermore, similarity is derived solely from the known user preferences, such as ratings, without any knowledge of the content of the items. Note that in practice only a tiny fraction of all user preferences are known. For example, Netflix users will only have watched a small fraction of all available content. . I find matrix-based collaborative filtering methods especially interesting. In those methods both the users and the items are represented by vectors in some high-dimensional space, called latent factors, which encapsulate both user preferences and item similarity: Vectors for two similar items (or for a user with a positive preference for an item) point in similar directions. . This latent space reflects patterns or structures in the set of items (for example movie genres), which we can visualize. For this we will need dimensionality reduction techniques, such as Principal Component Analysis, or PCA. It is interesting to see which structures emerge just from the set of user preferences, without providing any information about the items or users themselves. It is a useful check for our intuitions in regards to which items are more similar based on concepts like music style or movie genres. . Learning about this made me wonder which patterns the latent space of podcasts might reveal, given that I am a big fan of podcasts myself. This has likely already been studied internally by companies such as Apple and Spotify, but I haven&#39;t found any publicly available recommender system for podcasts. I imagine that part of the reason is the lack of large open access datasets, which do exist for movies, music, and books. This is probably because the mainstream appeal of podcasts is a relatively recent phenomenon. . Luckily I was able to find one pretty decent dataset of podcasts reviews on Kaggle. It consists of almost a million reviews for over 46,000 podcasts, stored in an SQLite database. Thanks to Stuart Axelbrooke for collecting the reviews and making them available for everyone! . We will use this dataset to create a recommender and visualize the latent factors for some popular podcasts. Before we can do that we will need to clean the data first. The data is a bit more raw than some more mainstream recommeder datasets like MovieLens. . import sqlite3 import pandas as pd import numpy as np from implicit.als import AlternatingLeastSquares from implicit.evaluation import precision_at_k, leave_k_out_split from scipy import sparse from sklearn.decomposition import PCA import matplotlib.pyplot as plt . . Import from SQLite . The whole data is in an SQLite file. The SQLite database contains three tables: . podcasts table containing the podcast ID, name and URL. | reviews table containing all the information associated with every review: the star rating, the review title and content, the date and time it was posted, and finally the author ID of the user who made the review as well as the podcast ID. | categories table, which simply contains a column with podcasts IDs and a column with categories into which to those podcasts have been classified. | . We will load the data from the SQLite file into a pandas DataFrame. Specifically, we will take a left join of the podcasts table and reviews table and select a subset of the columns. . For our purposes we will not need the review title and content. However, it would be interesting to do some NLP on the contents as a future project. Maybe some topic modeling which can be combined with collaborative filtering in a hybrid recommender system. . con = sqlite3.connect(&#39;data/database.sqlite&#39;) . get_ratings = &quot;&quot;&quot;SELECT author_id AS user_id, p.podcast_id, rating, p.title AS name, created_at FROM podcasts p INNER JOIN reviews r USING(podcast_id) &quot;&quot;&quot; ratings_raw = pd.read_sql(get_ratings, con, parse_dates=&#39;created_at&#39;) . ratings_raw . user_id podcast_id rating name created_at . 0 F7E5A318989779D | c61aa81c9b929a66f0c1db6cbe5d8548 | 5 | Backstage at Tilles Center | 2018-04-24 12:05:16-07:00 | . 1 F6BF5472689BD12 | c61aa81c9b929a66f0c1db6cbe5d8548 | 5 | Backstage at Tilles Center | 2018-05-09 18:14:32-07:00 | . 2 1AB95B8E6E1309E | ad4f2bf69c72b8db75978423c25f379e | 1 | TED Talks Daily | 2019-06-11 14:53:39-07:00 | . 3 11BB760AA5DEBD1 | ad4f2bf69c72b8db75978423c25f379e | 5 | TED Talks Daily | 2018-05-31 13:08:09-07:00 | . 4 D86032C8E57D15A | ad4f2bf69c72b8db75978423c25f379e | 5 | TED Talks Daily | 2019-06-19 13:56:05-07:00 | . ... ... | ... | ... | ... | ... | . 984400 4C3F6BE3495A23D | a23e18c73fd942fab41bcf6b6a1571da | 2 | Mile Higher Podcast | 2021-10-09 07:10:31-07:00 | . 984401 938F0A4490CE344 | ae44a724e94dcec1616d6e695e6198ba | 4 | Something Was Wrong | 2021-10-09 16:14:02-07:00 | . 984402 D6CBF74E9B2C7FE | ae44a724e94dcec1616d6e695e6198ba | 1 | Something Was Wrong | 2021-10-09 14:32:49-07:00 | . 984403 FED395AE84679C4 | ae44a724e94dcec1616d6e695e6198ba | 5 | Something Was Wrong | 2021-10-09 11:40:29-07:00 | . 984404 5058E0A92FF400B | ae44a724e94dcec1616d6e695e6198ba | 4 | Something Was Wrong | 2021-10-09 10:36:35-07:00 | . 984405 rows × 5 columns . Next we create a table of podcasts with some rating statistics: number of ratings, mean rating, and the years of the first and the last rating. . def extract_podcasts(ratings): &#39;Get the podcasts with rating count, rating mean and rating years.&#39; ratings_copy = ratings.copy() return (ratings_copy.groupby(&#39;podcast_id&#39;, as_index=False) .agg( name = (&#39;name&#39;, &#39;first&#39;), rating_count = (&#39;rating&#39;, &#39;count&#39;), rating_mean = (&#39;rating&#39;, &#39;mean&#39;), earliest_rating_year = (&#39;created_at&#39;, lambda c: c.min().year), latest_rating_year = (&#39;created_at&#39;, lambda c: c.max().year), ) ) . podcasts_raw = extract_podcasts(ratings_raw) podcasts_raw . podcast_id name rating_count rating_mean earliest_rating_year latest_rating_year . 0 a00018b54eb342567c94dacfb2a3e504 | Scaling Global | 1 | 5.000000 | 2017 | 2017 | . 1 a00043d34e734b09246d17dc5d56f63c | Cornerstone Baptist Church of Orlando | 1 | 5.000000 | 2019 | 2019 | . 2 a0004b1ef445af9dc84dad1e7821b1e3 | Mystery: Dancing in the Dark | 1 | 1.000000 | 2011 | 2011 | . 3 a00071f9aaae9ac725c3a586701abf4d | KTs Money Matters | 4 | 5.000000 | 2018 | 2018 | . 4 a000aa69852b276565c4f5eb9cdd999b | Speedway Soccer | 15 | 5.000000 | 2018 | 2020 | . ... ... | ... | ... | ... | ... | ... | . 46688 fffe3f208a56dfecfaf6d0a7f8399d63 | How Travel Writers Self-Publish | 4 | 5.000000 | 2019 | 2020 | . 46689 fffeb7d6d05f2b4c600fbebc828ca656 | TEDDY &amp; THE EMPRESS: Cooking the Queens | 43 | 4.837209 | 2017 | 2021 | . 46690 ffff5db4b5db2d860c49749e5de8a36d | Frankenstein, or the Modern Prometheus | 8 | 4.750000 | 2011 | 2021 | . 46691 ffff66f98c1adfc8d0d6c41bb8facfd0 | Who’s Bringing Wine? | 5 | 5.000000 | 2018 | 2018 | . 46692 ffff923482740bc21a0fe184865ec2e2 | TEFL Waffle | 2 | 5.000000 | 2018 | 2019 | . 46693 rows × 6 columns . Data Exploration and Cleaning . In this section we will deal with some issues in the data and prepare it for the recommender system below. . How far back do the reviews go? . A couple of ratings go all the way back to 2005 although most of them only go back to 2018. For many popular podcasts the reviews start in 2019. . When I asked the curator of the dataset on Kaggle why the reviews go much further back for some podcasts than for most others, he clarified that the reason is that the Apple API only gives access the latest 500 reviews of each podcast. This explains why for popular podcasts those 500 reviews only go back a couple of months, but for others they go back many years. . Inspecting the dates of the reviews of some popular podcasts, I found no gaps since 2019. This confirms that the reviews have been downloaded without interruption since then. . Curating the Ratings . We need to take care of the following complications: . Some users have left a suspiciously high number of reviews. Indeed, looking at the content of their reviews they do not look genuine at all: they repeat the same text hundreds of times, with slight variations. We will remove all the users with a rating volume beyond a specific threshold to weed out bots. We set the threshold at 135 reviews by inspecting the content of the reviews and making a judgment call. | It appears that some podcasts are no longer active, given that their latest review was made years ago. We need to decide whether we want to remove these seemingly inactive podcasts. While we don&#39;t want to recommend podcasts that don&#39;t exist anymore, their reviews can still help the collaborative filtering model. We will simply remove podcasts which have zero reviews made on or after 2020. Another option would be to include old podcasts in the training of the recommender system but skip them when making recommendations. | It turns out that there are repeat reviews in the data, meaning that some users left multiple reviews for the same podcast. They are probably just edited or updated reviews. Consequently, we will only consider the latest rating for each user-podcast pairing. | For the collaborative filtering approach to work, the users need to have rated multiple podcasts and, similarly, the podcasts need to have been rated by multiple users. To ensure this, we need to remove all users and podcasts with a number of reviews below a certain threshold. For example, we could remove all users with under 3 reviews and all podcasts with under 15 reviews. We have to be careful here: removing some users will reduce the number of reviews for some podcasts, which might push some podcasts below the threshold. In turn, removing those podcasts might push some users below the threshold. We need to keep doing this back and forth until the ratings DataFrame stops changing. | . We will write a separate function to deal with each point. . def remove_suspicious_users(ratings, max_reviews=135): &#39;Remove users with suspiciously high review count.&#39; mask = ratings.groupby(&#39;user_id&#39;)[&#39;podcast_id&#39;].transform(&#39;count&#39;) &lt;= max_reviews return ratings[mask] def remove_inactive_podcasts(ratings, latest_rating_year=2020): &#39;Remove podcasts with no reviews at or after latest_rating_year.&#39; active = (ratings.groupby(&#39;podcast_id&#39;)[&#39;created_at&#39;] .transform(lambda c: c.max().year) &gt;= latest_rating_year ) return ratings[active] def keep_only_latest_rating(ratings): &#39;Remove repeat reviews, keeping the latest. Also sorts the ratings by date.&#39; return ratings.sort_values(by=&#39;created_at&#39;, ascending=False).drop_duplicates(subset=[&#39;podcast_id&#39;, &#39;user_id&#39;]) def remove_low_rating_users_and_podcasts(ratings, min_user_reviews=3, min_podcast_reviews=15): &#39;Alternate between removing podcasts and users with insufficient reviews until there are none left.&#39; result = ratings.copy() while result.shape: previous_shape = result.shape mask = result.groupby(&#39;podcast_id&#39;)[&#39;user_id&#39;].transform(&#39;count&#39;) &gt;= min_podcast_reviews result = result[mask] mask = result.groupby(&#39;user_id&#39;)[&#39;podcast_id&#39;].transform(&#39;count&#39;) &gt;= min_user_reviews result = result[mask] if result.shape == previous_shape: return result . ratings = remove_suspicious_users(ratings_raw) ratings = remove_inactive_podcasts(ratings) ratings = keep_only_latest_rating(ratings) ratings = remove_low_rating_users_and_podcasts(ratings) . ratings . user_id podcast_id rating name created_at . 984386 2ED4C4FD5F1740E | a9bdaba5449189a4587793e36ce4f704 | 5 | Going West: True Crime | 2021-10-09 16:24:40-07:00 | . 984402 D6CBF74E9B2C7FE | ae44a724e94dcec1616d6e695e6198ba | 1 | Something Was Wrong | 2021-10-09 14:32:49-07:00 | . 984376 C0E30B0BB0AA10C | ddd451a18055f0108edf79f8c3c9bf15 | 5 | What If World - Stories for Kids | 2021-10-09 07:36:51-07:00 | . 984301 E220A79CBD5C5AD | b9a8d90ae43232769ecc68d7defb0c38 | 5 | Scared To Death | 2021-10-09 03:51:32-07:00 | . 984316 36B07D3E02D2516 | c3f080cc393035a81d4ac7c7bff1c6c1 | 5 | Smash Boom Best | 2021-10-09 03:45:35-07:00 | . ... ... | ... | ... | ... | ... | . 138992 4A06D17875D08A3 | fb3a6b9f12f4d0b050887e91684e68c0 | 5 | EconTalk | 2007-04-21 20:49:49-07:00 | . 50962 4A06D17875D08A3 | d2f31ced463510866c6adfbae95d3be2 | 2 | Money Girl&#39;s Quick and Dirty Tips for a Richer... | 2007-03-31 21:09:12-07:00 | . 479293 245BC52E7C82EA7 | b913c95d304bac8b6cddf9034294eab7 | 5 | PotterCast: The Harry Potter podcast since 2005 | 2006-10-18 14:14:46-07:00 | . 479291 39D9B7796AA0A6C | b913c95d304bac8b6cddf9034294eab7 | 5 | PotterCast: The Harry Potter podcast since 2005 | 2006-05-20 11:18:27-07:00 | . 202910 BF99560F0BB8F18 | cae7b4d90b57e8bae77907092d5e813e | 4 | All Songs Considered | 2005-12-15 04:23:27-07:00 | . 51015 rows × 5 columns . podcasts = extract_podcasts(ratings) . podcasts.sort_values(by=&#39;rating_count&#39;, ascending=False) . podcast_id name rating_count rating_mean earliest_rating_year latest_rating_year . 245 bc5ddad3898e0973eb541577d1df8004 | My Favorite Murder with Karen Kilgariff and Ge... | 688 | 3.228198 | 2019 | 2021 | . 228 bad6c91efdbee814db985c7a65199604 | Wow in the World | 649 | 4.648690 | 2019 | 2021 | . 807 f2377a9b0d9a2e0fb05c3dad55759328 | Story Pirates | 589 | 4.590832 | 2016 | 2021 | . 334 c3f080cc393035a81d4ac7c7bff1c6c1 | Smash Boom Best | 470 | 4.740426 | 2018 | 2021 | . 142 b1a3eb2aa8e82ecbe9c91ed9a963c362 | True Crime Obsessed | 434 | 3.679724 | 2019 | 2021 | . ... ... | ... | ... | ... | ... | ... | . 821 f3c3640687112903a1fa5869665682ae | American Ball Tales | 15 | 4.933333 | 2019 | 2020 | . 301 c0f3a638b96dd2482255211827f1a95a | Dreams In Drive | 15 | 5.000000 | 2016 | 2021 | . 306 c149f512e2e103090a77b32d42511479 | Cantina Cast | 15 | 4.400000 | 2013 | 2019 | . 202 b76c794eb4bb5b73748bd8e5dc8052b8 | Congeria | 15 | 4.200000 | 2018 | 2020 | . 800 f17f50670f7ba891f1b147687649da29 | OFFSHORE | 15 | 4.733333 | 2016 | 2018 | . 936 rows × 6 columns . Out of the 46,693 podcasts we started with, we are left with 936. Unfortunately, it is inevitable that we have to discard a large fraction of the podcasts because most of them have only a few reviews on Apple Podcasts. Consider the fact that more than a fourth of the podcasts (13,922 to be precise) had only a single review. More that half of the podcasts (a total of 25,104) had only up to 3 reviews! . That said, it&#39;s worth noting that there are actually as many as 8323 podcasts with at least 15 ratings. However, a sizable portion of the users leaving those ratings had to be removed because they only rated one or two podcasts in total (and of course removing some podcasts led to having to remove more users and so on). Thus, this is how we are left with just 936 podcasts. . The remaining ratings are still sufficient to yield interesting results, though! . The minimum threshold of ratings for users and podcasts is also reflected in the density of the ratings matrix. The so called ratings matrix contains all the ratings such that each row corresponds to one user and each column corresponds to one podcast. If there is a particular user hasn&#39;t rated a particular podcast, the corresponding entry (where the user row and podcast column meet) is simply $0$. Furthermore, the density of the ratings matrix is the percentage of non-zero entries. In other words, the density is the percentage of user-podcast pairs for which a rating exists in the dataset. . def compute_density(ratings): n_ratings = ratings.shape[0] n_podcasts = ratings[&#39;podcast_id&#39;].nunique() n_users = ratings[&#39;user_id&#39;].nunique() return n_ratings / (n_podcasts * n_users) . print(f&#39;The density of the curated rating matrix is {compute_density(ratings) * 100:.2f}%, while the density of the original rating matrix is {compute_density(ratings_raw) * 100:.4f}%.&#39;) . The density of the curated rating matrix is 0.45%, while the density of the original rating matrix is 0.0028%. . We went from 755,438 users to 12,212 users after cleaning up the data and discarding users and podcasts with too few reviews. . Unfortunately, the vast majority of users left only a single review (in this dataset at least). This is probably at least partly due to the fact that many popular podcasts are missing and even for those included the reviews go back only three years. However, even taking this into account, it is conceivable that most people listen to fewer podcasts than Netflix users watch different shows and movies, for example. There is also more friction (more time and steps involved) for leaving a review on Apple Podcasts than rating a show on Netflix, again as an example. . Implicit Recommender System . It turns out that the overwhelming majority of the ratings are 5 star ratings. It appears that most users do not go out of their way to give a negative rating unless they really dislike a show The following bar chart shows the frequency of each star rating in the curated ratings table. The situation is even more skewed in favor of 5 star ratings in the raw ratings data. . fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 4)) ax0.set_title(&#39;Star Ratings Before Curating&#39;) ax0.bar(ratings_raw[&#39;rating&#39;].value_counts().index, ratings_raw[&#39;rating&#39;].value_counts().values) ax1.set_title(&#39;Star Ratings After Curating &#39;) ax1.bar(ratings[&#39;rating&#39;].value_counts().index, ratings[&#39;rating&#39;].value_counts().values); . Why Implicit? . When I started this project I intended to use a model which tries to predict the specific star rating a user would give to &quot;unseen&quot; items, in order to recommend the item with the highest predicted rating. This is how explicit recommender systems work, which are trained on explicit user feedback (in this case, star ratings). However, the extreme imbalance in the ratings suggests that the explicit recommender system approach might not be appropriate here. . First of all, there is a well-known issue with imbalanced data which can be illustrated as follows. The simple baseline model which predicts that every user will rate every podcast with 5 stars would have a high accuracy, because it would be right almost all of the time. That said, this is not a big deal and can be corrected by choosing a more appropriate metric than plain accuracy. . The deeper concern in this case is the reason behind the class imbalance. It appears that that most users simply stop listening to a podcast they don&#39;t like without bothering to leave a negative review. Not only that, but people clearly don&#39;t just pick podcasts at random to listen to. Instead, there is a pre-selection: they follow a friend&#39;s recommendation, seek out podcasts on a particular topic or featuring a particular guest, and so on. Needless to say, users are unlikely to leave a review for a podcast they never listened to (although I am sure that a handful of people do). . All of this is to say: . In explicit recommender systems missing ratings are viewed simply as missing information. However, it appears that there actually is some information given by the fact that a podcast wasn&#39;t rated by a user. Maybe we should think of missing ratings as suggesting a negative preference, but assigning low confidence to that preference. Some reasons why a missing rating potentially reveals a negative preference were given above. Namely, users are less likely to rate a podcast they don&#39;t like and many are even unlikely to listen to it in the first place. On the other hand, the confidence in this negative preference is low because the rating might be missing for a variety of other reasons. The most likely reason is that the user isn&#39;t even aware of that particular podcast&#39;s existence. | Focusing mostly on the precise ratings (1 to 5 stars) is of limited value because users seem to be using the stars mostly to give a &quot;thumbs up&quot; (5 stars). | . It turns out that there is an approach which seems to be perfectly suited to address the two issues above: implicit recommender systems. They are called implicit because they usually do not use (explicit) feedback given by the users. Instead they infer the preferences of users from their activity, such as how often a user has listened to a song on Spotify, or if a user has watched the entirety of a movie on Netflix. The fundamental change from explicit to implicit systems is that instead of giving ratings, users have preferences and those preferences are known to us with a certain confidence. What this allows us to do is to interpret the absence of activity (user didn&#39;t watch a particular movie) as a negative preference, but with low confidence. . Unfortunately, we don&#39;t have access to user activity, but the ratings (which are explicit feedback) can be made &quot;implicit&quot; with the following interpretation: high ratings (4 or 5 stars) correspond to positive preferences with high confidence, while missing ratings and all lower ratings (1 to 3 stars) correspond to negative preferences with low confidence. It is possible to treat low ratings separately from missing ratings but this doesn&#39;t seem to improve the results, maybe due to the low frequency of low ratings. . Alternating Least Squares . We will use the implicit, a very fast recommender library written in Cython by Ben Frederickson. Specifically, we will use the Alternating Least Squares algorithm, or ALS. The ALS algorithm for implicit recommenders was introduced in this paper by Hu, Koren and Volinsky. I will not go into too much detail here, but a general explanation is outlined below. In addition to the original paper, I recommend reading this blog post, in which the algorithm is implemented in Python (although the implicit library is actually used for speed). . Here is a brief overview of the model we will use: Each user $u$ is assumed to have a preference $p_{ui}$ for podcast $i$ and we want to find latent factors $x_u$ and $y_i$ such that their inner product approximates the preference: $p_{ui} approx x_u cdot y_i$. More precisely, we want to find $x_u$ and $y_i$ which minimize the following cost function: $$ sum_{u,i} c_{ui}(p_{ui} - x_u cdot y_i)^2 + lambda left( sum_u |x_u |^2 + sum_i |y_i |^2 right) $$ . The weights $c_{ui}$ are the confidence that we have in the respective preference $p_{ui}$. The higher the confidence, the more importance we give to approximating the particular preference $p_{ui}$ by $x_u cdot y_i$. The summands multiplied by $ lambda$ are there to avoid overfitting. . If we hold constant the user vectors $x_u$, the cost function is quadratic in the podcast vectors $y_i$ and can be minimized efficiently. The same is true swapping $x_u$ and $y_i$. This where the Alternating Least Squares trick comes in: First compute the $y_i$ which minimize the cost function with $x_u$ held constant. Then fix $y_i$ at that (provisional) minimum and in turn find $x_u$ minimizing the resulting cost function. Amazingly, simply doing this back and forth several times yields pretty good results. . The Implicit Matrix . In order to feed our data to the implicit ALS model, we need to transform our table of explicit ratings into a matrix of implicit data. The entries of the matrix need to incorporate both the confidence factors $c_{ui}$ and the preference factors $p_{ui}$. . In order to construct the matrix correctly, we need to know which input the model implicit.AlternatingLeastSquares expects. We feed the ALS model a single matrix, which then (internally) deduces preferences and confidence from that single matrix. If there is a positive entry at a position $(u,i)$, this is taken to mean that $p_{ui} = 1$ (positive preference), otherwise $p_{ui} = 0$ (negative entries). The precise values of the entries are also important: The element at position $(u,i)$ equals the confidence $c_{ui}$, after adding 1 to make sure that the confidence is at least 1 for all $(u,i)$ (if the confidence at some point were 0 the preference $p_{ui}$ would be irrelevant in the cost function, which we want to avoid in the implicit setting). . In light of the above, it&#39;s clear that our implicit matrix needs strictly positive entries for each pair $(u,i)$ for which the user $u$ gave the podcast $i$ a high ratings, and all other entries should be set to 0. Marking low ratings (1 or 2 stars, say) with negative entries in the matrix did not help much when I tried it, so we will avoid this. (That would mean a higher confidence in the negative preference for low ratings, as opposed to missing ratings.) . Here is what we will do: The implicit matrix will have a 1 at every position corresponding to a high rating (4 or 5 stars) and a 0 everywhere else. There is nothing special about the value 1, which can be changed later to any other number (by simply multiplying the matrix by that number). Note that most entries are 0, given that most users have not left reviews for most podcasts. In other words, the matrix will have a high sparsity (low density). This is why it makes sense to use a scipy sparse matrix instead of a NumPy array. . def make_implicit(ratings, threshold=4): &#39;&#39;&#39;Replace star rating (1 to 5) by a +1 if rating &gt;= threshold and if rating &lt; threshold either replace it by a -1 (if negative is True) or remove it (if negative is False). Return a csr sparse matrix with the ratings (users rows and podcasts cols) and two lists: one with the user_ids corresponding to the rows and one with the podcast names corresponding to the columns. &#39;&#39;&#39; positive = ratings[&#39;rating&#39;] &gt;= threshold implicit_ratings = ratings.loc[positive].copy() implicit_ratings[&#39;rating&#39;] = 1 # Remove low rating users and podcasts again implicit_ratings = remove_low_rating_users_and_podcasts(implicit_ratings, 2, 5) user_idx = implicit_ratings[&#39;user_id&#39;].astype(&#39;category&#39;).cat.codes podcast_idx = implicit_ratings[&#39;podcast_id&#39;].astype(&#39;category&#39;).cat.codes # The codes simply number the user_id and podcast_id in alphabetical order # We keep track of the order of the users and podcasts with the following arrays user_ids = implicit_ratings[&#39;user_id&#39;].sort_values().unique() podcast_names = implicit_ratings.sort_values(by=&#39;podcast_id&#39;)[&#39;name&#39;].unique() implicit_ratings = sparse.csr_matrix((implicit_ratings[&#39;rating&#39;], (user_idx, podcast_idx))) return implicit_ratings, user_ids, podcast_names . implicit_ratings, user_ids, podcast_names = make_implicit(ratings) implicit_ratings.shape . (10607, 933) . Training and Evaluation . At last, we are ready to train our recommender! . To evaluate the performance of a recommender we need to be able to decide if recommendations are relevant. However, if the system simply recommends podcasts that it already &quot;knows&quot; the user likes (positive entry in the implicit matrix), this doesn&#39;t reflect how well the system can make recommendations for podcasts the user hasn&#39;t shown a preference for yet (0 in the implicit matrix). . To address this, we will turn one positive entry into a 0 entry for each user. In other words, for each user we will forget one podcast the user rated highly. Then we train the recommender system on this modified implicit dataset (called the training set). Next, we let the model make one recommendation per user, but require that for each user the podcast recommended has not already been &quot;liked&quot; by that user in the training set. Finally, we compute the precision of the recommender: the fraction of the users for which the recommendation is precisely the podcast we &quot;forgot&quot; for that user when creating the training set. Recall that we know those &quot;forgotten&quot; podcasts to be relevant recommendations, because the user gave them a high rating (which we omitted in the training set). . Note that recommendations other than the one positive preference we omitted (for each user) might also be relevant, but there is no way for us to verify that with our data. In light of this, the precision might in fact underestimate how often the recommendations are relevant. . The (simple) precision is not the best metric. For example, it would be better to omit several ratings for each user and then compute the precision at k (denoted p@k), which consists of recommending $k$ podcasts for each user and determining which fraction of those recommendations is relevant. What we are doing above is effectively p@1 (precision at 1). There are other more sophisticated metrics, but they also require making multiple recommendations per user. The reason we cannot use these metrics is that most users only have 3 ratings and removing more than one would leave them with 1 rating, which is basically useless for collaborative filtering. If we instead only removed ratings from a subset of users who left many ratings, we would be biasing our metric in favor of a minority of very active users. . ratings_train, ratings_test = leave_k_out_split(implicit_ratings, K=1) . import os os.environ[&quot;MKL_NUM_THREADS&quot;] = &quot;1&quot; als_recommender = AlternatingLeastSquares(factors=50, regularization=0.1, random_state=42) als_recommender.fit(2 * ratings_train.T) . precision_at_k(als_recommender, ratings_train, ratings_test, K=1) . 0.09263038548752835 . As a baseline, we will also compute the precision for a simple recommender which recommends the most popular podcast to all users. To be precise, it recommends the most popular podcast among those not already liked by the user in the training set, because those recommendations are not scored as hits when computing the precision (we want the recommender to suggest &quot;new&quot; podcasts after all). . We write the baseline in such a way that it can also recommend multiple podcasts. It simply recommends the $N$ most popular podcasts, given some $N$. . class PopularityBaseline(): def __init__(self, implicit_ratings): podcast_ids, count = np.unique(implicit_ratings.tocoo().col, return_counts=True) self.top_podcasts = podcast_ids[np.argsort(-count)] def recommend(self, user_id, user_items, N=10): &#39;&#39;&#39;Recommend the most popular podcasts, but exclude podcasts which the users in user_ids have already interacted with according to user_items&#39;&#39;&#39; user_items = user_items.tocoo() this_user = user_items.row == user_id liked_podcasts = set(user_items.col[this_user]) recom = [] for podcast in self.top_podcasts: if podcast not in liked_podcasts: recom.append(podcast) if len(recom) == N: break else: raise Exception(&#39;Not enough podcasts remaining to recommend&#39;) return list(zip(recom, [0] * N)) # The implicit API expects a score for each podcast popularity_baseline = PopularityBaseline(implicit_ratings) . precision_at_k(popularity_baseline, ratings_train, ratings_test, K=1) . 0.02913832199546485 . Our recommender system is significantly better than the baseline recommender ($9.3 %$ versus $2.9 %$). It appears the recommender learned something! . Now we will train the recommender again but with the whole implicit rating set, not just the a smaller training set. We will use this recommender going forward. . als_recommender = AlternatingLeastSquares(factors=50, regularization=0.1, random_state=42) als_recommender.fit(2 * implicit_ratings.T) . Latent Factors . Recall that the our recommender works by finding latent factors for all podcasts and all users, such that the inner product of the user and podcast vectors is as close as possible to the corresponding user preferences. Another way of looking at this is that preference (of a user for a podcast) or similarity (of two podcasts, or two users, to each other) corresponds to vectors pointing in a similar direction (technically, having a high cosine similarity, or low cosine distance). . In light of the above, to introspect the recommender we must visualize the latent factors. We will do this for the most popular podcasts in the dataset. Because the latent space is 50-dimensional we will project it down to 2 dimensions. We will use Principal Component Analysis (PCA) to find the two directions in which the latent factors vary the most and project down to those. . podcast_ids, count = np.unique(implicit_ratings.tocoo().col, return_counts=True) top_podcasts = podcast_ids[np.argsort(-count)][:25] . pca = PCA(n_components=5) reduced_item_factors = pca.fit_transform(als_recommender.item_factors) . fig, ax = plt.subplots(figsize=(15, 15)) X = reduced_item_factors[top_podcasts].T[1] Y = reduced_item_factors[top_podcasts].T[0] ax.set_title(&#39;Latent Podcast Space&#39;, fontdict = {&#39;fontsize&#39; : 20}) ax.scatter(X, Y) for i, x, y in zip(podcast_names[top_podcasts], X, Y): ax.text(x, y, i, color=np.random.rand(3)*0.7, fontsize=14) . We must take the visualization with a grain of salt because obviously information is lost when we project a 50-dimensional space down to two dimensions. Specifically, podcasts that appear close in the projection might not be close at all in the full space. . That said, there appears to be some clear structure, which we will describe below. We must also remember that this is not some random 2D projection, but a projection to the two axes of highest variability (principal components). . Let&#39;s start with the horizontal direction (or x axis). Podcasts targeted at children are on the right and podcasts targeted at more mature audiences are to the left. The most extreme values are attained by &#39;Wow to the World&#39; and &#39;Story Pirates&#39;, which are the most popular podcasts for kids. Judging from the content of the reviews there seems to be a bit of a rivalry between those two podcasts, although they have a large overlap in preference. &#39;Smash Boom Best&#39; and &#39;Pants on Fire&#39; are for children as well. It is interesting that the two podcasts on stories for kids are so close to each other. . In the vertical direction (or y axis), the situation is not as clear-cut but we can recognize different genres bunch together. The podcasts at the top all focus on self-improvement or self-help. The tiles &#39;The Learning Leader Show&#39;, &#39;Discover Your Talent&#39;, and &#39;Mindulness Mode&#39; are self-explanatory. &#39;Confessions of a Terrible Husband&#39; is about relationship advice. As for &#39;Leveling Up&#39;, this is a (partial) quote from the official website: &quot;Leveling Up is a radical new perspective on achieving success [...]&quot;. On the other hand the podcasts at the bottom are all for pure entertainment (true crime themed and slightly above, pop culture and comedy). . Podcast Similarity . As a reality check, we will go through a couple of popular podcasts and inspect the 10 most similar podcasts according to our model. I find the results pretty impressive considering the limited information the model was trained on. Click on &quot;show output&quot; to view the list of similar podcasts. . def get_k_most_similar_podcasts(name, recommender, podcast_names, K=10): this_name = np.where(podcast_names == name)[0][0] return [podcast_names[idx] for idx, _ in recommender.similar_items(this_name, N=K+1)[1:]] . get_k_most_similar_podcasts(&#39;My Favorite Murder with Karen Kilgariff and Georgia Hardstark&#39;, als_recommender, podcast_names, 10) . [&#39;Scary Stories To Tell On The Pod&#39;, &#39;Puck Soup&#39;, &#39;Who Killed Theresa?&#39;, &#39;Murder Minute&#39;, &#39;Mother, May I Sleep With Podcast?&#39;, &#39;Manifestation Babe&#39;, &#39;Encyclopedia Womannica&#39;, &#39;Highest Self Podcast&#39;, &#39;The TryPod&#39;, &#39;Story Story Podcast: Stories and fairy tales for families, parents, kids and beautiful nerds.&#39;] . . get_k_most_similar_podcasts(&#39;The Joe Rogan Experience&#39;, als_recommender, podcast_names, 10) . [&#39;Bret Weinstein | DarkHorse Podcast&#39;, &#39;Artificial Intelligence (AI)&#39;, &#39;Cleared Hot&#39;, &#39;Ben Greenfield Fitness&#39;, &#39;The Bill Bert Podcast&#39;, &#39;Walk-Ins Welcome w/ Bridget Phetasy&#39;, &#34;Congratulations with Chris D&#39;Elia&#34;, &#39;Jocko Podcast&#39;, &#39;expediTIously with Tip &#34;T.I.&#34; Harris&#39;, &#39;Team Never Quit&#39;] . . get_k_most_similar_podcasts(&#39;Story Pirates&#39;, als_recommender, podcast_names, 10) . [&#39;By Kids, For Kids Story Time&#39;, &#39;Katie, The Ordinary Witch&#39;, &#39;Listen Out Loud with The Loud House&#39;, &#39;Minecraft Me - SD Video&#39;, &#39;Story Story Podcast: Stories and fairy tales for families, parents, kids and beautiful nerds.&#39;, &#39;Scary Story Podcast&#39;, &#39;Wow in the World&#39;, &#39;Highlights Hangout&#39;, &#39;The Casagrandes Familia Sounds&#39;, &#39;KiDNuZ&#39;] . . get_k_most_similar_podcasts(&#39;Best Real Estate Investing Advice Ever&#39;, als_recommender, podcast_names, 10) . [&#39;Real Estate Investing For Cash Flow Hosted by Kevin Bupp.&#39;, &#39;Jake and Gino: Multifamily Real Estate Investing &amp; More&#39;, &#39;Target Market Insights: Multifamily + Marketing&#39;, &#39;Before the Millions | Lifestyle Design Through Real Estate | Passive Cashflow Investing Tips and Str...&#39;, &#39;Lifetime Cash Flow Through Real Estate Investing&#39;, &#39;Dwellynn Show - Financial Freedom through Real Estate Investing&#39;, &#39;Investing In The U.S.&#39;, &#39;Accelerated Investor Podcast&#39;, &#39;Support is Sexy Podcast with Elayne Fluker | Interviews with Successful Women Entrepreneurs 5 Days a...&#39;, &#39;Simple Passive Cashflow&#39;] . . get_k_most_similar_podcasts(&#39;Mindfulness Mode&#39;, als_recommender, podcast_names, 10) . [&#39;Pregnancy Podcast&#39;, &#39;The Bitcoin Knowledge Podcast&#39;, &#39;Undone Redone&#39;, &#39;Recording Studio Rockstars&#39;, &#39;Take Up Code&#39;, &#39;Wedding Planning Podcast&#39;, &#39;Podcast Junkies&#39;, &#39;Tandem Nomads - From expat partners to global entrepreneurs! Build a successful business and thrive...&#39;, &#39;Fearless And Healthy Podcast|Holistic Health|Success Habits|Lifestyle&#39;, &#39;Play Your Position with Mary Lou Kayser&#39;] . . get_k_most_similar_podcasts(&#39;ADHD reWired&#39;, als_recommender, podcast_names, 10) . [&#39;People Behind the Science Podcast - Stories from Scientists about Science, Life, Research, and Scien...&#39;, &#39;Brilliant Business Moms with Beth Anne Schwamberger&#39;, &#39;The Inside Winemaking Podcast with Jim Duane&#39;, &#39;Top Traders Unplugged&#39;, &#39;Big Wig Nation with Darrin Bentley&#39;, &#39;Own It! For Entrepreneurs. Talking Digital Marketing, Small Business, Being Digital Nomads and Succ...&#39;, &#39;Maura Sweeney: Living Happy Inside Out | Encouragement | Inspiration | Empowerment | Leadership&#39;, &#39;Commercial Real Estate Elite: Broker to Brokers&#39;, &#39;Whistle and a Clipboard- the coaching communities resource&#39;, &#39;Podcast Junkies&#39;] . . get_k_most_similar_podcasts(&#39;Good Night Stories for Rebel Girls&#39;, als_recommender, podcast_names, 10) . [&#39;Fierce Girls&#39;, &#39;Short &amp; Curly&#39;, &#39;Big Life Kids Podcast&#39;, &#39;Dream Big Podcast&#39;, &#39;Book Club for Kids&#39;, &#39;Katie, The Ordinary Witch&#39;, &#39;Saturday Morning Theatre&#39;, &#39;KiDNuZ&#39;, &#39;Storynory - Stories for Kids&#39;, &#39;The Calm Kids Podcast&#39;] . . get_k_most_similar_podcasts(&#39;Leveling Up with Eric Siu&#39;, als_recommender, podcast_names, 10) . [&#39;Leaders Inspire Leaders | Koy McDermott - Millennial Leadership Consultant | Personal &amp; Professional...&#39;, &#34;Where There&#39;s Smoke&#34;, &#39;Tandem Nomads - From expat partners to global entrepreneurs! Build a successful business and thrive...&#39;, &#39;On Air with Ella&#39;, &#39;Everyday MBA&#39;, &#39;MLM Nation&#39;, &#34;Worldbuilder&#39;s Anvil&#34;, &#39;The Bitcoin Knowledge Podcast&#39;, &#39;Fearless And Healthy Podcast|Holistic Health|Success Habits|Lifestyle&#39;, &#39;Play Your Position with Mary Lou Kayser&#39;] . . get_k_most_similar_podcasts(&#39;Pants on Fire&#39;, als_recommender, podcast_names, 10) . [&#39;The Mayan Crystal&#39;, &#39;The Cramazingly Incredifun Sugarcrash Kids Podcast&#39;, &#39;Minecraft Me - SD Video&#39;, &#39;Saturday Morning Theatre&#39;, &#39;Short &amp; Curly&#39;, &#39;Smash Boom Best&#39;, &#39;Book Club for Kids&#39;, &#39;The Casagrandes Familia Sounds&#39;, &#39;The Past and The Curious: A History Podcast for Kids and Families&#39;, &#39;Listen Out Loud with The Loud House&#39;] . . get_k_most_similar_podcasts(&#39;Bachelor Happy Hour with Rachel &amp; Ali – The Official Bachelor Podcast&#39;, als_recommender, podcast_names, 10)#collapse-output . [&#39;Logically Irrational&#39;, &#39;Mommies Tell All&#39;, &#39;The Ben and Ashley I Almost Famous Podcast&#39;, &#39;Hot Marriage. Cool Parents.&#39;, &#39;Miraculous Mamas&#39;, &#39;Off The Vine with Kaitlyn Bristowe&#39;, &#39;Scrubbing In with Becca Tilley &amp; Tanya Rad&#39;, &#39;Ringer Dish&#39;, &#39;Know Your Aura with Mystic Michaela&#39;, &#39;Another Bachelor Podcast&#39;] . . get_k_most_similar_podcasts(&quot;And That&#39;s Why We Drink&quot;, als_recommender, podcast_names, 10) . [&#39;Two Girls One Ghost&#39;, &#34;Dude, That&#39;s F****d Up&#34;, &#39;Wine &amp; Crime&#39;, &#39;Ghost Town&#39;, &#39;I Said God Damn! A True Crime Podcast&#39;, &#39;Beyond the Secret&#39;, &#39;Cult Podcast&#39;, &#39;Potterless&#39;, &#39;Death in the Afternoon&#39;, &#39;The Alarmist&#39;] . . Discussion . Despite the fact that the dataset was drastically reduced after curation (removing podcasts with insufficient reviews and so on), the recommender still has 933 podcasts and about 10,607 users to work with, with a total of 40,585 positive ratings. The density is around $0.4 %$, meaning that around $0.4 %$ of all possible ratings (in other words, of all user-podcast pairs) are are actually realized in the data. . While this is a relatively small dataset for collaborative filtering, our recommender did pretty well: . On our test set, the accuracy was $0.09$ which is three times as high as the baseline recommender (which simply recommends the most popular podcasts). Recall that we computed this number by training the recommender while omitting 1 rating per user and then checking how often the omitted podcast was the first recommendation for each user. Getting precisely the omitted podcast as the first recommendation for $9 %$ of users seems pretty good, considering that there are probably many relevant podcasts that the users just haven&#39;t rated yet (we consider those irrelevant by default because we cannot verify their relevance). | When we looked at recommendations of individual podcasts they were very compelling. | Finally, as we described above, there are clear patterns in the latent factors of the podcasts which can be visualized with PCA. We can summarize those findings as follows: The first principal component seems to correspond to a spectrum going from self-improvement to pure entertainment (with true crime at the very end). Along the second principal component the podcasts separate according to whether they are targeted at kids or adults. | . Closing Thoughts . It seems that it was a good choice to turn the star ratings into an implicit dataset, with preferences and confidences. Remember that we did this because the vast majority of ratings give 5 stars, which suggests that a lot of information lies in the podcasts a user did not rate. That information is lost in the explicit paradigm because missing ratings are ignored, unlike in the implicit paradigm, where they are taken into account as low confidence negative preferences. . I noticed that many popular podcasts are missing (based on this list of top 100 podcasts as of early 2022). When I brought this up with the curator of the dataset on Kaggle he confirmed that many podcasts are left out on purpose. However, he admitted that he hadn&#39;t realized how many popular podcasts were missing. This is unfortunate because if we do not know exactly how podcasts have been selected, we cannot correct for sampling bias. . On a related note: Joe Rogan&#39;s immensely popular and controversial podcast is not on Apple Podcasts since 2020, when it became a Spotify exclusive in a deal involving reportedly two hundred million dollars. Nonetheless, it appears many users were still able to leave reviews after the move, and some even wonder in their review why they aren&#39;t able to access the podcast anymore (and sometimes leave a negative rating in protest). This doesn&#39;t seem to have thrown off the recommender, judging by the list of podcasts most similar to &#39;The Joe Rogan Experience&#39;, which seems very appropriate. . The next steps would be to put together a larger dataset in which most popular podcasts are actually included. Then we would tune the hyperparameters of our model and evaluate the model with the best parameters using cross-validation. Note that a larger dataset is needed to properly carry out the parameter search and final evaluation. The parameter search requires cross-validation to evaluate the models with different parameters and this needs to be nested within a larger cross-validation to evaluate the performance of the best parameters found in the search. The nested cross-validation in this context requires removing one rating per user for the outer split and an additional rating per user for the inner split. In our data a majority of users only have 3 ratings, which would leave them with only a single rating in the training set (useless for collaborative filtering). If we wanted to use a better metric such as p@3, a total of 6 ratings per user would be omitted, needing even more ratings per user. .",
            "url": "david-recio.com/2022/03/19/podcasts-recommender.html",
            "relUrl": "/2022/03/19/podcasts-recommender.html",
            "date": " • Mar 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "What is the effect of flipped classroom groups on grades?",
            "content": "The Covid pandemic arrived in the US while I was a visiting professor at Lehigh University. Like many, we had to quickly adapt to a rapidly evolving situation and transition from a physical classroom one week to a fully virtual format the next. To capitalize on the situation, I implemented a flipped classroom with prerecorded lectures. This entailed the students working together on exercises over Zoom divided into breakout rooms. The breakout room groups were created at random for fairness and retained throughout the whole semester. The main rationale for not changing the groups is that the students needed time to get to know each other and figure out a team-based work flow (students in each group had to prepare shared answers on OneNote). However, the downside was that some groups worked together better than others. . This brings me to the central question motivating this notebook: Does the group each student was (randomly) assigned to make a noticeable difference to that student&#39;s final grade? More precisely, can we detect a statistically significant difference between the grade distributions of the groups? . This is a classic hypothesis testing question: Assuming the groups make no difference at all to the student grades (this assumption is called the null hypothesis), how extreme is the observed data (according to some specified measure called the statistic)? If the data is sufficiently extreme, it might be better explained by an alternative hypothesis, in this case that there actually is some difference in the grade distributions between the groups. . A more general null hypothesis would be that the group means are equal, even if the distributions might differ in other ways. In terms of the grades, testing this hypothesis corresponds to the question: Do the groups have an effect on the expected grade of a student (i.e. the group mean that student is a part of)? . We will use three different models belonging to three different paradigms: nonparametric, semiparametric, and parametric: . Permutation test: This test makes no assumptions on the shape of the probability distributions underlying the data in each group. However, the null hypothesis in this case is very broad (or weak): there is no difference at all between the groups, i.e. all grades come from the same distribution (which is unknown and we make no assumptions on). For instance, the null hypothesis does not include the case in which all groups have normal distributions with the same mean but different variances. That said, the statistic we are using is mostly sensitive to differences in means. | Semiparametric bootstrap: In this case we do make a few assumptions on the grade distributions in each group, but they are quite minimal. To put it simply, we assume that all the group distributions have the same &quot;shape&quot; but allow them to have different means and variances. Crucially, we do not make any assumptions on the &quot;distribution shape&quot;, which is instead approximated by bootstrap resampling. The null hypothesis in this case is more narrow: there is no difference in the means of the groups, i.e. the underlying grade distributions have the same mean but could have different variances. | F-test (One-way ANOVA): This is probably the most commonly used hypothesis test for comparing group means. It requires the strongest assumptions, but it comes with a narrow null hypothesis (the same as the bootstrap) and has higher power than the boostrap method (we will see later that the bootstrap method has low power due to the small group sizes). The assumptions will be explained further bellow, but basically the data is assumed to be normal with equal variances. We will also use the Welch F-test, which corrects for some deviations from these assumptions. | . Outline . This exposition is organized into the following parts: . Preamble: We load the grades and save in a dictionary of named tuples, each containing the final grade and group membership of one section. (I taught three sections of the same course.) | Implementation of the Tests: We motivate the three tests mentioned above and write functions which compute the p-values for each test. We implement the first two from scratch and write a wrapper for the F-test provided by statsmodels. We implement the permutations and bootstrap in a vectorized manner which makes them much faster than the library implementations we found, which are implemented using for loops. | Data Exploration: We visualize the data and test it for normality and heteroscedasticity. The data does not appear to be normal and it has a left skew consistent with an article cited below. We also have a look at the empirical distribution of the residuals used in the semiparametric bootstrap. That distribution does not appear to be a good approximation for the actual underlying grades distribution, which might explain the low power of the semiparametric bootstrap. | Compute p-values: We compute the p-values for our data. | Discussion: Discuss the results, considering issues like multiple comparisons and the power of the tests. | Takeaways | Simulations (APPENDIX): We approximate the power and size of the hypothesis tests with synthetic data. Specifically, we consider several realistic distributions underlying the grades and study the distribution of the p-values in each scenario. | . Preamble . from collections import namedtuple, defaultdict import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats as st from statsmodels.stats.diagnostic import kstest_normal from statsmodels.graphics.gofplots import qqplot from statsmodels.stats.oneway import anova_oneway as statsmodels_ftest from tqdm.notebook import tqdm %matplotlib inline . . First we load the grades. There are three sections, all of the same course. Each section has about 10 groups of around 4 students each. . Some groups have 5 students to limit the number of groups per section. There are several groups which had only 3 students by the end of the semester for two reasons. Firstly, some students dropped the course at some point during the semester. Also, after the semester had already started, I created a new group for students who had to remain in Asia because of travel restrictions. This group met at a different time due to their timezone. That group was excluded from this analysis so as not to interfere with the randomization principle. . In a separate notebook we already computed the final grades for all three sections, after extracting the homework and exam grades from a CSV exported from GradeScope (the platform we used for online submission and grading). In that same notebook we changed the group numbers and the section numbers for anonymity, before saving the final grades (together with the renamed sections and groups) in &#39;anonymized_grades.csv&#39;. . Here we load that CSV file and save the grades in a dictionary, where each item corresponds to a section. The grades and group sizes are stored in a named tuple. . all_grades = pd.read_csv(&#39;data/anonymized_grades.csv&#39;, usecols=[&#39;Section&#39;, &#39;Group&#39;, &#39;Percentage&#39;]) GradesWithGroups = namedtuple(&#39;GradesWithGroups&#39;, [&#39;grades&#39;, &#39;group_sizes&#39;]) section = {} for n in [1, 2, 3]: this_section = all_grades[all_grades[&#39;Section&#39;] == f&#39;Section {n}&#39;].sort_values(by=[&#39;Group&#39;]) grades = this_section[&#39;Percentage&#39;].to_numpy() group_sizes = this_section[&#39;Group&#39;].value_counts(sort=False).to_numpy() section[n] = GradesWithGroups(grades, group_sizes) . Implementation of the Tests . Permutation Test . The permutation test starts with a simple idea: Assuming it is the case that the student groups make no difference at all to the final grades, then those groups are essentially arbitrary. We call this assumption the null hypothesis. To estimate how unlikely our &quot;observed&quot; grades would be under the null hypothesis (that groups have no influence on grades), we repeatedly split the students into random groups (of the appropriate sizes): This allows us to estimate how extreme the differences in grades between the actual groups are relative to the differences in means between the random splits (we will see how to quantify the difference between the groups in a moment). Because under the null hypothesis each split is equally likely, the more extreme the differences between groups, the more unlikely those groups are (if we assume the null hypothesis is true). . How do we quantify how extreme the difference between the groups is? We need a statistic, which is a number that can be can be computed for any grades sample and correlates in some way with group differences. There is no single right choice because there are different ways in which the groups might differ. Since we are mostly interested in the group means, which are the expected grades for a student in each group, we will choose a statistic that is sensitive to differences in the group means. . If we only had two groups we could simply take the difference of the two group means. For several groups we will use the F-test statistic. Denote the grades in group $i$ by $y_{i,j}$ and the size of group $i$ by $n_i$. Furthermore, let $ bar y_i$ be the sample group mean of group $i$ and let $ bar y$ be the overall sample mean (of the pooled grades). We will use the following statistic: $$ F = frac{n_i sum_i ( bar y_{i} - bar y)^2}{ sum_{i,j}(y_{i,j} - bar y_{i})^2} $$ . We are dividing the variance between the groups (called explained variance) by the variance within the groups (called unexplained variance). It is clear the the larger the variance between the groups, the more likely it is that the groups have different true means. The reason we need to divide by the group variances is to control for the fact that more dispersed groups are more likely to give rise to larger variance in sample means due to chance. . Note that we left out some constants that are part of the usual definition of the F-test statistic. This is because a constant factor has no influence on the permutation test. . Before we can explain in detail how the test works, there is still one problem to resolve: Even for relatively small samples there is an astronomical number of permutations, which makes the permutation test intractable (impossible to actually compute except in very small examples). However, essentially the same results can be achieved by simply sampling permutations at random, provided sufficiently many samples are taken (tens of thousands, say). . The p-value under the randomized permutation test is computed as follows: . Resample a large number of random splits of the data (into groups of the right sizes). | Compute $F$ for each permuted sample. | The p-value is given by the fraction of $F$ values which are equal or larger than the value of $F$ for our original data. (Note that this fraction doesn&#39;t change if we multiply $F$ by a non-zero constant.) | . For example, if only $8 %$ of random splits result in a larger $F$ than the original data, the p-value is $0.08$. . The permutation test is nonparametric because it doesn&#39;t require us to postulate a parametric model for the distribution which underlies the observations. For example, a parametric model might be that the observations are drawn from a normal distribution, which is completely determined by two parameters: its mean and its variance. . This lack of parametric assumptions makes the permutation test more flexible and preferable whenever we don&#39;t have a good a priori model of our data and thus no reason to assume that the observations come from a specific distribution. . Implementing the Permutation Test . Because we are going to be taking ten thousand samples or even a hundred of thousand samples every time we perform a permutation test and then computing the statistic for each sampled permutation, we will want to leverage vectorized operations on NumPy arrays to speed the computations up considerably. This is especially important for the simulations at the end of the notebook, where we apply the hypothesis tests $10,000$ times to approximate the p-value distributions for each test. Even with the vectorized code some simulations take half an hour to run on my MacBook Pro. . Below we compare three implementations to sample permutations. . rng = np.random.default_rng() N = 10**5 sample = np.array([0] * 40) . First we use a for loop, which is the simplest for least optimal option. It doesn&#39;t take full advantage of NumPy arrays. . %%timeit permutations = [] for _ in range(N): permutations.append(rng.permuted(sample)) permuted = np.array(permutations) . 399 ms ± 23.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Next we implement a vectorized approach. In this case that means that instead of using a Python for loop we perform the permutation on the rows of a 2D array, using the NumPy function permuted. The 2D array consists of copies of the sample stacked on top of each other. . %%timeit permuted = rng.permuted(np.stack((sample,) * N), axis=-1) . 129 ms ± 2.88 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . We see that the vectorized approach is almost 3 times faster in this case. . There is an alternative vectorized implementation which is faster for smaller samples, although it is a little hacky. . %%timeit reindex = np.random.rand(N, sample.size).argsort(axis=1) permuted = sample[reindex] . 161 ms ± 43.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . This approach is fast for small samples but the previous approach has a better time complexity over the sample length (sorting an array has time complexity $ mathcal{O}(n log n)$ but finding a permutation has time complexity $ mathcal{O}(n)$). For samples of size 40 the permuted approach has the upper hand but for size 32 the argsort approach is still clearly faster (the sections have sizes 40, 32, 41). . We will go with the implementation using permuted. . def permute(sample, n_resample=10**5): &#39;&#39;&#39;Take 1D array sample and return 2D array with random permutations of sample as its rows&#39;&#39;&#39; rng = np.random.default_rng() return rng.permuted(np.stack((sample,) * n_resample), axis=-1) . Now we need to implement the test, which involves computing the F-statistic, which in turn requires the group means and group variances. We will actually need to compute those again later on and it will be convenient to have it in a separate function. It will be helpful to store the group means and the group standard deviations using a named tuple. . MeansVars = namedtuple(&#39;MeansVars&#39;, &#39;group_means group_vars&#39;) def take_group_means_and_vars(resamplings, group_sizes): &#39;&#39;&#39;Take 1D/2D array (each row is a resampling) and a 1D array of group sizes. Take the means of slices of the specified group sizes along the rows. Return an array containing the group means (with the same dimensions as the input).&#39;&#39;&#39; left = 0 group_means, group_vars = [], [] for l in group_sizes: right = left + l group_mean = resamplings[..., left: right].mean(axis=-1, keepdims=True) group_var = resamplings[..., left: right].var(axis=-1, keepdims=True) group_means.append(group_mean) group_vars.append(group_var) left = right return MeansVars(np.hstack(group_means), np.hstack(group_vars)) def F_stat(samples, group_sizes, version=&#39;regular&#39;): &#39;&#39;&#39;Compute F-test statistic (up to a constant factor which doesn&#39;t matter for the permutation test) for every row of a 1D/2D array and return an array with the computed values&#39;&#39;&#39; sample_size = samples.shape[-1] group_means = take_group_means_and_vars(samples, group_sizes).group_means pooled_mean = (group_means @ group_sizes / sample_size) if len(samples.shape) &gt; 1: pooled_mean = pooled_mean.reshape(samples.shape[0], 1) explained_variance = np.square(group_means - pooled_mean) @ group_sizes unexplained_variance = np.square(samples - np.repeat(group_means, group_sizes, axis=-1)).sum(axis=-1) return explained_variance / unexplained_variance # This is the F-test statistic up to a constant (degrees of freedom factors) def permutation_test(sample, group_sizes, n_resample=10**5): &#39;&#39;&#39;Compute the p-value according to the permutation test using the F-test statistic&#39;&#39;&#39; resamplings = permute(sample, n_resample) original = F_stat(sample, group_sizes) resampled = F_stat(resamplings, group_sizes) return (original &lt; resampled).mean() . The Semiparametric Bootstrap . On a superficial level, the nonparametric bootstrap (when used in a hypothesis test) is almost the same as the randomized permutation test, in that it involves repeatedly resampling from the actual data and computing a statistic each time. The main difference is that in the bootstrap we resample with replacement, meaning that the same value can be sampled repeatedly. However, conceptually the bootstrap is actually a richer tool, with applications that extend beyond hypothesis testing. . The main idea motivating the bootstrap is to think of the resamplings as new observations: We are drawing samples from the empirical distribution function (ECDF) instead of the &quot;true&quot; distribution underlying our observations. If we have a sufficiently large sample (and this is an important limitation to keep in mind), the ECDF will be a good approximation of the true distribution. The aim of the bootstrap method is usually to approximate the distribution of some statistic over the observations by the distribution of the statistic over the resamplings. . In our situation we could apply the nonparametric bootstrap by: . Resampling from the pooled grades (if we assume that the groups are interchangeable). This would amount to the (randomized) permutation test we implemented above, except for resampling with replacement (which wouldn&#39;t make a big difference to the end result if we take sufficiently many resamplings). | In the latter case we are resampling from very small groups (sizes ranging from 3 to 5) and so the approximation of the distributions of each group given by the bootstrap would be inadequate. | . Instead, we will use the semiparametric bootstrap. This bootstrap relies on a minimal model that allows us to pool the &quot;residuals&quot; from all groups and resample from those. . The Model . We assume that the grades in group $i$ are given by $y_{i,j} = mu_i + sigma_i epsilon_{i,j}$, where we assume that the $ epsilon_{i,j}$ are drawn from the same distribution, but we do not make any assumptions on which distribution that is. In words, we assume that the data in all groups follows the same distribution, except for perhaps having a different mean $ mu_i$ and a different standard deviation $ sigma_i$. I took this model from Example 4.14 in &quot;Bootstrap Methods and Their Application&quot; by Davison and Hinkley. . Our null hypothesis is that all group means are equal: $ mu_1= mu_2= ldots= mu_0$. Thus, the null hypothesis is sharper than for the permutation test (where the null hypothesis is that the distributions of the groups are identical), but we pay the price of having to make assumptions about the distributions. . Bootstrap Resampling . Now we need to take bootstrap resamples to approximate the distributions of each subgroup. What makes the bootstrap (semi) parametric is that we need to estimate some parameters: the group means $ mu_i$ and the group variances $ sigma_i^2$. Furthermore, what distinguishes this method from a fully parametric bootstrap is that we make no assumption on the distribution of the $ epsilon_{i,j}$, other than the fact that it is the same for all groups. In the parametric bootstrap the resamplings come from a distribution such as the normal distribution, which is analytically defined by some parameters (such as the mean and variance in the case of the normal distribution). Instead, we pool all the studentized residuals $e_{i,j}$ (see bellow) and resample from those residuals just like with the nonparametric bootstrap. In other words, we (implicitly) compute an estimated ECDF of the $ epsilon_{i,j}$, under the (parametric) assumptions of our model. . We do the bootstrap resampling under the null hypothesis, i.e. that assuming all the group means are the same. In order to do the resampling, we need to estimate the pooled mean $ mu_0$ and the group variances $ sigma_j^2$, and then use those to compute the studentized residuals $e_{i,j}$. . Let $n_i$ denote the number of students in group $i$, let $ bar y_i$ denote the sample means of each group and let $s_i^2$ denote the sample variances. To estimate the mean under the null hypothesis we weight the sample means with the factors $w_i = n_i/s_i^2$ (which are the reciprocals of the squared standard errors of the sample means). This gives the groups with larger variance a smaller weight to reduce the standard error in the estimate of the pooled mean. The null estimate of the mean is: $$ hat mu_0 = frac{ sum_{i=1}^nw_i bar y_i}{ sum_{i=1}^nw_i}$$ . The null estimates of the variances are given by: $$ hat sigma_{i,0}^2 = frac{n_i-1}{n_i}s_i^2 + ( bar y_i - hat mu_0)^2$$ . Now we have the necessary estimates to compute the studentized residuals: $$e_{i,j} = frac{y_{i,j} - hat mu_0}{ sqrt{ hat sigma_{i,0}^2 - ( sum_i w_i)^{-1}}}$$ . The bootstrap now resamples $ epsilon_{i,j}^*$ from the $e_{i,j}$ and replicates the $y_{i,j}$ as $y_{i,j}^* = hat mu_0 + hat sigma_{i,0} epsilon_{i,j}^*$. . The Statistic . In order to determine how extreme the differences between the group means are in our data we need an appropriate statistic. Following the book I mentioned above, we will use the statistic $$ tilde F = sum_{i=1}^k w_i( bar y_i - hat mu_0)^2.$$ Note that this is similar to the F-test statistic (hence the notation). The larger the differences between the sample means, the more extreme the value $ tilde F$ is. We use the weights $w_i$ introduced above to give sample means from groups with higher variance lower weight. This makes sense because the sample means of groups with higher variance will naturally deviate more strongly from the true mean $ mu_0$ (under the null hypothesis). By multiplying by $w_i$ we are essentially normalizing the squared errors by dividing by the squared standard error of the sample means. . Implementation of the Bootstrap Test . def estimate_params(samples, group_sizes): &#39;&#39;&#39;Takes a 2D array where each row is a sample (or a 1D array with just one sample), as well as an array of group sizes. The grades in each sample are ordered by group. Returns a named tuple (Estimates) with statistics computed from those samples. The computations are performed for all rows in a vectorized fashion.&#39;&#39;&#39; epsilon = 10**-5 # To prevent division by 0 group_means, group_vars = take_group_means_and_vars(samples, group_sizes) weights = group_sizes / (group_vars + epsilon) est_mean = np.expand_dims((group_means * weights).sum(axis=-1) / weights.sum(axis=-1), -1) est_vars = (group_sizes - 1) / group_sizes * group_vars + (group_means - est_mean)**2 residuals = (samples - est_mean) / np.repeat(np.sqrt(est_vars - np.expand_dims(1 / weights.sum(axis=-1), -1) + epsilon), group_sizes, axis=-1) Estimates = namedtuple(&#39;Estimates&#39;, &#39;residuals est_vars est_mean group_means group_vars weights&#39;) return Estimates(residuals, est_vars, est_mean, group_means, group_vars, weights) def bootstrap(original_sample, group_sizes, n_resample=10**5): &#39;&#39;&#39;Takes the data and generates n_resample new samples based on the semiparametric bootstrap introduced above. They are computed in a vectorized fashion and returned as the rows of a 2D array&#39;&#39;&#39; original_estimates = estimate_params(original_sample, group_sizes) original_residuals = original_estimates.residuals rng = np.random.default_rng() resample = rng.choice(np.arange(original_residuals.shape[0]), original_residuals.shape[0] * n_resample) resampled_residuals = original_residuals[resample].reshape(n_resample, original_residuals.shape[0]) replicatations = original_estimates.est_mean + resampled_residuals * np.repeat(np.sqrt(original_estimates.est_vars), group_sizes) return replicatations def F_tilde(samples, group_sizes, estimates=None): &#39;&#39;&#39;Computes the F tilde statistic for a 2D array (where each row is a sample) or a 1D array for a single sample. The statistic is built on top of some other statistics which might be supplied with the estimates keyword argument, to avoid computing them twice.&#39;&#39;&#39; if estimates is None: estimates = estimate_params(samples, group_sizes) return np.sum((estimates.group_means - estimates.est_mean)**2 * estimates.weights, axis=-1) def bootstrap_test(original_sample, group_sizes, n_resample=10**5): &#39;&#39;&#39;Computes the p-value for the bootstrap test using F tilde.&#39;&#39;&#39; replicates = bootstrap(original_sample, group_sizes, n_resample) return np.mean(F_tilde(replicates, group_sizes) &gt; F_tilde(original_sample, group_sizes)) . ANOVA F-test . ANOVA stands for Analysis of Variance and it is a well-known parametric approach to testing for differences of group means. There are several tools within ANOVA. We will use both the basic F-test and the Welch corrected F-test, which is a variant that makes the test more robust. . We actually used the (basic, uncorrected) F-test statistic in the permutation test, but these two tests compute the p-value very differently. For the permutation test, we simply re-compute the statistic repeatedly for many permutations of the original data and see how often the statistic takes values higher than on the original data. This gives us a measure of how extreme the data is. . The F-test, on the other hand, is a parametric test, which means that we have a parametric model for the probability distribution of the statistic (over the data). In other words, if we repeatedly took data samples and computed the statistic for those samples, those values would follow a probability distribution we can compute. In this instance we assume that the F-test statistic follows the so called F-distribution (with the appropriate degrees of freedom). Because the CDF of this distribution is known, we can directly compute which quantile that value belongs to (i.e. how extreme it is) without needing to resample the data. . Of course, the F-test will only give sensible results if the statistic does in fact follow the F-distribution, at least approximately. Thankfully, it can be mathematically proven that the F-test statistic follows the F-distribution if the data satisfies some assumptions: the grades of different students need to be independent of each other and the grades within each group need to come from normal distributions which have the same variance for all groups. . Those assumptions are unlikely to be fully satisfied for our data. The grade distributions is unlikely to be normal and there might be some differences in the group variances (see the Data Exploration section below). However, the test is known to be quite robust against deviations from normality. The Welch correction we will also use makes the test more robust against deviations from heteroscedasticity (different variance between groups). . In the last sections we do some simulations using the empirical distribution function of our pooled grades and find that the F-test does very well, despite the distribution not being normal. Surprisingly, the Welch test doesn&#39;t do that well. This may be in part due to the small group sizes. . Implementation of the F-test . We will just write a wrapper around the F-test included in the scipy library, which does the Welch correction by default. We will compare the results when the variances are assumed to be equal (no correction is applied) and when this assumption is not made and the statistic is corrected to compensate for the possibility of different variances. . def anova(pooled, group_sizes, equal_vars=False): groups = [] left = 0 for size in group_sizes: right = left + size groups.append(pooled[left:right]) left = right if equal_vars: return statsmodels_ftest(groups, use_var=&#39;equal&#39;).pvalue return statsmodels_ftest(groups).pvalue . Data Exploration . In this section we will: . Visualize the grades distribution. | Visualize the residuals used in the bootstrap test with a view to understand its low power. | Check if there is a significant deviation from normality and equal variance between groups (which are both assumptions of the F-test). | . Grades Distribution . Let&#39;s plot a histogram of the pooled grades from all three sections. . all_sections = GradesWithGroups( np.concatenate([section[1].grades, section[2].grades, section[3].grades]), np.concatenate([section[1].group_sizes, section[2].group_sizes, section[3].group_sizes]), ) fig, ax = plt.subplots(figsize=(8, 5)) ax.hist(all_sections.grades, bins=&#39;auto&#39;); . The data appears to have a clear left skew and thus does not look normal (more on that below). This made wonder why the data would differ from normality in this way and if this is a typical grade distribution for an exam. I found a 2019 paper (citation below), in which the authors analyzed 4000 assignments graded on Gradescope and essentially determined that most of the grade distributions were too skewed to be normal. Interestingly, the skewness was usually negative, just as in our case. They found the logit-normal distribution to be a good fit for exam grade distributions. . The authors do not venture to speculate why exam grades tend to have a left skew. One guess I have is that the students with the top grades are not given the opportunity to differentiate more, by having more challenging exam questions. It is also conceivable that there is higher variability among students with lower grades for reasons beyond the specific exam design. This is all just speculation of course, but it would be interesting to investigate. . Full citation of the paper: &quot;Grades are not Normal&quot; by N. Arthurs, B. Stenhaug, S. Karayev, C. Piech, published in Proceedings of the 12th International Conference on Educational Data Mining, Montréal, Canada. 2019 . Residuals . We will see in the simulations at the end of this notebook that the bootstrap has very low power when the groups are small, as is the case with our data. This might be due to the fact that the bootstrap method works by taking samples from the empirical distribution function of the residuals, which might not actually be a good approximation of the actual probability distribution of the groups, due to the small sizes of the groups. . Below we visualize the empirical distribution functions of the residuals for section 10 (the results are similar for the other two sections). We also compare them to synthetic data in two scenarios: . All groups have the same normal distribution and the group sizes are the same as in Section 1. | All groups have the same normal distribution and there are 10 groups of size 8. (For comparison, there are 10 groups in Section 1, mostly of size 4). | . fig, axs = plt.subplots(3, 2, figsize=(12, 8)) fig.tight_layout() axs[0, 0].hist(section[1].grades, bins=&#39;auto&#39;) axs[0, 0].set_title(&#39;Section 1 grades&#39;) axs[0, 1].hist(estimate_params(*section[1]).residuals, bins=&#39;auto&#39;) axs[0, 1].set_title(&#39;Section 1 residuals&#39;) synthetic_normal_small = GradesWithGroups(st.norm.rvs(size=section[1].group_sizes.sum()), section[1].group_sizes) synthetic_normal_large = GradesWithGroups(st.norm.rvs(size=100), np.array([10] * 10)) axs[1, 0].hist(synthetic_normal_small.grades, bins=&#39;auto&#39;) axs[1, 0].set_title(&#39;Synthetic normal data with small groups&#39;) axs[1, 1].hist(estimate_params(*synthetic_normal_small).residuals, bins=&#39;auto&#39;) axs[1, 1].set_title(&#39;Synthetic normal data with small groups residuals&#39;); axs[2, 0].hist(synthetic_normal_large.grades, bins=&#39;auto&#39;) axs[2, 0].set_title(&#39;Synthetic normal data with large groups&#39;) axs[2, 1].hist(estimate_params(*synthetic_normal_large).residuals, bins=&#39;auto&#39;) axs[2, 1].set_title(&#39;Synthetic normal data with large groups residuals&#39;); . The histograms seem to make clear that the empirical distributions of the residuals (pictured on the right) are not a good approximation of the underlying distribution (pictured on the left). This holds also for the synthetic data. These small sample effects might explain the low power of the bootstrap we will observe in the simulations below and why the power of the bootstrap converges with the other tests for larger group sizes. . Normality . We observed that the grade distribution looks too skewed to be normal. One way to quantify this is to compute the sample skewness and sample kurtosis. Of course, we expect these two quantities to be close to 0 if the data is sampled from a normal distribution (note that by default the scipy kurtosis function subtracts 3 from the fourth moment, which makes the kurtosis of the normal distribution 0). We will compare them to a random normal sample of the same size as our data to get an idea of how close to 0 the sample skewness and sample kurtosis usually are. . Another common method to visualize the data to see if it looks normal is the QQ plot, where the quantiles of the sample distribution are plotted against the quantiles of the normal distribution. We will again do this for both our grades data and for comparable synthetic data from a normal distribution. . qqplot(all_sections.grades) print(f&#39;The sample skew is {st.skew(all_sections.grades)} and the sample kurtosis is {st.kurtosis(all_sections.grades)}.&#39;) . The sample skew is -1.0902692480611051 and the sample kurtosis is 1.3753992488399467. . Now we compute compute the sample skew and kurtosis for a random sample of a normal distribution, as a reference for our data. We also look at the QQ plot. . normal_sample = st.norm.rvs(size=110) qqplot(normal_sample) print(f&#39;The sample skew is {st.skew(normal_sample)} and the sample kurtosis is {st.kurtosis(normal_sample)}.&#39;) . The sample skew is -0.1817125808235642 and the sample kurtosis is -0.34271547675743186. . After running the cell above a couple of times, it is evident that a sample skew of -1 and a sample kurtosis of 1.4 are very unlikely for data sampled from a normal distribution. What this means is that the the grades distribution is skewed to the left (as we observed above) and has a longer tails than the normal distribution (a long left tail, really). . This can also be seen in the QQ plots, from the fact that the graph is steeper at the beginning than towards the end. For a normal sample the QQ plot is expected to be close to a line of slope 1. . For a more principled way to determine significant deviation from normality we apply the Kolmogorov–Smirnov test. This test directly measures how different the observed distribution is from the normal distribution. First it computes the maximum distance between the empirical cumulative distribution function and the cumulative distribution of the normal distribution and then it determines how extreme this distance would be if we assume that the data does come from a normal distribution. The p-value it outputs is the probability of observing a difference as large as the one observed or larger for samples from a normal distribution. . kstest_normal(estimate_params(*all_sections).residuals)[1] . 0.044937074410551656 . The deviation from normality is significant, under the usual level for significance of 0.05. Specifically, what this means is the only in 4.5% of the cases would a sample from a normal distribution lead to such an extreme difference between the distribution of the data and the underlying normal distribution. This reinforces our analysis above. . Equal Variances . The Levene test is more suitable than than Bartlett test because the data significantly deviates from normality. . def test_equal_variances(pooled, group_sizes): groups = [] left = 0 for size in group_sizes: right = left + size groups.append(pooled[left:right]) left = right return st.levene(*groups)[1] for n in [1, 2, 3]: levene = test_equal_variances(*section[n]) print(f&#39;For Section {n} we have Levene p-value {levene}&#39;) . For Section 1 we have Levene p-value 0.8941346714392719 For Section 2 we have Levene p-value 0.9712056919863893 For Section 3 we have Levene p-value 0.9326385373075483 . The test does not detect significant differences in the variances. However, this should be taken with a grain of salt because the groups are so small that the test probably has a very small power. One indication that the test is not working well, at least in the case of comparisons between groups within each section, is how all three p-values are so high. This is because a well-behaved test should yield p-values which are uniformly distributed under the null hypothesis and which tend to have small values under the alternative hypothesis. . Compute p-values . In the following we compare the p-values from four hypothesis tests: . the (non-parametric) permutation test. | the semiparametric bootstrap. | the (parametric) ANOVA F-test. | the (parametric) ANOVA Welch F-test. | . We use all four tests to check for significant differences between the groups in each of the three sections. . index = [] pvalues = {&#39;Permutation Test&#39;: [], &#39;Semiparametric Bootstrap&#39;: [], &#39;ANOVA&#39;: [], &#39;ANOVA (Welch)&#39;: []} for n in [1, 2, 3]: pvalues[&#39;Permutation Test&#39;].append(permutation_test(*section[n])) pvalues[&#39;Semiparametric Bootstrap&#39;].append(bootstrap_test(*section[n])) pvalues[&#39;ANOVA&#39;].append(anova(*section[n], equal_vars=True)) pvalues[&#39;ANOVA (Welch)&#39;].append(anova(*section[n])) index.append(f&#39;Section {n} Groups&#39;) . pd.DataFrame(pvalues, index=index) . Permutation Test Semiparametric Bootstrap ANOVA ANOVA (Welch) . Section 1 Groups 0.18275 | 0.36078 | 0.188831 | 0.210221 | . Section 2 Groups 0.05562 | 0.48233 | 0.053237 | 0.129899 | . Section 3 Groups 0.38250 | 0.87914 | 0.392885 | 0.851698 | . Discussion . The p-values . The first thing that jumps out at us is that the p-values of the permutation test are almost identical to the p-values of the F-test without Welch correction. In the simulations done at the end of this notebook these two tests have persistently similar p-values. While we used the same (uncorrected) F-test statistic for the permutation test, it cannot be assumed that the p-values will be so close, considering that the tests compute the p-values in very different ways: the former simply recomputed the statistic for many permutations of the data, while the latter compares the value of the statistic to a theoretical distribution (called the F-test distribution). This seems to indicate that F-test is working well despite the data not being normally distributed. . As for the statistical significance of differences in grades between the groups: Based on the permutation test (or equivalently the uncorrected F-test) alone, there are indications of a small effect, especially in Section 2, but unfortunately it is not statistically significant. The usual convention is to set the rejection threshold at $0.05$. Section 2 comes close but we need to consider that we made multiple comparisons (see below). . Furthermore, the results suggest that the semiparametric bootstrap has low power, probably due to the small group sizes. At the end of this notebook we investigate the size and the power of the tests with various simulations. The results there show that the bootstrap does indeed have very low power when the groups are small (such as in our data). . Multiple Comparisons . Recall that the p-value tells us what the probability is of observing data as extreme or more extreme than the data we actually observed, assuming the null hypothesis is true (or perhaps an approximation of this probability). For example, according to the permutation test the probability that the distributions of the different groups in Section 2 are exactly the same is only $5.5 %$. Usually results are considered statistically significant if the p-value is under $0.05$, although this just a convention which is intended to keep the rate of false discoveries in scientific publications low (this threshold has been often criticized, as has the over-reliance on p-values more generally, but we will not get into that here). . However, that is only true for one test in isolation. Unlikely results are more likely to occur if we make multiple observations and it is very important to take this into account. . We will go through two different methods for handling so called multiple comparisons. To be concrete, let&#39;s say we set the significance level at $0.1$. Recall that the permutation test p-values for each of the three sections are $0.18$, $0.05$ and $0.38$. . Bonferroni: Divide the level $0.1$ by the number of comparisons to get the rejection threshold. In this case we would only reject the null if a p-value is under $0.1/3 = 0.0 overline{3}$. Because none of our p-values is under that threshold, we cannot reject the null in any of the three cases. This method ensures the probability of a single false positive will be no higher than $0.1$. The problem is that this also limits the true positives, i.e. rejections of null hypothesis which is false and should be rejected. We say that it increases the type II error rate and that it decreases the power. The next method strikes a balance between keeping false positives down while not reducing true positives too much. . Benjamin-Hochberg: This one is a little more involved than Bonferroni. It ensures that on average at most $10 %$ of all rejections will be false positives. Another way of putting it is that BH ensures that most of the rejections (discoveries) are in fact correct. To be clear, it does not ensure that the probability of making even a single type I error is at most $0.1$. In fact, BH (or variations of it) is often used in cases where thousands of comparisons are made (such as in genomics) where many true positives are expected and a small fraction of false positives is a price worth paying. . It is worth emphasizing that BH works regardless how many of the null hypotheses tested are in fact true. If all null hypotheses happen to be false, then of course $100 %$ of rejections will be false positives. However, BH ensures that this happens only $10 %$ of the time (if we set the level to $0.1$). Thus the expected false positive ratio is less than $0.1$. . Let&#39;s demonstrate BH with our p-values above. Sort the p-values: $p_{(1)} = 0.05$, $p_{(2)} = 0.18$ and $p_{(3)} = 0.38$. To find the rejection threshold using BH, we look for the largest p-value $p_{(i)}$ satifying $p_{({i})} le 0.1 cdot i/m$ ($m$ is the number of p-values, in this case 3). To put it in words, we need the p-value not just to be bounded above by the level of the test (in this case $ alpha=0.1$), but in addition to this to be bounded above by a fraction of the level $ alpha$ which is equal to the fraction of p-values smaller than or equal to $p_{(i)}$. As a hypothetical example, if we had a p-value $p_{(i)}= alpha/2$, then we could only use it as the rejection threshold if at least half of the p-values are smaller than this potential threshold $ alpha/2$. In our example, there is no such p-value because even $p_{(1)} = 0.05$ does not satisfy the condition: $0.05 &gt; 0.1 cdot1/3$. . Actually, in this case Bonferroni and BH give similar results regardless of how we set $ alpha$ because one p-value is so much smaller than the others. If we let the level be $0.15$ both would reject one null hypothesis. We would need to raise the level all the way up to $0.27$ for BH to reject two null hypotheses and to $0.38$ for BH to reject all three. Bonferroni would reject two at $ alpha=0.54$ and there is no level at which all three would be rejected. . Such high p-values would not really be very sensible in practice, of course. It should also be made clear that the level needs to be set in advance and not adapted to the experiment. The previous paragraph is just intended to clarify the differences between the two methods. . To consider an extreme example, if all p-values happen to be less than $ alpha$, then every single hypothesis is rejected because the largest p-value would be chosen as the threshold. The main intuition behind BH is that the more concentrated the p-values from multiple tests are closer to 0, the more null hypotheses must be true (or are at least expected to be true). A logically equivalent way of saying this is that p-values would be expected to be spread out if many null hypotheses are true since p-values should be uniformly distributed (between 0 and 1) under the null hypothesis. . Conclusions . Unfortunately, the differences in mean grades between the groups are not statistically significant. One of the p-values is 0.05, but taking into account the multiple comparisons we would have had to put the rejection threshold at 0.15 (this is true for Benjamin-Hochberg, not just for the more conservative Bonferroni). That threshold would be quite high. To be clear, the rejection threshold needs to be set before looking at the results for it to work as intended. The whole situation is also complicated by the fact that we did several tests. This certainly increases the likelihood of false positives but it is tricky to say by how much because the p-values of different tests are not independent of each other. We did this more as an exercise, rather than to get significant results for the group differences (it became clear early on that the group sizes are too small for significant results). . Of course, the results do not necessarily imply that there is a high probability that the group means are the same. As can be seen in the simulations below, in the more realistic scenarios all four hypothesis tests have pretty low power, due to the small sample sizes. This means that even if there were a meaningful effect it would be unlikely to be discovered by these tests (or probably any reasonable tests) unless it were very large. The only scenario in which the power of the tests is above $0.8$ is when we assume that the true group means vary as much as the sample group means (i.e. if there is a pretty large effect size). . To actually find the probability of a particular effect size in a principled way we would need to turn to Bayesian statistics. In a nutshell, we would need to come up with a prior (the credence we would we give to each scenario before we even look at the data) and then update this prior using the likelihood function (probability of seeing our data under each scenario). This would be an interesting direction to explore in the future. . One big takeaway is that the semiparametric bootstrap has especially low power, even with large effect sizes. In simulations with larger groups (size 10 specifically) the power of the bootstrap is similar to the other tests, which confirms that the small group sizes are the problem. Evidently, larger groups are needed for the bootstrap resamplings to adequately approximate the underlying distribution of the residuals. Recall that we avoided a purely nonparametric bootstrap which resamples from the individual groups because of their small sizes. We had hoped that combining the residuals of all groups using the semiparametric bootstrap would mitigate this, but this clearly failed. To salvage the bootstrap approach would need to either increase the group sizes or modify the underlying model in some way. . APPENDIX: Simulations of Power and Size . In this last section we simulate alternative hypotheses to investigate the size and power of the three tests through simulation. . Power and Size . The size of a test is easy to explain. It is simply the probability of rejecting the null hypothesis when it is in fact true. We want to keep the size as small as possible, of course. Crucially, the size is not supposed exceed the level of the test. Similarly, the power of a test is the probability of rejecting the null hypothesis if the alternative hypothesis is true. Just like we want to minimize the size, we also want to maximize the power and those two aims are always in tension. . To be fair, the concepts are a little trickier to make precise than the previous paragraph makes it seem. In the following two points we go into the weeds a bit more: . The alternative hypothesis is usually defined as the logical negation of the null hypothesis, but not always. For example, if the null hypothesis is that the means of all groups are equal, then the logical negation would yield an alternative hypothesis consisting of all situations in which the means differ in any way. However, maybe we want to restrict the alternative hypothesis to include only differences in means that would be significant in a certain context (that is, maybe we are only interested in rejecting a hypothesis when there is a minimal effect size). See the Neyman-Pearson Lemma for an example of an alternative hypothesis which is not the negation of the null hypothesis. | Another technicality is that both the the null hypothesis and the alternative hypothesis usually are usually composite hypotheses, meaning that they don&#39;t simply say that the data comes from a specific probability distribution, but rather from a set of possible probability distributions. For example, if the null hypothesis is that the means of the groups are equal, that doesn&#39;t fully determine the probability distribution at all. In truth, the size of a test is the supremum (which can be roughly thought of as the maximum) of the probability to reject the null hypothesis under a specific probability distribution which is part of the null hypothesis, where the supremum is taken over all those probability distributions. The same thing goes for the power and the alternative hypothesis. | . In the simulations we can&#39;t consider all scenarios and take the supremum to actually compute the power and size over the whole space of possibilities. Instead, we will contemplate some realistic scenarios for our situation. We do this by drawing samples from probability distributions which seem reasonable models for our data, apply our tests and finally inspect the distribution of the resulting p-values. If the tests work well, they will be uniformly distributed whenever the null hypothesis is true and concentrated towards small values when the null hypothesis is false. . Simulations Setup . In all simulations we will assume that the grade distributions have the same &quot;shape&quot; for all groups, meaning that they are the same up to shifting and scaling. We will use both the empirical distribution function (ECDF) of the pooled grades and the normal distribution. . Beyond this, the three parameters we can tune are the means, variances and group sizes. We will consider cases in which the true group means are equal, differ slightly and differ strongly. Similarly, we will consider cases in which the true group variances are equal, differ slightly or differ strongly. Finally, we will do simulations with small group sizes and with large group sizes. Specifically, we will use the exact group sizes of Section 1 (which happens to have 10 groups) or 10 groups of size 10. . All in all, there are 36 possible scenarios. To avoid clutter we will only include a subset of those. That will allow us to compute thousands of p-values in each case, which can take a long time because we are doing bootstrap and permutation resamplings. . To ensure the simulations are relevant to our actual grades data, we use the sample group means and sample group standard deviations. . Constant mean and variance: median group mean and average group variance over all groups from all three sections. | Weakly varying means and variances: a random sample of 10 out of middle $33 %$ of the group means and group variances over all groups (i.e. excluding the top third and bottom third). | Strongly varying means and variances: a random sample of 10 out of middle $80 %$ of the group means and group variances over all groups (i.e. excluding the top $10 %$ and bottom $10 %$). | Extremely varying means: a random sample of 10 among all group means. | . Simulation Results Overview . All tests have low power, especially the bootstrap: As we had anticipated above, the semiparametric bootstrap has a very low power when the groups are small (around size 4, as is the case in our data). It does similarly to the other tests when we increase the group size to 10. Something else we see in the simulations is that all the tests have low power, unless we let the group means vary a lot. . How much the true means vary matters a lot: In the following we assume that the group standard deviations are equal. . For weakly varying means (chosen from middle $33 %$ of sample group means, ranging from $85.0$ to $87.4$) the rejection rate is $7 %$, almost the same as under the null hypothesis. | For strongly varying means (chosen from middle $80 %$ of sample group means, ranging from $82.8$ to $90.2$) the rejection rate is $17 %$, which is still way too low. | For very strongly varying means (chosen from $100 %$ of sample group means, ranging from $73.6$ to $92.5$) the rejection rate is $85 %$, which is reasonable. | . Poor performance of the Welch F-test: It is surprising (at least to me) that the uncorrected F-test did better than the Welch F-test, considering that it is widely recommended to use the Welch test in all circumstances. Especially considering that we violated the normality and equal variances assumptions of the F-test and we have unequal group sizes, which is precisely when the Welch F-test should be doing better. My guess is that the Welch correction is not working well with the very small groups in our data. For larger groups it does better. . The group sizes are very important: Unsurprisingly, increasing the group sizes to 10 increases the power of all tests significantly. This is especially the case for the bootstrap test. . Close enough to normal: Even though the empirical distribution function is too skewed to be normal, it seems to be close enough for the F-tests. In the simulations it made almost no difference whether we used the empirical distribution or the normal distribution (not all simulations are included below). . Bimodal p-value distributions: Interestingly, when we have equal means and let the group standard deviations vary strongly, the p-value distributions for the permutation test and uncorrected F-test become bimodal: there is a peak at 0 and a peak at 1. The peak at 0 corresponds to samples with very small variation between sample group means (the groups form one big cluster), while the peak at 1 corresponds to sampled with very large variation between sample group means (the groups are spread into separate clusters). Remember that this case (equal means and unequal variances) is not included in the null hypothesis for the permutation test. It also violates the assumptions of the non-corrected F-test (ANOVA). Thus, a uniform p-value distribution was definitely not expected in those cases. What we might have expected for the permutation test is that the p-value distribution has only one peak, namely at 0. However, the statistic we used in the permutation test (the F-test statistic) is meant to capture differences between group means, not group standard deviations. This is presumably why we see this double peak and consequent low power. The bootstrap test and the Welch F-test did not result in bimodal p-value histograms in any of our simulations. For large group sizes the p-values are close to uniformly distributed and the rejection rate is close to the level. This is as it should be because the former are precisely designed to handle variation in the standard distributions. . Simulation Results . First we need to compute the group means and standard deviations, as well as the empirical distribution, to be used in the simulations below. . all_group_means = take_group_means_and_vars(*all_sections).group_means all_group_stds = np.sqrt(take_group_means_and_vars(*all_sections).group_vars) central_mean = np.median(all_group_means) central_std = np.median(all_group_stds) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4)) fig.tight_layout() ax1.hist(all_group_means, bins=&#39;auto&#39;) ax1.set_title(&#39;Group means&#39;) ax2.hist(all_group_stds, bins=&#39;auto&#39;) ax2.set_title(&#39;Group stds&#39;) print(f&#39;The median of the group means is {central_mean} and the median of the group standard deviations is {central_std}.&#39;) . The median of the group means is 86.5875 and the median of the group standard deviations is 6.331982950879348. . section1_group_sizes = section[1].group_sizes section1_n_groups = section1_group_sizes.size large_group_sizes = np.array([10] * section1_n_groups) # Truncated lists of group means and group stds total_number_of_groups = all_group_means.size third_group_means = np.sort(all_group_means)[total_number_of_groups // 3: 2 * total_number_of_groups // 3] third_group_stds = np.sort(all_group_stds)[total_number_of_groups // 3: 2 * total_number_of_groups // 3] most_group_means = np.sort(all_group_means)[total_number_of_groups // 10: 9 * total_number_of_groups // 10] most_group_stds = np.sort(all_group_stds)[total_number_of_groups // 10: 9 * total_number_of_groups // 10] # Sample of means and stds for use in the simulations rng = np.random.default_rng(42) constant_means = np.array([central_mean] * section1_n_groups) constant_stds = np.array([central_std] * section1_n_groups) weakly_varying_means = rng.choice(third_group_means, size=section1_n_groups) weakly_varying_stds = rng.choice(third_group_stds, size=section1_n_groups) strongly_varying_means = rng.choice(most_group_means, size=section1_n_groups) strongly_varying_stds = rng.choice(most_group_stds, size=section1_n_groups) very_strongly_varying_means = rng.choice(all_group_means, size=section1_n_groups) # Normalized empirical distribution normalized_ecdf = (all_sections.grades - all_sections.grades.mean()) / all_sections.grades.std() . Finally, we write two functions for the simulations to do the following: . Draw samples from some given group distributions and compute the p-value distributions of each hypothesis test. | Plot the p-value histograms for all the tests in a grid. | . def simulate_pvalues(group_sizes, group_means, group_stds, dist, n_sims, level): &#39;&#39;&#39;Take n_sims samples with distribution dist and the given group means, group variances and group sizes. For each sample, compute the p-value for each of the four hypothesis tests. &#39;&#39;&#39; coverage, pvalues = defaultdict(int), defaultdict(list) rng = np.random.default_rng() for _ in tqdm(range(n_sims)): group_samples = [] for size, mean, std in zip(group_sizes, group_means, group_stds): if isinstance(dist, np.ndarray): # dist is the normalized ECDF sample = mean + rng.choice(dist, size) * std group_samples.append(sample) elif dist == &#39;normal&#39;: sample = st.norm.rvs(loc=mean, scale=std, size=size, random_state=rng) group_samples.append(sample) else: raise Exception(f&#39;Invalid dist argument: {dist}&#39;) pooled_sample = np.concatenate(group_samples) pvalue = permutation_test(pooled_sample, group_sizes, n_resample=10**4) pvalues[&#39;permutation&#39;].append(pvalue) coverage[&#39;permutation&#39;] += pvalue &lt;= level pvalue = bootstrap_test(pooled_sample, group_sizes, n_resample=10**4) pvalues[&#39;bootstrap&#39;].append(pvalue) coverage[&#39;bootstrap&#39;] += pvalue &lt;= level pvalue = anova(pooled_sample, group_sizes, equal_vars=True) pvalues[&#39;ANOVA&#39;].append(pvalue) coverage[&#39;ANOVA&#39;] += pvalue &lt;= level pvalue = anova(pooled_sample, group_sizes) pvalues[&#39;ANOVA (Welch)&#39;].append(pvalue) coverage[&#39;ANOVA (Welch)&#39;] += pvalue &lt;= level for key in coverage: coverage[key] /= n_sims return coverage, pvalues . def plot_histograms(pvalues_dict): fig, axs = plt.subplots(2, 2, figsize=(10, 6)) positions = [(0, 0), (0, 1), (1, 0), (1, 1)] for position, key in zip(positions, pvalues_dict): axs[position].hist(pvalues_dict[key], bins=&#39;auto&#39;) axs[position].set_title(key) . Groups Have Equal Distributions . In the following we simulate the case in which the grade distributions are exactly the same for all groups. . This is precisely the null hypothesis of the permutation test and it is contained in the null hypothesis of the other two tests (equal means). Therefore, we would expect the p-values to be uniformly distributed if the tests are working well. . We start with the normal distribution, which satisfies the assumptions of the F-test. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, constant_stds, &#39;normal&#39;, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0463 bootstrap 0.0110 ANOVA 0.0469 ANOVA (Welch) 0.1073 dtype: float64 . Firstly, the permutation test and the uncorrected F-test (ANOVA) have almost identical p-values and they are also performing the best by far. Their rejection rates are just under the level $0.05$, which is perfect. The distributions look uniformly distributed. . The p-values for the semiparametric bootstrap and the Welch corrected F-test are clearly not uniformly distributed. . For the bootstrap the actual size turns out to actually be smaller than the level. This is not in itself undesirable but it indicates that the power will be very low. . Let&#39;s see if the bootstrap and Welch F-test do better for larger groups: . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, constant_means, constant_stds, &#39;normal&#39;, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0474 bootstrap 0.0428 ANOVA 0.0475 ANOVA (Welch) 0.0578 dtype: float64 . From what we have seen above, it seems that the bootstrap and Welch F-test are not performing well for the small group sizes present in our data (around 4), while doing reasonably well for size 10 groups. The permutation test and the regular uncorrected F-test (one-way ANOVA) are still the best but not by much. . Next, let&#39;s use the actual empirical grades distribution instead of the normal distribution. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0551 bootstrap 0.0127 ANOVA 0.0543 ANOVA (Welch) 0.1454 dtype: float64 . Now let&#39;s make the groups larger again. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, constant_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0486 bootstrap 0.0499 ANOVA 0.0480 ANOVA (Welch) 0.0758 dtype: float64 . We can see that the empirical distribution and the normal distribution yield pretty results. The biggest difference is for the Welch F-test, which has an even worse rejection rate for the empirical distribution. This might be due to the long left tail. We will only use the empirical distribution function from now on. . Groups Have Equal Means but Varying Standard Deviations . Now we are strictly speaking stepping out of the null hypothesis of the permutation test, which requires standard deviations to be equal too (although the statistic we chose makes it mostly sensitive to the means). We are still within the null hypothesis of the other tests. Again, we would expect uniformly distributed p-values if the tests are working as they should. . First let us vary the standard deviations only slightly. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, weakly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0512 bootstrap 0.0145 ANOVA 0.0509 ANOVA (Welch) 0.1438 dtype: float64 . The p-values are almost identical to the case with constant standard deviation. They are very slightly larger in this case (as was expected) but not significantly. The same is true for larger group sizes: . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, constant_means, weakly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0493 bootstrap 0.0508 ANOVA 0.0472 ANOVA (Welch) 0.0730 dtype: float64 . If we let the standard deviation vary strongly between the groups something interesting happens: some p-value distributions have a peak at 1 as well as the usual peak at 0. This is the case also for very large group sizes. For instance, we let the groups be of size 50 below, which is much larger than the groups we have been considering. The peaks do not seem to change much as we vary the group sizes from 4 to 10 to 50. . When the groups are this large and the group standard deviations differ strongly the bootstrap and the Welch F-test do better than the permutation test and regular F-test. This is as it should be because the former are precisely designed to handle variation in the standard distributions. It is worth noting that this is the only case in which the bootstrap and the Welch F-test seem to be doing better than the other two, assuming our goal is to detect differences in means (which, to be fair, is not really the aim of the permutation test). . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, constant_means, strongly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0672 bootstrap 0.0174 ANOVA 0.0654 ANOVA (Welch) 0.1576 dtype: float64 . n_sims = 10**4 level = 0.05 very_large_group_sizes = np.array([50] * 10) result = simulate_pvalues(very_large_group_sizes, constant_means, strongly_varying_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0679 bootstrap 0.0535 ANOVA 0.0680 ANOVA (Welch) 0.0598 dtype: float64 . Unequal means . Finally, we have arrived at the arguably more important alternative hypothesis: there is a difference in the expected grade (mean) of the groups. . First, we will assume that the means vary weakly to see if the tests would pick up on that. Later, we will let the means vary more strongly. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, weakly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.0727 bootstrap 0.0185 ANOVA 0.0715 ANOVA (Welch) 0.1585 dtype: float64 . We see that the tests are very weak in this case, due to the small effect size (weakly varying means) and the small group sizes. The bootstrap is still performing terribly and the permutation tests and ANOVA rejection rates are barely higher than the level of the test. . The Welch F-test has the highest power, but this is useless given that the rejection rate is pretty much the same as it was when the null hypothesis was true (making it impossible to distinguish between true and false positives). . Let&#39;s see if the power increases with larger group sizes: . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, weakly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.1241 bootstrap 0.1120 ANOVA 0.1218 ANOVA (Welch) 0.1531 dtype: float64 . Leaving the Welch F-test aside, the power has increased for larger groups, but it is still not great. It is worth noting that the bootstrap now yields almost the same p-value distribution as the permutation test and regular F-test, underscoring how the bootstrap does fine for larger groups, but just can&#39;t deal with the small groups in our data. . Next we will let the group means vary strongly. Recall that we randomly picked 10 the means from the middle $80 %$ sample group means. . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, strongly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.1710 bootstrap 0.0329 ANOVA 0.1704 ANOVA (Welch) 0.2581 dtype: float64 . n_sims = 10**4 level = 0.05 result = simulate_pvalues(large_group_sizes, strongly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.4901 bootstrap 0.3989 ANOVA 0.4847 ANOVA (Welch) 0.4882 dtype: float64 . As we can see, going from the middle $33 %$ group means to the middle $80 %$ group means made an enormous difference. At around $0.4$, the power is still not great. . One way to increase the power is increasing the effect size even more. In the following, we will randomly choose from $100 %$ of the group means (what we called very strongly varying means). . n_sims = 10**4 level = 0.05 result = simulate_pvalues(section1_group_sizes, very_strongly_varying_means, constant_stds, normalized_ecdf, n_sims, level) plot_histograms(result[1]) pd.Series(dict(result[0])) . permutation 0.8508 bootstrap 0.2242 ANOVA 0.8502 ANOVA (Welch) 0.7863 dtype: float64 . We see that in the extremely varying means scenario the power of all tests other than the bootstrap is reasonably good ($0.85$). Surprisingly, the Welch corrected F-test has a lower power that the uncorrected F-test in this case, even though in most other cases it had a higher power (even too high, for equal means). .",
            "url": "david-recio.com/2022/03/19/grades-analysis.html",
            "relUrl": "/2022/03/19/grades-analysis.html",
            "date": " • Mar 19, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Math PhD, self-taught in Python, stats, and machine learning while my Green Card application was pending. Most recently a postdoc at Lehigh University doing research on Topological Robotics. . Grew up in Spain 🇪🇸, studied math and physics in Germany 🇩🇪, and did my PhD in the UK 🇬🇧. . Now a US permanent resident 🇺🇸 based in Boston. . You can view my publications here. Note that I used my double surname Recio-Mitter on my articles. (In Spain everyone has a double surname but now that I am in the US I use my paternal surname to avoid confusion.) .",
          "url": "david-recio.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "david-recio.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}